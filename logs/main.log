INFO:__main__:++++++++++++++++++++++++++++++
INFO:__main__:===== starting run with seed 0 =====
INFO:src.utils:-- loading data from ../processed_data/narrative_trajectories.pkl with test size 0.5 --
INFO:__main__:-- training fqe for evaluation --
INFO:src.model.fqe:-- loaded fqe with run_name seed_0 --
INFO:__main__:-- training narrative planner with original data --
INFO:src.model.bcq:-- loaded behavior cloning with run_name seed_0 --
INFO:src.model.bcq:-- loaded bcq with run_name seed_0 --
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  76.2766 | is:  0.0000 | wis:  1.0000 | dr:  153.3529 | dm:  142.5612
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  76.2766 | is:  0.0000 | wis:  1.0000 | dr:  153.3529 | dm:  142.5612
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  76.2766 | is:  0.0000 | wis:  1.0000 | dr:  153.3529 | dm:  142.5612
INFO:__main__:-- training narrative planner with sim data --
INFO:src.model.bcq:-- loaded behavior cloning with run_name seed_0 --
INFO:src.model.bcq:-- loaded bcq with run_name seed_0 --
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  5.5392 | is: -0.0000 | wis:  0.9750 | dr:  147.1513 | dm:  125.1010
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  5.5392 | is: -0.0000 | wis:  0.9750 | dr:  147.1513 | dm:  125.1010
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  5.5392 | is: -0.0000 | wis:  0.9750 | dr:  147.1513 | dm:  125.1010
INFO:__main__:-- training narrative planner with combined data --
INFO:src.model.bcq:-- loaded behavior cloning with run_name seed_0 --
INFO:src.model.bcq:-- loaded bcq with run_name seed_0 --
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr: -18.3510 | is: -0.0000 | wis:  1.0000 | dr:  150.6893 | dm:  142.0341
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr: -18.3510 | is: -0.0000 | wis:  1.0000 | dr:  150.6893 | dm:  142.0341
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr: -18.3510 | is: -0.0000 | wis:  1.0000 | dr:  150.6893 | dm:  142.0341
INFO:__main__:++++++++++++++++++++++++++++++
INFO:__main__:===== starting run with seed 1 =====
INFO:src.utils:-- loading data from ../processed_data/narrative_trajectories.pkl with test size 0.5 --
INFO:__main__:-- training fqe for evaluation --
INFO:src.model.fqe:-- loaded fqe with run_name seed_1 --
INFO:__main__:-- training narrative planner with original data --
INFO:src.model.bcq:-- loaded behavior cloning with run_name seed_1 --
INFO:src.model.bcq:-- loaded bcq with run_name seed_1 --
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  136.6449 | is: -0.0000 | wis:  0.9625 | dr:  138.5900 | dm:  123.1215
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  136.6449 | is: -0.0000 | wis:  0.9625 | dr:  138.5900 | dm:  123.1215
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  136.6449 | is: -0.0000 | wis:  0.9625 | dr:  138.5900 | dm:  123.1215
INFO:src.utils:-- loading data from ../processed_data/student_trajectories.pkl with test size 0.2 --
INFO:src.model.validator:no validator with run_name seed_1
INFO:src.model.validator:-- training validator --
INFO:src.model.validator:reward training epoch 0 | current loss  0.25386387
INFO:src.model.validator:reward training epoch 1 | current loss  0.25328934
INFO:src.model.validator:reward training epoch 2 | current loss  0.24941313
INFO:src.model.validator:reward training epoch 3 | current loss  0.24391407
INFO:src.model.validator:reward training epoch 4 | current loss  0.24482593
INFO:src.model.validator:reward training epoch 5 | current loss  0.24421956
INFO:src.model.validator:reward training epoch 6 | current loss  0.24036773
INFO:src.model.validator:reward training epoch 7 | current loss  0.23786663
INFO:src.model.validator:reward training epoch 8 | current loss  0.23776171
INFO:src.model.validator:reward training epoch 9 | current loss  0.23681161
INFO:src.model.validator:reward training epoch 10 | current loss  0.23395841
INFO:src.model.validator:reward training epoch 11 | current loss  0.23145315
INFO:src.model.validator:reward training epoch 12 | current loss  0.23054877
INFO:src.model.validator:reward training epoch 13 | current loss  0.22958240
INFO:src.model.validator:reward training epoch 14 | current loss  0.22749731
INFO:src.model.validator:reward training epoch 15 | current loss  0.22539395
INFO:src.model.validator:reward training epoch 16 | current loss  0.22428736
INFO:src.model.validator:reward training epoch 17 | current loss  0.22339344
INFO:src.model.validator:reward training epoch 18 | current loss  0.22189328
INFO:src.model.validator:reward training epoch 19 | current loss  0.22030002
INFO:src.model.validator:reward training epoch 20 | current loss  0.21932787
INFO:src.model.validator:reward training epoch 21 | current loss  0.21854389
INFO:src.model.validator:reward training epoch 22 | current loss  0.21725067
INFO:src.model.validator:reward training epoch 23 | current loss  0.21587694
INFO:src.model.validator:reward training epoch 24 | current loss  0.21489272
INFO:src.model.validator:reward training epoch 25 | current loss  0.21389845
INFO:src.model.validator:reward training epoch 26 | current loss  0.21271016
INFO:src.model.validator:reward training epoch 27 | current loss  0.21179910
INFO:src.model.validator:reward training epoch 28 | current loss  0.21117397
INFO:src.model.validator:reward training epoch 29 | current loss  0.21030493
INFO:src.model.validator:reward training epoch 30 | current loss  0.20925891
INFO:src.model.validator:reward training epoch 31 | current loss  0.20844525
INFO:src.model.validator:reward training epoch 32 | current loss  0.20770441
INFO:src.model.validator:reward training epoch 33 | current loss  0.20680621
INFO:src.model.validator:reward training epoch 34 | current loss  0.20596962
INFO:src.model.validator:reward training epoch 35 | current loss  0.20524803
INFO:src.model.validator:reward training epoch 36 | current loss  0.20444493
INFO:src.model.validator:reward training epoch 37 | current loss  0.20366064
INFO:src.model.validator:reward training epoch 38 | current loss  0.20302404
INFO:src.model.validator:reward training epoch 39 | current loss  0.20234743
INFO:src.model.validator:reward training epoch 40 | current loss  0.20161234
INFO:src.model.validator:reward training epoch 41 | current loss  0.20099224
INFO:src.model.validator:reward training epoch 42 | current loss  0.20038316
INFO:src.model.validator:reward training epoch 43 | current loss  0.19972347
INFO:src.model.validator:reward training epoch 44 | current loss  0.19910568
INFO:src.model.validator:reward training epoch 45 | current loss  0.19847861
INFO:src.model.validator:reward training epoch 46 | current loss  0.19781196
INFO:src.model.validator:reward training epoch 47 | current loss  0.19721754
INFO:src.model.validator:reward training epoch 48 | current loss  0.19667344
INFO:src.model.validator:reward training epoch 49 | current loss  0.19609882
INFO:src.model.validator:reward training epoch 50 | current loss  0.19552799
INFO:src.model.validator:reward training epoch 51 | current loss  0.19493626
INFO:src.model.validator:reward training epoch 52 | current loss  0.19431165
INFO:src.model.validator:reward training epoch 53 | current loss  0.19373159
INFO:src.model.validator:reward training epoch 54 | current loss  0.19317269
INFO:src.model.validator:reward training epoch 55 | current loss  0.19259629
INFO:src.model.validator:reward training epoch 56 | current loss  0.19202593
INFO:src.model.validator:reward training epoch 57 | current loss  0.19143626
INFO:src.model.validator:reward training epoch 58 | current loss  0.19083913
INFO:src.model.validator:reward training epoch 59 | current loss  0.19025980
INFO:src.model.validator:reward training epoch 60 | current loss  0.18966529
INFO:src.model.validator:reward training epoch 61 | current loss  0.18906420
INFO:src.model.validator:reward training epoch 62 | current loss  0.18847813
INFO:src.model.validator:reward training epoch 63 | current loss  0.18789668
INFO:src.model.validator:reward training epoch 64 | current loss  0.18730713
INFO:src.model.validator:reward training epoch 65 | current loss  0.18669206
INFO:src.model.validator:reward training epoch 66 | current loss  0.18603133
INFO:src.model.validator:reward training epoch 67 | current loss  0.18533354
INFO:src.model.validator:reward training epoch 68 | current loss  0.18456766
INFO:src.model.validator:reward training epoch 69 | current loss  0.18372649
INFO:src.model.validator:reward training epoch 70 | current loss  0.18287243
INFO:src.model.validator:reward training epoch 71 | current loss  0.18207426
INFO:src.model.validator:reward training epoch 72 | current loss  0.18143892
INFO:src.model.validator:reward training epoch 73 | current loss  0.18090059
INFO:src.model.validator:reward training epoch 74 | current loss  0.18031129
INFO:src.model.validator:reward training epoch 75 | current loss  0.17962579
INFO:src.model.validator:reward training epoch 76 | current loss  0.17887713
INFO:src.model.validator:reward training epoch 77 | current loss  0.17810421
INFO:src.model.validator:reward training epoch 78 | current loss  0.17733663
INFO:src.model.validator:reward training epoch 79 | current loss  0.17659226
INFO:src.model.validator:reward training epoch 80 | current loss  0.17588371
INFO:src.model.validator:reward training epoch 81 | current loss  0.17518499
INFO:src.model.validator:reward training epoch 82 | current loss  0.17446116
INFO:src.model.validator:reward training epoch 83 | current loss  0.17370094
INFO:src.model.validator:reward training epoch 84 | current loss  0.17291319
INFO:src.model.validator:reward training epoch 85 | current loss  0.17211314
INFO:src.model.validator:reward training epoch 86 | current loss  0.17132348
INFO:src.model.validator:reward training epoch 87 | current loss  0.17056735
INFO:src.model.validator:reward training epoch 88 | current loss  0.16983935
INFO:src.model.validator:reward training epoch 89 | current loss  0.16911748
INFO:src.model.validator:reward training epoch 90 | current loss  0.16838631
INFO:src.model.validator:reward training epoch 91 | current loss  0.16764662
INFO:src.model.validator:reward training epoch 92 | current loss  0.16691144
INFO:src.model.validator:reward training epoch 93 | current loss  0.16619317
INFO:src.model.validator:reward training epoch 94 | current loss  0.16549760
INFO:src.model.validator:reward training epoch 95 | current loss  0.16481777
INFO:src.model.validator:reward training epoch 96 | current loss  0.16413845
INFO:src.model.validator:reward training epoch 97 | current loss  0.16345346
INFO:src.model.validator:reward training epoch 98 | current loss  0.16277118
INFO:src.model.validator:reward training epoch 99 | current loss  0.16210102
INFO:src.model.validator:reward training epoch 100 | current loss  0.16144441
INFO:src.model.validator:reward training epoch 101 | current loss  0.16079536
INFO:src.model.validator:reward training epoch 102 | current loss  0.16014920
INFO:src.model.validator:reward training epoch 103 | current loss  0.15950471
INFO:src.model.validator:reward training epoch 104 | current loss  0.15886272
INFO:src.model.validator:reward training epoch 105 | current loss  0.15822743
INFO:src.model.validator:reward training epoch 106 | current loss  0.15760346
INFO:src.model.validator:reward training epoch 107 | current loss  0.15698831
INFO:src.model.validator:reward training epoch 108 | current loss  0.15637496
INFO:src.model.validator:reward training epoch 109 | current loss  0.15576282
INFO:src.model.validator:reward training epoch 110 | current loss  0.15515856
INFO:src.model.validator:reward training epoch 111 | current loss  0.15456548
INFO:src.model.validator:reward training epoch 112 | current loss  0.15397877
INFO:src.model.validator:reward training epoch 113 | current loss  0.15339546
INFO:src.model.validator:reward training epoch 114 | current loss  0.15281896
INFO:src.model.validator:reward training epoch 115 | current loss  0.15225029
INFO:src.model.validator:reward training epoch 116 | current loss  0.15168788
INFO:src.model.validator:reward training epoch 117 | current loss  0.15113154
INFO:src.model.validator:reward training epoch 118 | current loss  0.15058254
INFO:src.model.validator:reward training epoch 119 | current loss  0.15004031
INFO:src.model.validator:reward training epoch 120 | current loss  0.14950381
INFO:src.model.validator:reward training epoch 121 | current loss  0.14897381
INFO:src.model.validator:reward training epoch 122 | current loss  0.14845026
INFO:src.model.validator:reward training epoch 123 | current loss  0.14793241
INFO:src.model.validator:reward training epoch 124 | current loss  0.14742091
INFO:src.model.validator:reward training epoch 125 | current loss  0.14691609
INFO:src.model.validator:reward training epoch 126 | current loss  0.14641677
INFO:src.model.validator:reward training epoch 127 | current loss  0.14592220
INFO:src.model.validator:reward training epoch 128 | current loss  0.14543346
INFO:src.model.validator:reward training epoch 129 | current loss  0.14495094
INFO:src.model.validator:reward training epoch 130 | current loss  0.14447372
INFO:src.model.validator:reward training epoch 131 | current loss  0.14400110
INFO:src.model.validator:reward training epoch 132 | current loss  0.14353323
INFO:src.model.validator:reward training epoch 133 | current loss  0.14307030
INFO:src.model.validator:reward training epoch 134 | current loss  0.14261225
INFO:src.model.validator:reward training epoch 135 | current loss  0.14215875
INFO:src.model.validator:reward training epoch 136 | current loss  0.14170922
INFO:src.model.validator:reward training epoch 137 | current loss  0.14126354
INFO:src.model.validator:reward training epoch 138 | current loss  0.14082181
INFO:src.model.validator:reward training epoch 139 | current loss  0.14038403
INFO:src.model.validator:reward training epoch 140 | current loss  0.13994989
INFO:src.model.validator:reward training epoch 141 | current loss  0.13951892
INFO:src.model.validator:reward training epoch 142 | current loss  0.13909091
INFO:src.model.validator:reward training epoch 143 | current loss  0.13866608
INFO:src.model.validator:reward training epoch 144 | current loss  0.13824439
INFO:src.model.validator:reward training epoch 145 | current loss  0.13782552
INFO:src.model.validator:reward training epoch 146 | current loss  0.13740936
INFO:src.model.validator:reward training epoch 147 | current loss  0.13699590
INFO:src.model.validator:reward training epoch 148 | current loss  0.13658518
INFO:src.model.validator:reward training epoch 149 | current loss  0.13617727
INFO:src.model.validator:reward training epoch 150 | current loss  0.13577221
INFO:src.model.validator:reward training epoch 151 | current loss  0.13537002
INFO:src.model.validator:reward training epoch 152 | current loss  0.13497061
INFO:src.model.validator:reward training epoch 153 | current loss  0.13457412
INFO:src.model.validator:reward training epoch 154 | current loss  0.13418056
INFO:src.model.validator:reward training epoch 155 | current loss  0.13378984
INFO:src.model.validator:reward training epoch 156 | current loss  0.13340184
INFO:src.model.validator:reward training epoch 157 | current loss  0.13301641
INFO:src.model.validator:reward training epoch 158 | current loss  0.13263337
INFO:src.model.validator:reward training epoch 159 | current loss  0.13225244
INFO:src.model.validator:reward training epoch 160 | current loss  0.13187325
INFO:src.model.validator:reward training epoch 161 | current loss  0.13149548
INFO:src.model.validator:reward training epoch 162 | current loss  0.13111903
INFO:src.model.validator:reward training epoch 163 | current loss  0.13074437
INFO:src.model.validator:reward training epoch 164 | current loss  0.13037306
INFO:src.model.validator:reward training epoch 165 | current loss  0.13000767
INFO:src.model.validator:reward training epoch 166 | current loss  0.12965058
INFO:src.model.validator:reward training epoch 167 | current loss  0.12930252
INFO:src.model.validator:reward training epoch 168 | current loss  0.12896192
INFO:src.model.validator:reward training epoch 169 | current loss  0.12862611
INFO:src.model.validator:reward training epoch 170 | current loss  0.12829317
INFO:src.model.validator:reward training epoch 171 | current loss  0.12796234
INFO:src.model.validator:reward training epoch 172 | current loss  0.12763342
INFO:src.model.validator:reward training epoch 173 | current loss  0.12730630
INFO:src.model.validator:reward training epoch 174 | current loss  0.12698084
INFO:src.model.validator:reward training epoch 175 | current loss  0.12665711
INFO:src.model.validator:reward training epoch 176 | current loss  0.12633535
INFO:src.model.validator:reward training epoch 177 | current loss  0.12601605
INFO:src.model.validator:reward training epoch 178 | current loss  0.12569979
INFO:src.model.validator:reward training epoch 179 | current loss  0.12538698
INFO:src.model.validator:reward training epoch 180 | current loss  0.12507787
INFO:src.model.validator:reward training epoch 181 | current loss  0.12477224
INFO:src.model.validator:reward training epoch 182 | current loss  0.12446948
INFO:src.model.validator:reward training epoch 183 | current loss  0.12416890
INFO:src.model.validator:reward training epoch 184 | current loss  0.12386985
INFO:src.model.validator:reward training epoch 185 | current loss  0.12357177
INFO:src.model.validator:reward training epoch 186 | current loss  0.12327430
INFO:src.model.validator:reward training epoch 187 | current loss  0.12297719
INFO:src.model.validator:reward training epoch 188 | current loss  0.12268037
INFO:src.model.validator:reward training epoch 189 | current loss  0.12238389
INFO:src.model.validator:reward training epoch 190 | current loss  0.12208791
INFO:src.model.validator:reward training epoch 191 | current loss  0.12179278
INFO:src.model.validator:reward training epoch 192 | current loss  0.12149893
INFO:src.model.validator:reward training epoch 193 | current loss  0.12120686
INFO:src.model.validator:reward training epoch 194 | current loss  0.12091715
INFO:src.model.validator:reward training epoch 195 | current loss  0.12063037
INFO:src.model.validator:reward training epoch 196 | current loss  0.12034713
INFO:src.model.validator:reward training epoch 197 | current loss  0.12006799
INFO:src.model.validator:reward training epoch 198 | current loss  0.11979347
INFO:src.model.validator:reward training epoch 199 | current loss  0.11952396
INFO:src.model.validator:reward training epoch 200 | current loss  0.11925969
INFO:src.model.validator:reward training epoch 201 | current loss  0.11900084
INFO:src.model.validator:reward training epoch 202 | current loss  0.11874736
INFO:src.model.validator:reward training epoch 203 | current loss  0.11849915
INFO:src.model.validator:reward training epoch 204 | current loss  0.11825597
INFO:src.model.validator:reward training epoch 205 | current loss  0.11801759
INFO:src.model.validator:reward training epoch 206 | current loss  0.11778364
INFO:src.model.validator:reward training epoch 207 | current loss  0.11755379
INFO:src.model.validator:reward training epoch 208 | current loss  0.11732774
INFO:src.model.validator:reward training epoch 209 | current loss  0.11710528
INFO:src.model.validator:reward training epoch 210 | current loss  0.11688616
INFO:src.model.validator:reward training epoch 211 | current loss  0.11667026
INFO:src.model.validator:reward training epoch 212 | current loss  0.11645745
INFO:src.model.validator:reward training epoch 213 | current loss  0.11624765
INFO:src.model.validator:reward training epoch 214 | current loss  0.11604083
INFO:src.model.validator:reward training epoch 215 | current loss  0.11583687
INFO:src.model.validator:reward training epoch 216 | current loss  0.11563576
INFO:src.model.validator:reward training epoch 217 | current loss  0.11543740
INFO:src.model.validator:reward training epoch 218 | current loss  0.11524174
INFO:src.model.validator:reward training epoch 219 | current loss  0.11504874
INFO:src.model.validator:reward training epoch 220 | current loss  0.11485831
INFO:src.model.validator:reward training epoch 221 | current loss  0.11467047
INFO:src.model.validator:reward training epoch 222 | current loss  0.11448510
INFO:src.model.validator:reward training epoch 223 | current loss  0.11430222
INFO:src.model.validator:reward training epoch 224 | current loss  0.11412173
INFO:src.model.validator:reward training epoch 225 | current loss  0.11394365
INFO:src.model.validator:reward training epoch 226 | current loss  0.11376788
INFO:src.model.validator:reward training epoch 227 | current loss  0.11359439
INFO:src.model.validator:reward training epoch 228 | current loss  0.11342312
INFO:src.model.validator:reward training epoch 229 | current loss  0.11325400
INFO:src.model.validator:reward training epoch 230 | current loss  0.11308701
INFO:src.model.validator:reward training epoch 231 | current loss  0.11292208
INFO:src.model.validator:reward training epoch 232 | current loss  0.11275920
INFO:src.model.validator:reward training epoch 233 | current loss  0.11259826
INFO:src.model.validator:reward training epoch 234 | current loss  0.11243927
INFO:src.model.validator:reward training epoch 235 | current loss  0.11228216
INFO:src.model.validator:reward training epoch 236 | current loss  0.11212689
INFO:src.model.validator:reward training epoch 237 | current loss  0.11197343
INFO:src.model.validator:reward training epoch 238 | current loss  0.11182174
INFO:src.model.validator:reward training epoch 239 | current loss  0.11167188
INFO:src.model.validator:reward training epoch 240 | current loss  0.11152375
INFO:src.model.validator:reward training epoch 241 | current loss  0.11137740
INFO:src.model.validator:reward training epoch 242 | current loss  0.11123276
INFO:src.model.validator:reward training epoch 243 | current loss  0.11108978
INFO:src.model.validator:reward training epoch 244 | current loss  0.11094846
INFO:src.model.validator:reward training epoch 245 | current loss  0.11080872
INFO:src.model.validator:reward training epoch 246 | current loss  0.11067057
INFO:src.model.validator:reward training epoch 247 | current loss  0.11053402
INFO:src.model.validator:reward training epoch 248 | current loss  0.11039901
INFO:src.model.validator:reward training epoch 249 | current loss  0.11026556
INFO:src.model.validator:reward training epoch 250 | current loss  0.11013362
INFO:src.model.validator:reward training epoch 251 | current loss  0.11000315
INFO:src.model.validator:reward training epoch 252 | current loss  0.10987412
INFO:src.model.validator:reward training epoch 253 | current loss  0.10974650
INFO:src.model.validator:reward training epoch 254 | current loss  0.10962027
INFO:src.model.validator:reward training epoch 255 | current loss  0.10949539
INFO:src.model.validator:reward training epoch 256 | current loss  0.10937186
INFO:src.model.validator:reward training epoch 257 | current loss  0.10924965
INFO:src.model.validator:reward training epoch 258 | current loss  0.10912876
INFO:src.model.validator:reward training epoch 259 | current loss  0.10900914
INFO:src.model.validator:reward training epoch 260 | current loss  0.10889076
INFO:src.model.validator:reward training epoch 261 | current loss  0.10877361
INFO:src.model.validator:reward training epoch 262 | current loss  0.10865767
INFO:src.model.validator:reward training epoch 263 | current loss  0.10854289
INFO:src.model.validator:reward training epoch 264 | current loss  0.10842931
INFO:src.model.validator:reward training epoch 265 | current loss  0.10831685
INFO:src.model.validator:reward training epoch 266 | current loss  0.10820552
INFO:src.model.validator:reward training epoch 267 | current loss  0.10809531
INFO:src.model.validator:reward training epoch 268 | current loss  0.10798620
INFO:src.model.validator:reward training epoch 269 | current loss  0.10787816
INFO:src.model.validator:reward training epoch 270 | current loss  0.10777118
INFO:src.model.validator:reward training epoch 271 | current loss  0.10766523
INFO:src.model.validator:reward training epoch 272 | current loss  0.10756032
INFO:src.model.validator:reward training epoch 273 | current loss  0.10745639
INFO:src.model.validator:reward training epoch 274 | current loss  0.10735345
INFO:src.model.validator:reward training epoch 275 | current loss  0.10725151
INFO:src.model.validator:reward training epoch 276 | current loss  0.10715052
INFO:src.model.validator:reward training epoch 277 | current loss  0.10705047
INFO:src.model.validator:reward training epoch 278 | current loss  0.10695137
INFO:src.model.validator:reward training epoch 279 | current loss  0.10685317
INFO:src.model.validator:reward training epoch 280 | current loss  0.10675589
INFO:src.model.validator:reward training epoch 281 | current loss  0.10665950
INFO:src.model.validator:reward training epoch 282 | current loss  0.10656400
INFO:src.model.validator:reward training epoch 283 | current loss  0.10646938
INFO:src.model.validator:reward training epoch 284 | current loss  0.10637562
INFO:src.model.validator:reward training epoch 285 | current loss  0.10628274
INFO:src.model.validator:reward training epoch 286 | current loss  0.10619071
INFO:src.model.validator:reward training epoch 287 | current loss  0.10609953
INFO:src.model.validator:reward training epoch 288 | current loss  0.10600919
INFO:src.model.validator:reward training epoch 289 | current loss  0.10591970
INFO:src.model.validator:reward training epoch 290 | current loss  0.10583104
INFO:src.model.validator:reward training epoch 291 | current loss  0.10574324
INFO:src.model.validator:reward training epoch 292 | current loss  0.10565624
INFO:src.model.validator:reward training epoch 293 | current loss  0.10557012
INFO:src.model.validator:reward training epoch 294 | current loss  0.10548481
INFO:src.model.validator:reward training epoch 295 | current loss  0.10540032
INFO:src.model.validator:reward training epoch 296 | current loss  0.10531668
INFO:src.model.validator:reward training epoch 297 | current loss  0.10523389
INFO:src.model.validator:reward training epoch 298 | current loss  0.10515189
INFO:src.model.validator:reward training epoch 299 | current loss  0.10507078
INFO:src.model.validator:reward training epoch 300 | current loss  0.10499046
INFO:src.model.validator:reward training epoch 301 | current loss  0.10491099
INFO:src.model.validator:reward training epoch 302 | current loss  0.10483234
INFO:src.model.validator:reward training epoch 303 | current loss  0.10475451
INFO:src.model.validator:reward training epoch 304 | current loss  0.10467754
INFO:src.model.validator:reward training epoch 305 | current loss  0.10460136
INFO:src.model.validator:reward training epoch 306 | current loss  0.10452601
INFO:src.model.validator:reward training epoch 307 | current loss  0.10445146
INFO:src.model.validator:reward training epoch 308 | current loss  0.10437773
INFO:src.model.validator:reward training epoch 309 | current loss  0.10430481
INFO:src.model.validator:reward training epoch 310 | current loss  0.10423268
INFO:src.model.validator:reward training epoch 311 | current loss  0.10416133
INFO:src.model.validator:reward training epoch 312 | current loss  0.10409080
INFO:src.model.validator:reward training epoch 313 | current loss  0.10402101
INFO:src.model.validator:reward training epoch 314 | current loss  0.10395200
INFO:src.model.validator:reward training epoch 315 | current loss  0.10388378
INFO:src.model.validator:reward training epoch 316 | current loss  0.10381629
INFO:src.model.validator:reward training epoch 317 | current loss  0.10374954
INFO:src.model.validator:reward training epoch 318 | current loss  0.10368355
INFO:src.model.validator:reward training epoch 319 | current loss  0.10361827
INFO:src.model.validator:reward training epoch 320 | current loss  0.10355373
INFO:src.model.validator:reward training epoch 321 | current loss  0.10348991
INFO:src.model.validator:reward training epoch 322 | current loss  0.10342678
INFO:src.model.validator:reward training epoch 323 | current loss  0.10336434
INFO:src.model.validator:reward training epoch 324 | current loss  0.10330261
INFO:src.model.validator:reward training epoch 325 | current loss  0.10324154
INFO:src.model.validator:reward training epoch 326 | current loss  0.10318115
INFO:src.model.validator:reward training epoch 327 | current loss  0.10312144
INFO:src.model.validator:reward training epoch 328 | current loss  0.10306238
INFO:src.model.validator:reward training epoch 329 | current loss  0.10300395
INFO:src.model.validator:reward training epoch 330 | current loss  0.10294621
INFO:src.model.validator:reward training epoch 331 | current loss  0.10288905
INFO:src.model.validator:reward training epoch 332 | current loss  0.10283254
INFO:src.model.validator:reward training epoch 333 | current loss  0.10277667
INFO:src.model.validator:reward training epoch 334 | current loss  0.10272138
INFO:src.model.validator:reward training epoch 335 | current loss  0.10266670
INFO:src.model.validator:reward training epoch 336 | current loss  0.10261261
INFO:src.model.validator:reward training epoch 337 | current loss  0.10255912
INFO:src.model.validator:reward training epoch 338 | current loss  0.10250622
INFO:src.model.validator:reward training epoch 339 | current loss  0.10245387
INFO:src.model.validator:reward training epoch 340 | current loss  0.10240209
INFO:src.model.validator:reward training epoch 341 | current loss  0.10235091
INFO:src.model.validator:reward training epoch 342 | current loss  0.10230026
INFO:src.model.validator:reward training epoch 343 | current loss  0.10225015
INFO:src.model.validator:reward training epoch 344 | current loss  0.10220058
INFO:src.model.validator:reward training epoch 345 | current loss  0.10215156
INFO:src.model.validator:reward training epoch 346 | current loss  0.10210305
INFO:src.model.validator:reward training epoch 347 | current loss  0.10205507
INFO:src.model.validator:reward training epoch 348 | current loss  0.10200760
INFO:src.model.validator:reward training epoch 349 | current loss  0.10196064
INFO:src.model.validator:reward training epoch 350 | current loss  0.10191418
INFO:src.model.validator:reward training epoch 351 | current loss  0.10186823
INFO:src.model.validator:reward training epoch 352 | current loss  0.10182276
INFO:src.model.validator:reward training epoch 353 | current loss  0.10177777
INFO:src.model.validator:reward training epoch 354 | current loss  0.10173327
INFO:src.model.validator:reward training epoch 355 | current loss  0.10168923
INFO:src.model.validator:reward training epoch 356 | current loss  0.10164566
INFO:src.model.validator:reward training epoch 357 | current loss  0.10160257
INFO:src.model.validator:reward training epoch 358 | current loss  0.10155991
INFO:src.model.validator:reward training epoch 359 | current loss  0.10151773
INFO:src.model.validator:reward training epoch 360 | current loss  0.10147598
INFO:src.model.validator:reward training epoch 361 | current loss  0.10143466
INFO:src.model.validator:reward training epoch 362 | current loss  0.10139377
INFO:src.model.validator:reward training epoch 363 | current loss  0.10135334
INFO:src.model.validator:reward training epoch 364 | current loss  0.10131330
INFO:src.model.validator:reward training epoch 365 | current loss  0.10127371
INFO:src.model.validator:reward training epoch 366 | current loss  0.10123452
INFO:src.model.validator:reward training epoch 367 | current loss  0.10119575
INFO:src.model.validator:reward training epoch 368 | current loss  0.10115736
INFO:src.model.validator:reward training epoch 369 | current loss  0.10111941
INFO:src.model.validator:reward training epoch 370 | current loss  0.10108183
INFO:src.model.validator:reward training epoch 371 | current loss  0.10104464
INFO:src.model.validator:reward training epoch 372 | current loss  0.10100785
INFO:src.model.validator:reward training epoch 373 | current loss  0.10097142
INFO:src.model.validator:reward training epoch 374 | current loss  0.10093540
INFO:src.model.validator:reward training epoch 375 | current loss  0.10089975
INFO:src.model.validator:reward training epoch 376 | current loss  0.10086443
INFO:src.model.validator:reward training epoch 377 | current loss  0.10082951
INFO:src.model.validator:reward training epoch 378 | current loss  0.10079496
INFO:src.model.validator:reward training epoch 379 | current loss  0.10076074
INFO:src.model.validator:reward training epoch 380 | current loss  0.10072688
INFO:src.model.validator:reward training epoch 381 | current loss  0.10069338
INFO:src.model.validator:reward training epoch 382 | current loss  0.10066023
INFO:src.model.validator:reward training epoch 383 | current loss  0.10062741
INFO:src.model.validator:reward training epoch 384 | current loss  0.10059494
INFO:src.model.validator:reward training epoch 385 | current loss  0.10056277
INFO:src.model.validator:reward training epoch 386 | current loss  0.10053097
INFO:src.model.validator:reward training epoch 387 | current loss  0.10049947
INFO:src.model.validator:reward training epoch 388 | current loss  0.10046829
INFO:src.model.validator:reward training epoch 389 | current loss  0.10043745
INFO:src.model.validator:reward training epoch 390 | current loss  0.10040692
INFO:src.model.validator:reward training epoch 391 | current loss  0.10037670
INFO:src.model.validator:reward training epoch 392 | current loss  0.10034680
INFO:src.model.validator:reward training epoch 393 | current loss  0.10031719
INFO:src.model.validator:reward training epoch 394 | current loss  0.10028788
INFO:src.model.validator:reward training epoch 395 | current loss  0.10025887
INFO:src.model.validator:reward training epoch 396 | current loss  0.10023017
INFO:src.model.validator:reward training epoch 397 | current loss  0.10020175
INFO:src.model.validator:reward training epoch 398 | current loss  0.10017361
INFO:src.model.validator:reward training epoch 399 | current loss  0.10014576
INFO:src.model.validator:reward training epoch 400 | current loss  0.10011821
INFO:src.model.validator:reward training epoch 401 | current loss  0.10009094
INFO:src.model.validator:reward training epoch 402 | current loss  0.10006393
INFO:src.model.validator:reward training epoch 403 | current loss  0.10003722
INFO:src.model.validator:reward training epoch 404 | current loss  0.10001075
INFO:src.model.validator:reward training epoch 405 | current loss  0.09998456
INFO:src.model.validator:reward training epoch 406 | current loss  0.09995864
INFO:src.model.validator:reward training epoch 407 | current loss  0.09993296
INFO:src.model.validator:reward training epoch 408 | current loss  0.09990757
INFO:src.model.validator:reward training epoch 409 | current loss  0.09988244
INFO:src.model.validator:reward training epoch 410 | current loss  0.09985753
INFO:src.model.validator:reward training epoch 411 | current loss  0.09983289
INFO:src.model.validator:reward training epoch 412 | current loss  0.09980849
INFO:src.model.validator:reward training epoch 413 | current loss  0.09978435
INFO:src.model.validator:reward training epoch 414 | current loss  0.09976044
INFO:src.model.validator:reward training epoch 415 | current loss  0.09973679
INFO:src.model.validator:reward training epoch 416 | current loss  0.09971336
INFO:src.model.validator:reward training epoch 417 | current loss  0.09969018
INFO:src.model.validator:reward training epoch 418 | current loss  0.09966723
INFO:src.model.validator:reward training epoch 419 | current loss  0.09964451
INFO:src.model.validator:reward training epoch 420 | current loss  0.09962200
INFO:src.model.validator:reward training epoch 421 | current loss  0.09959973
INFO:src.model.validator:reward training epoch 422 | current loss  0.09957768
INFO:src.model.validator:reward training epoch 423 | current loss  0.09955585
INFO:src.model.validator:reward training epoch 424 | current loss  0.09953424
INFO:src.model.validator:reward training epoch 425 | current loss  0.09951287
INFO:src.model.validator:reward training epoch 426 | current loss  0.09949169
INFO:src.model.validator:reward training epoch 427 | current loss  0.09947073
INFO:src.model.validator:reward training epoch 428 | current loss  0.09944998
INFO:src.model.validator:reward training epoch 429 | current loss  0.09942944
INFO:src.model.validator:reward training epoch 430 | current loss  0.09940909
INFO:src.model.validator:reward training epoch 431 | current loss  0.09938895
INFO:src.model.validator:reward training epoch 432 | current loss  0.09936903
INFO:src.model.validator:reward training epoch 433 | current loss  0.09934929
INFO:src.model.validator:reward training epoch 434 | current loss  0.09932975
INFO:src.model.validator:reward training epoch 435 | current loss  0.09931041
INFO:src.model.validator:reward training epoch 436 | current loss  0.09929126
INFO:src.model.validator:reward training epoch 437 | current loss  0.09927229
INFO:src.model.validator:reward training epoch 438 | current loss  0.09925353
INFO:src.model.validator:reward training epoch 439 | current loss  0.09923495
INFO:src.model.validator:reward training epoch 440 | current loss  0.09921656
INFO:src.model.validator:reward training epoch 441 | current loss  0.09919836
INFO:src.model.validator:reward training epoch 442 | current loss  0.09918030
INFO:src.model.validator:reward training epoch 443 | current loss  0.09916247
INFO:src.model.validator:reward training epoch 444 | current loss  0.09914479
INFO:src.model.validator:reward training epoch 445 | current loss  0.09912729
INFO:src.model.validator:reward training epoch 446 | current loss  0.09910997
INFO:src.model.validator:reward training epoch 447 | current loss  0.09909283
INFO:src.model.validator:reward training epoch 448 | current loss  0.09907584
INFO:src.model.validator:reward training epoch 449 | current loss  0.09905903
INFO:src.model.validator:reward training epoch 450 | current loss  0.09904238
INFO:src.model.validator:reward training epoch 451 | current loss  0.09902591
INFO:src.model.validator:reward training epoch 452 | current loss  0.09900960
INFO:src.model.validator:reward training epoch 453 | current loss  0.09899346
INFO:src.model.validator:reward training epoch 454 | current loss  0.09897745
INFO:src.model.validator:reward training epoch 455 | current loss  0.09896164
INFO:src.model.validator:reward training epoch 456 | current loss  0.09894596
INFO:src.model.validator:reward training epoch 457 | current loss  0.09893043
INFO:src.model.validator:reward training epoch 458 | current loss  0.09891508
INFO:src.model.validator:reward training epoch 459 | current loss  0.09889986
INFO:src.model.validator:reward training epoch 460 | current loss  0.09888481
INFO:src.model.validator:reward training epoch 461 | current loss  0.09886991
INFO:src.model.validator:reward training epoch 462 | current loss  0.09885515
INFO:src.model.validator:reward training epoch 463 | current loss  0.09884052
INFO:src.model.validator:reward training epoch 464 | current loss  0.09882606
INFO:src.model.validator:reward training epoch 465 | current loss  0.09881173
INFO:src.model.validator:reward training epoch 466 | current loss  0.09879755
INFO:src.model.validator:reward training epoch 467 | current loss  0.09878351
INFO:src.model.validator:reward training epoch 468 | current loss  0.09876961
INFO:src.model.validator:reward training epoch 469 | current loss  0.09875584
INFO:src.model.validator:reward training epoch 470 | current loss  0.09874221
INFO:src.model.validator:reward training epoch 471 | current loss  0.09872873
INFO:src.model.validator:reward training epoch 472 | current loss  0.09871536
INFO:src.model.validator:reward training epoch 473 | current loss  0.09870215
INFO:src.model.validator:reward training epoch 474 | current loss  0.09868904
INFO:src.model.validator:reward training epoch 475 | current loss  0.09867609
INFO:src.model.validator:reward training epoch 476 | current loss  0.09866326
INFO:src.model.validator:reward training epoch 477 | current loss  0.09865055
INFO:src.model.validator:reward training epoch 478 | current loss  0.09863797
INFO:src.model.validator:reward training epoch 479 | current loss  0.09862550
INFO:src.model.validator:reward training epoch 480 | current loss  0.09861319
INFO:src.model.validator:reward training epoch 481 | current loss  0.09860097
INFO:src.model.validator:reward training epoch 482 | current loss  0.09858887
INFO:src.model.validator:reward training epoch 483 | current loss  0.09857691
INFO:src.model.validator:reward training epoch 484 | current loss  0.09856506
INFO:src.model.validator:reward training epoch 485 | current loss  0.09855334
INFO:src.model.validator:reward training epoch 486 | current loss  0.09854171
INFO:src.model.validator:reward training epoch 487 | current loss  0.09853020
INFO:src.model.validator:reward training epoch 488 | current loss  0.09851884
INFO:src.model.validator:reward training epoch 489 | current loss  0.09850755
INFO:src.model.validator:reward training epoch 490 | current loss  0.09849639
INFO:src.model.validator:reward training epoch 491 | current loss  0.09848534
INFO:src.model.validator:reward training epoch 492 | current loss  0.09847441
INFO:src.model.validator:reward training epoch 493 | current loss  0.09846357
INFO:src.model.validator:reward training epoch 494 | current loss  0.09845286
INFO:src.model.validator:reward training epoch 495 | current loss  0.09844224
INFO:src.model.validator:reward training epoch 496 | current loss  0.09843173
INFO:src.model.validator:reward training epoch 497 | current loss  0.09842133
INFO:src.model.validator:reward training epoch 498 | current loss  0.09841102
INFO:src.model.validator:reward training epoch 499 | current loss  0.09840081
INFO:src.model.validator:reward training epoch 500 | current loss  0.09839072
INFO:src.model.validator:reward training epoch 501 | current loss  0.09838071
INFO:src.model.validator:reward training epoch 502 | current loss  0.09837080
INFO:src.model.validator:reward training epoch 503 | current loss  0.09836100
INFO:src.model.validator:reward training epoch 504 | current loss  0.09835128
INFO:src.model.validator:reward training epoch 505 | current loss  0.09834170
INFO:src.model.validator:reward training epoch 506 | current loss  0.09833217
INFO:src.model.validator:reward training epoch 507 | current loss  0.09832276
INFO:src.model.validator:reward training epoch 508 | current loss  0.09831344
INFO:src.model.validator:reward training epoch 509 | current loss  0.09830420
INFO:src.model.validator:reward training epoch 510 | current loss  0.09829507
INFO:src.model.validator:reward training epoch 511 | current loss  0.09828602
INFO:src.model.validator:reward training epoch 512 | current loss  0.09827706
INFO:src.model.validator:reward training epoch 513 | current loss  0.09826819
INFO:src.model.validator:reward training epoch 514 | current loss  0.09825940
INFO:src.model.validator:reward training epoch 515 | current loss  0.09825072
INFO:src.model.validator:reward training epoch 516 | current loss  0.09824209
INFO:src.model.validator:reward training epoch 517 | current loss  0.09823357
INFO:src.model.validator:reward training epoch 518 | current loss  0.09822513
INFO:src.model.validator:reward training epoch 519 | current loss  0.09821678
INFO:src.model.validator:reward training epoch 520 | current loss  0.09820850
INFO:src.model.validator:reward training epoch 521 | current loss  0.09820031
INFO:src.model.validator:reward training epoch 522 | current loss  0.09819220
INFO:src.model.validator:reward training epoch 523 | current loss  0.09818418
INFO:src.model.validator:reward training epoch 524 | current loss  0.09817623
INFO:src.model.validator:reward training epoch 525 | current loss  0.09816837
INFO:src.model.validator:reward training epoch 526 | current loss  0.09816056
INFO:src.model.validator:reward training epoch 527 | current loss  0.09815285
INFO:src.model.validator:reward training epoch 528 | current loss  0.09814522
INFO:src.model.validator:reward training epoch 529 | current loss  0.09813765
INFO:src.model.validator:reward training epoch 530 | current loss  0.09813017
INFO:src.model.validator:reward training epoch 531 | current loss  0.09812277
INFO:src.model.validator:reward training epoch 532 | current loss  0.09811541
INFO:src.model.validator:reward training epoch 533 | current loss  0.09810816
INFO:src.model.validator:reward training epoch 534 | current loss  0.09810097
INFO:src.model.validator:reward training epoch 535 | current loss  0.09809384
INFO:src.model.validator:reward training epoch 536 | current loss  0.09808679
INFO:src.model.validator:reward training epoch 537 | current loss  0.09807982
INFO:src.model.validator:reward training epoch 538 | current loss  0.09807289
INFO:src.model.validator:reward training epoch 539 | current loss  0.09806605
INFO:src.model.validator:reward training epoch 540 | current loss  0.09805929
INFO:src.model.validator:reward training epoch 541 | current loss  0.09805259
INFO:src.model.validator:reward training epoch 542 | current loss  0.09804594
INFO:src.model.validator:reward training epoch 543 | current loss  0.09803936
INFO:src.model.validator:reward training epoch 544 | current loss  0.09803285
INFO:src.model.validator:reward training epoch 545 | current loss  0.09802642
INFO:src.model.validator:reward training epoch 546 | current loss  0.09802002
INFO:src.model.validator:reward training epoch 547 | current loss  0.09801371
INFO:src.model.validator:reward training epoch 548 | current loss  0.09800747
INFO:src.model.validator:reward training epoch 549 | current loss  0.09800127
INFO:src.model.validator:reward training epoch 550 | current loss  0.09799514
INFO:src.model.validator:reward training epoch 551 | current loss  0.09798907
INFO:src.model.validator:reward training epoch 552 | current loss  0.09798305
INFO:src.model.validator:reward training epoch 553 | current loss  0.09797712
INFO:src.model.validator:reward training epoch 554 | current loss  0.09797122
INFO:src.model.validator:reward training epoch 555 | current loss  0.09796540
INFO:src.model.validator:reward training epoch 556 | current loss  0.09795962
INFO:src.model.validator:reward training epoch 557 | current loss  0.09795390
INFO:src.model.validator:reward training epoch 558 | current loss  0.09794825
INFO:src.model.validator:reward training epoch 559 | current loss  0.09794264
INFO:src.model.validator:reward training epoch 560 | current loss  0.09793708
INFO:src.model.validator:reward training epoch 561 | current loss  0.09793162
INFO:src.model.validator:reward training epoch 562 | current loss  0.09792615
INFO:src.model.validator:reward training epoch 563 | current loss  0.09792078
INFO:src.model.validator:reward training epoch 564 | current loss  0.09791546
INFO:src.model.validator:reward training epoch 565 | current loss  0.09791018
INFO:src.model.validator:reward training epoch 566 | current loss  0.09790497
INFO:src.model.validator:reward training epoch 567 | current loss  0.09789979
INFO:src.model.validator:reward training epoch 568 | current loss  0.09789467
INFO:src.model.validator:reward training epoch 569 | current loss  0.09788958
INFO:src.model.validator:reward training epoch 570 | current loss  0.09788457
INFO:src.model.validator:reward training epoch 571 | current loss  0.09787961
INFO:src.model.validator:reward training epoch 572 | current loss  0.09787469
INFO:src.model.validator:reward training epoch 573 | current loss  0.09786981
INFO:src.model.validator:reward training epoch 574 | current loss  0.09786498
INFO:src.model.validator:reward training epoch 575 | current loss  0.09786020
INFO:src.model.validator:reward training epoch 576 | current loss  0.09785549
INFO:src.model.validator:reward training epoch 577 | current loss  0.09785081
INFO:src.model.validator:reward training epoch 578 | current loss  0.09784617
INFO:src.model.validator:reward training epoch 579 | current loss  0.09784158
INFO:src.model.validator:reward training epoch 580 | current loss  0.09783704
INFO:src.model.validator:reward training epoch 581 | current loss  0.09783255
INFO:src.model.validator:reward training epoch 582 | current loss  0.09782808
INFO:src.model.validator:reward training epoch 583 | current loss  0.09782369
INFO:src.model.validator:reward training epoch 584 | current loss  0.09781931
INFO:src.model.validator:reward training epoch 585 | current loss  0.09781501
INFO:src.model.validator:reward training epoch 586 | current loss  0.09781072
INFO:src.model.validator:reward training epoch 587 | current loss  0.09780648
INFO:src.model.validator:reward training epoch 588 | current loss  0.09780230
INFO:src.model.validator:reward training epoch 589 | current loss  0.09779815
INFO:src.model.validator:reward training epoch 590 | current loss  0.09779403
INFO:src.model.validator:reward training epoch 591 | current loss  0.09778997
INFO:src.model.validator:reward training epoch 592 | current loss  0.09778592
INFO:src.model.validator:reward training epoch 593 | current loss  0.09778194
INFO:src.model.validator:reward training epoch 594 | current loss  0.09777800
INFO:src.model.validator:reward training epoch 595 | current loss  0.09777407
INFO:src.model.validator:reward training epoch 596 | current loss  0.09777021
INFO:src.model.validator:reward training epoch 597 | current loss  0.09776638
INFO:src.model.validator:reward training epoch 598 | current loss  0.09776258
INFO:src.model.validator:reward training epoch 599 | current loss  0.09775883
INFO:src.model.validator:reward training epoch 600 | current loss  0.09775511
INFO:src.model.validator:reward training epoch 601 | current loss  0.09775142
INFO:src.model.validator:reward training epoch 602 | current loss  0.09774778
INFO:src.model.validator:reward training epoch 603 | current loss  0.09774417
INFO:src.model.validator:reward training epoch 604 | current loss  0.09774061
INFO:src.model.validator:reward training epoch 605 | current loss  0.09773705
INFO:src.model.validator:reward training epoch 606 | current loss  0.09773355
INFO:src.model.validator:reward training epoch 607 | current loss  0.09773009
INFO:src.model.validator:reward training epoch 608 | current loss  0.09772665
INFO:src.model.validator:reward training epoch 609 | current loss  0.09772327
INFO:src.model.validator:reward training epoch 610 | current loss  0.09771989
INFO:src.model.validator:reward training epoch 611 | current loss  0.09771655
INFO:src.model.validator:reward training epoch 612 | current loss  0.09771326
INFO:src.model.validator:reward training epoch 613 | current loss  0.09770998
INFO:src.model.validator:reward training epoch 614 | current loss  0.09770676
INFO:src.model.validator:reward training epoch 615 | current loss  0.09770356
INFO:src.model.validator:reward training epoch 616 | current loss  0.09770037
INFO:src.model.validator:reward training epoch 617 | current loss  0.09769725
INFO:src.model.validator:reward training epoch 618 | current loss  0.09769414
INFO:src.model.validator:reward training epoch 619 | current loss  0.09769107
INFO:src.model.validator:reward training epoch 620 | current loss  0.09768801
INFO:src.model.validator:reward training epoch 621 | current loss  0.09768501
INFO:src.model.validator:reward training epoch 622 | current loss  0.09768201
INFO:src.model.validator:reward training epoch 623 | current loss  0.09767906
INFO:src.model.validator:reward training epoch 624 | current loss  0.09767614
INFO:src.model.validator:reward training epoch 625 | current loss  0.09767324
INFO:src.model.validator:reward training epoch 626 | current loss  0.09767038
INFO:src.model.validator:reward training epoch 627 | current loss  0.09766755
INFO:src.model.validator:reward training epoch 628 | current loss  0.09766471
INFO:src.model.validator:reward training epoch 629 | current loss  0.09766193
INFO:src.model.validator:reward training epoch 630 | current loss  0.09765919
INFO:src.model.validator:reward training epoch 631 | current loss  0.09765647
INFO:src.model.validator:reward training epoch 632 | current loss  0.09765375
INFO:src.model.validator:reward training epoch 633 | current loss  0.09765108
INFO:src.model.validator:reward training epoch 634 | current loss  0.09764843
INFO:src.model.validator:reward training epoch 635 | current loss  0.09764580
INFO:src.model.validator:reward training epoch 636 | current loss  0.09764320
INFO:src.model.validator:reward training epoch 637 | current loss  0.09764064
INFO:src.model.validator:reward training epoch 638 | current loss  0.09763809
INFO:src.model.validator:reward training epoch 639 | current loss  0.09763557
INFO:src.model.validator:reward training epoch 640 | current loss  0.09763309
INFO:src.model.validator:reward training epoch 641 | current loss  0.09763063
INFO:src.model.validator:reward training epoch 642 | current loss  0.09762818
INFO:src.model.validator:reward training epoch 643 | current loss  0.09762575
INFO:src.model.validator:reward training epoch 644 | current loss  0.09762336
INFO:src.model.validator:reward training epoch 645 | current loss  0.09762098
INFO:src.model.validator:reward training epoch 646 | current loss  0.09761863
INFO:src.model.validator:reward training epoch 647 | current loss  0.09761630
INFO:src.model.validator:reward training epoch 648 | current loss  0.09761401
INFO:src.model.validator:reward training epoch 649 | current loss  0.09761172
INFO:src.model.validator:reward training epoch 650 | current loss  0.09760947
INFO:src.model.validator:reward training epoch 651 | current loss  0.09760725
INFO:src.model.validator:reward training epoch 652 | current loss  0.09760502
INFO:src.model.validator:reward training epoch 653 | current loss  0.09760283
INFO:src.model.validator:reward training epoch 654 | current loss  0.09760067
INFO:src.model.validator:reward training epoch 655 | current loss  0.09759852
INFO:src.model.validator:reward training epoch 656 | current loss  0.09759640
INFO:src.model.validator:reward training epoch 657 | current loss  0.09759430
INFO:src.model.validator:reward training epoch 658 | current loss  0.09759221
INFO:src.model.validator:reward training epoch 659 | current loss  0.09759016
INFO:src.model.validator:reward training epoch 660 | current loss  0.09758811
INFO:src.model.validator:reward training epoch 661 | current loss  0.09758608
INFO:src.model.validator:reward training epoch 662 | current loss  0.09758408
INFO:src.model.validator:reward training epoch 663 | current loss  0.09758209
INFO:src.model.validator:reward training epoch 664 | current loss  0.09758013
INFO:src.model.validator:reward training epoch 665 | current loss  0.09757821
INFO:src.model.validator:reward training epoch 666 | current loss  0.09757626
INFO:src.model.validator:reward training epoch 667 | current loss  0.09757438
INFO:src.model.validator:reward training epoch 668 | current loss  0.09757249
INFO:src.model.validator:reward training epoch 669 | current loss  0.09757061
INFO:src.model.validator:reward training epoch 670 | current loss  0.09756877
INFO:src.model.validator:reward training epoch 671 | current loss  0.09756694
INFO:src.model.validator:reward training epoch 672 | current loss  0.09756512
INFO:src.model.validator:reward training epoch 673 | current loss  0.09756334
INFO:src.model.validator:reward training epoch 674 | current loss  0.09756157
INFO:src.model.validator:reward training epoch 675 | current loss  0.09755979
INFO:src.model.validator:reward training epoch 676 | current loss  0.09755805
INFO:src.model.validator:reward training epoch 677 | current loss  0.09755632
INFO:src.model.validator:reward training epoch 678 | current loss  0.09755463
INFO:src.model.validator:reward training epoch 679 | current loss  0.09755294
INFO:src.model.validator:reward training epoch 680 | current loss  0.09755127
INFO:src.model.validator:reward training epoch 681 | current loss  0.09754961
INFO:src.model.validator:reward training epoch 682 | current loss  0.09754798
INFO:src.model.validator:reward training epoch 683 | current loss  0.09754635
INFO:src.model.validator:reward training epoch 684 | current loss  0.09754474
INFO:src.model.validator:reward training epoch 685 | current loss  0.09754315
INFO:src.model.validator:reward training epoch 686 | current loss  0.09754159
INFO:src.model.validator:reward training epoch 687 | current loss  0.09754002
INFO:src.model.validator:reward training epoch 688 | current loss  0.09753847
INFO:src.model.validator:reward training epoch 689 | current loss  0.09753695
INFO:src.model.validator:reward training epoch 690 | current loss  0.09753544
INFO:src.model.validator:reward training epoch 691 | current loss  0.09753393
INFO:src.model.validator:reward training epoch 692 | current loss  0.09753246
INFO:src.model.validator:reward training epoch 693 | current loss  0.09753098
INFO:src.model.validator:reward training epoch 694 | current loss  0.09752954
INFO:src.model.validator:reward training epoch 695 | current loss  0.09752809
INFO:src.model.validator:reward training epoch 696 | current loss  0.09752666
INFO:src.model.validator:reward training epoch 697 | current loss  0.09752525
INFO:src.model.validator:reward training epoch 698 | current loss  0.09752386
INFO:src.model.validator:reward training epoch 699 | current loss  0.09752247
INFO:src.model.validator:reward training epoch 700 | current loss  0.09752110
INFO:src.model.validator:reward training epoch 701 | current loss  0.09751975
INFO:src.model.validator:reward training epoch 702 | current loss  0.09751839
INFO:src.model.validator:reward training epoch 703 | current loss  0.09751707
INFO:src.model.validator:reward training epoch 704 | current loss  0.09751577
INFO:src.model.validator:reward training epoch 705 | current loss  0.09751445
INFO:src.model.validator:reward training epoch 706 | current loss  0.09751316
INFO:src.model.validator:reward training epoch 707 | current loss  0.09751190
INFO:src.model.validator:reward training epoch 708 | current loss  0.09751063
INFO:src.model.validator:reward training epoch 709 | current loss  0.09750938
INFO:src.model.validator:reward training epoch 710 | current loss  0.09750814
INFO:src.model.validator:reward training epoch 711 | current loss  0.09750690
INFO:src.model.validator:reward training epoch 712 | current loss  0.09750570
INFO:src.model.validator:reward training epoch 713 | current loss  0.09750449
INFO:src.model.validator:reward training epoch 714 | current loss  0.09750330
INFO:src.model.validator:reward training epoch 715 | current loss  0.09750211
INFO:src.model.validator:reward training epoch 716 | current loss  0.09750096
INFO:src.model.validator:reward training epoch 717 | current loss  0.09749980
INFO:src.model.validator:reward training epoch 718 | current loss  0.09749866
INFO:src.model.validator:reward training epoch 719 | current loss  0.09749752
INFO:src.model.validator:reward training epoch 720 | current loss  0.09749639
INFO:src.model.validator:reward training epoch 721 | current loss  0.09749530
INFO:src.model.validator:reward training epoch 722 | current loss  0.09749419
INFO:src.model.validator:reward training epoch 723 | current loss  0.09749310
INFO:src.model.validator:reward training epoch 724 | current loss  0.09749202
INFO:src.model.validator:reward training epoch 725 | current loss  0.09749096
INFO:src.model.validator:reward training epoch 726 | current loss  0.09748989
INFO:src.model.validator:reward training epoch 727 | current loss  0.09748886
INFO:src.model.validator:reward training epoch 728 | current loss  0.09748781
INFO:src.model.validator:reward training epoch 729 | current loss  0.09748679
INFO:src.model.validator:reward training epoch 730 | current loss  0.09748578
INFO:src.model.validator:reward training epoch 731 | current loss  0.09748477
INFO:src.model.validator:reward training epoch 732 | current loss  0.09748378
INFO:src.model.validator:reward training epoch 733 | current loss  0.09748279
INFO:src.model.validator:reward training epoch 734 | current loss  0.09748182
INFO:src.model.validator:reward training epoch 735 | current loss  0.09748085
INFO:src.model.validator:reward training epoch 736 | current loss  0.09747989
INFO:src.model.validator:reward training epoch 737 | current loss  0.09747894
INFO:src.model.validator:reward training epoch 738 | current loss  0.09747802
INFO:src.model.validator:reward training epoch 739 | current loss  0.09747708
INFO:src.model.validator:reward training epoch 740 | current loss  0.09747616
INFO:src.model.validator:reward training epoch 741 | current loss  0.09747526
INFO:src.model.validator:reward training epoch 742 | current loss  0.09747436
INFO:src.model.validator:reward training epoch 743 | current loss  0.09747346
INFO:src.model.validator:reward training epoch 744 | current loss  0.09747258
INFO:src.model.validator:reward training epoch 745 | current loss  0.09747171
INFO:src.model.validator:reward training epoch 746 | current loss  0.09747084
INFO:src.model.validator:reward training epoch 747 | current loss  0.09746999
INFO:src.model.validator:reward training epoch 748 | current loss  0.09746913
INFO:src.model.validator:reward training epoch 749 | current loss  0.09746830
INFO:src.model.validator:reward training epoch 750 | current loss  0.09746747
INFO:src.model.validator:reward training epoch 751 | current loss  0.09746664
INFO:src.model.validator:reward training epoch 752 | current loss  0.09746583
INFO:src.model.validator:reward training epoch 753 | current loss  0.09746502
INFO:src.model.validator:reward training epoch 754 | current loss  0.09746421
INFO:src.model.validator:reward training epoch 755 | current loss  0.09746344
INFO:src.model.validator:reward training epoch 756 | current loss  0.09746265
INFO:src.model.validator:reward training epoch 757 | current loss  0.09746188
INFO:src.model.validator:reward training epoch 758 | current loss  0.09746110
INFO:src.model.validator:reward training epoch 759 | current loss  0.09746035
INFO:src.model.validator:reward training epoch 760 | current loss  0.09745959
INFO:src.model.validator:reward training epoch 761 | current loss  0.09745885
INFO:src.model.validator:reward training epoch 762 | current loss  0.09745812
INFO:src.model.validator:reward training epoch 763 | current loss  0.09745738
INFO:src.model.validator:reward training epoch 764 | current loss  0.09745667
INFO:src.model.validator:reward training epoch 765 | current loss  0.09745595
INFO:src.model.validator:reward training epoch 766 | current loss  0.09745523
INFO:src.model.validator:reward training epoch 767 | current loss  0.09745454
INFO:src.model.validator:reward training epoch 768 | current loss  0.09745383
INFO:src.model.validator:reward training epoch 769 | current loss  0.09745316
INFO:src.model.validator:reward training epoch 770 | current loss  0.09745247
INFO:src.model.validator:reward training epoch 771 | current loss  0.09745180
INFO:src.model.validator:reward training epoch 772 | current loss  0.09745114
INFO:src.model.validator:reward training epoch 773 | current loss  0.09745047
INFO:src.model.validator:reward training epoch 774 | current loss  0.09744981
INFO:src.model.validator:reward training epoch 775 | current loss  0.09744916
INFO:src.model.validator:reward training epoch 776 | current loss  0.09744854
INFO:src.model.validator:reward training epoch 777 | current loss  0.09744789
INFO:src.model.validator:reward training epoch 778 | current loss  0.09744727
INFO:src.model.validator:reward training epoch 779 | current loss  0.09744664
INFO:src.model.validator:reward training epoch 780 | current loss  0.09744602
INFO:src.model.validator:reward training epoch 781 | current loss  0.09744541
INFO:src.model.validator:reward training epoch 782 | current loss  0.09744480
INFO:src.model.validator:reward training epoch 783 | current loss  0.09744422
INFO:src.model.validator:reward training epoch 784 | current loss  0.09744361
INFO:src.model.validator:reward training epoch 785 | current loss  0.09744304
INFO:src.model.validator:reward training epoch 786 | current loss  0.09744246
INFO:src.model.validator:reward training epoch 787 | current loss  0.09744187
INFO:src.model.validator:reward training epoch 788 | current loss  0.09744132
INFO:src.model.validator:reward training epoch 789 | current loss  0.09744074
INFO:src.model.validator:reward training epoch 790 | current loss  0.09744018
INFO:src.model.validator:reward training epoch 791 | current loss  0.09743965
INFO:src.model.validator:reward training epoch 792 | current loss  0.09743910
INFO:src.model.validator:reward training epoch 793 | current loss  0.09743855
INFO:src.model.validator:reward training epoch 794 | current loss  0.09743802
INFO:src.model.validator:reward training epoch 795 | current loss  0.09743749
INFO:src.model.validator:reward training epoch 796 | current loss  0.09743696
INFO:src.model.validator:reward training epoch 797 | current loss  0.09743644
INFO:src.model.validator:reward training epoch 798 | current loss  0.09743592
INFO:src.model.validator:reward training epoch 799 | current loss  0.09743541
INFO:src.model.validator:reward training epoch 800 | current loss  0.09743491
INFO:src.model.validator:reward training epoch 801 | current loss  0.09743441
INFO:src.model.validator:reward training epoch 802 | current loss  0.09743392
INFO:src.model.validator:reward training epoch 803 | current loss  0.09743343
INFO:src.model.validator:reward training epoch 804 | current loss  0.09743295
INFO:src.model.validator:reward training epoch 805 | current loss  0.09743246
INFO:src.model.validator:reward training epoch 806 | current loss  0.09743199
INFO:src.model.validator:reward training epoch 807 | current loss  0.09743150
INFO:src.model.validator:reward training epoch 808 | current loss  0.09743106
INFO:src.model.validator:reward training epoch 809 | current loss  0.09743059
INFO:src.model.validator:reward training epoch 810 | current loss  0.09743014
INFO:src.model.validator:reward training epoch 811 | current loss  0.09742969
INFO:src.model.validator:reward training epoch 812 | current loss  0.09742924
INFO:src.model.validator:reward training epoch 813 | current loss  0.09742880
INFO:src.model.validator:reward training epoch 814 | current loss  0.09742835
INFO:src.model.validator:reward training epoch 815 | current loss  0.09742790
INFO:src.model.validator:reward training epoch 816 | current loss  0.09742749
INFO:src.model.validator:reward training epoch 817 | current loss  0.09742706
INFO:src.model.validator:reward training epoch 818 | current loss  0.09742665
INFO:src.model.validator:reward training epoch 819 | current loss  0.09742623
INFO:src.model.validator:reward training epoch 820 | current loss  0.09742581
INFO:src.model.validator:reward training epoch 821 | current loss  0.09742540
INFO:src.model.validator:reward training epoch 822 | current loss  0.09742498
INFO:src.model.validator:reward training epoch 823 | current loss  0.09742460
INFO:src.model.validator:reward training epoch 824 | current loss  0.09742419
INFO:src.model.validator:reward training epoch 825 | current loss  0.09742380
INFO:src.model.validator:reward training epoch 826 | current loss  0.09742341
INFO:src.model.validator:reward training epoch 827 | current loss  0.09742304
INFO:src.model.validator:reward training epoch 828 | current loss  0.09742264
INFO:src.model.validator:reward training epoch 829 | current loss  0.09742227
INFO:src.model.validator:reward training epoch 830 | current loss  0.09742189
INFO:src.model.validator:reward training epoch 831 | current loss  0.09742154
INFO:src.model.validator:reward training epoch 832 | current loss  0.09742116
INFO:src.model.validator:reward training epoch 833 | current loss  0.09742080
INFO:src.model.validator:reward training epoch 834 | current loss  0.09742043
INFO:src.model.validator:reward training epoch 835 | current loss  0.09742008
INFO:src.model.validator:reward training epoch 836 | current loss  0.09741972
INFO:src.model.validator:reward training epoch 837 | current loss  0.09741937
INFO:src.model.validator:reward training epoch 838 | current loss  0.09741903
INFO:src.model.validator:reward training epoch 839 | current loss  0.09741870
INFO:src.model.validator:reward training epoch 840 | current loss  0.09741836
INFO:src.model.validator:reward training epoch 841 | current loss  0.09741803
INFO:src.model.validator:reward training epoch 842 | current loss  0.09741769
INFO:src.model.validator:reward training epoch 843 | current loss  0.09741735
INFO:src.model.validator:reward training epoch 844 | current loss  0.09741703
INFO:src.model.validator:reward training epoch 845 | current loss  0.09741670
INFO:src.model.validator:reward training epoch 846 | current loss  0.09741639
INFO:src.model.validator:reward training epoch 847 | current loss  0.09741607
INFO:src.model.validator:reward training epoch 848 | current loss  0.09741577
INFO:src.model.validator:reward training epoch 849 | current loss  0.09741545
INFO:src.model.validator:reward training epoch 850 | current loss  0.09741514
INFO:src.model.validator:reward training epoch 851 | current loss  0.09741484
INFO:src.model.validator:reward training epoch 852 | current loss  0.09741454
INFO:src.model.validator:reward training epoch 853 | current loss  0.09741425
INFO:src.model.validator:reward training epoch 854 | current loss  0.09741396
INFO:src.model.validator:reward training epoch 855 | current loss  0.09741366
INFO:src.model.validator:reward training epoch 856 | current loss  0.09741336
INFO:src.model.validator:reward training epoch 857 | current loss  0.09741309
INFO:src.model.validator:reward training epoch 858 | current loss  0.09741280
INFO:src.model.validator:reward training epoch 859 | current loss  0.09741253
INFO:src.model.validator:reward training epoch 860 | current loss  0.09741225
INFO:src.model.validator:reward training epoch 861 | current loss  0.09741198
INFO:src.model.validator:reward training epoch 862 | current loss  0.09741171
INFO:src.model.validator:reward training epoch 863 | current loss  0.09741144
INFO:src.model.validator:reward training epoch 864 | current loss  0.09741116
INFO:src.model.validator:reward training epoch 865 | current loss  0.09741089
INFO:src.model.validator:reward training epoch 866 | current loss  0.09741064
INFO:src.model.validator:reward training epoch 867 | current loss  0.09741037
INFO:src.model.validator:reward training epoch 868 | current loss  0.09741012
INFO:src.model.validator:reward training epoch 869 | current loss  0.09740988
INFO:src.model.validator:reward training epoch 870 | current loss  0.09740961
INFO:src.model.validator:reward training epoch 871 | current loss  0.09740937
INFO:src.model.validator:reward training epoch 872 | current loss  0.09740912
INFO:src.model.validator:reward training epoch 873 | current loss  0.09740888
INFO:src.model.validator:reward training epoch 874 | current loss  0.09740862
INFO:src.model.validator:reward training epoch 875 | current loss  0.09740840
INFO:src.model.validator:reward training epoch 876 | current loss  0.09740816
INFO:src.model.validator:reward training epoch 877 | current loss  0.09740794
INFO:src.model.validator:reward training epoch 878 | current loss  0.09740770
INFO:src.model.validator:reward training epoch 879 | current loss  0.09740748
INFO:src.model.validator:reward training epoch 880 | current loss  0.09740724
INFO:src.model.validator:reward training epoch 881 | current loss  0.09740702
INFO:src.model.validator:reward training epoch 882 | current loss  0.09740680
INFO:src.model.validator:reward training epoch 883 | current loss  0.09740657
INFO:src.model.validator:reward training epoch 884 | current loss  0.09740636
INFO:src.model.validator:reward training epoch 885 | current loss  0.09740615
INFO:src.model.validator:reward training epoch 886 | current loss  0.09740592
INFO:src.model.validator:reward training epoch 887 | current loss  0.09740571
INFO:src.model.validator:reward training epoch 888 | current loss  0.09740550
INFO:src.model.validator:reward training epoch 889 | current loss  0.09740529
INFO:src.model.validator:reward training epoch 890 | current loss  0.09740509
INFO:src.model.validator:reward training epoch 891 | current loss  0.09740490
INFO:src.model.validator:reward training epoch 892 | current loss  0.09740469
INFO:src.model.validator:reward training epoch 893 | current loss  0.09740449
INFO:src.model.validator:reward training epoch 894 | current loss  0.09740429
INFO:src.model.validator:reward training epoch 895 | current loss  0.09740409
INFO:src.model.validator:reward training epoch 896 | current loss  0.09740391
INFO:src.model.validator:reward training epoch 897 | current loss  0.09740372
INFO:src.model.validator:reward training epoch 898 | current loss  0.09740352
INFO:src.model.validator:reward training epoch 899 | current loss  0.09740333
INFO:src.model.validator:reward training epoch 900 | current loss  0.09740315
INFO:src.model.validator:reward training epoch 901 | current loss  0.09740296
INFO:src.model.validator:reward training epoch 902 | current loss  0.09740277
INFO:src.model.validator:reward training epoch 903 | current loss  0.09740259
INFO:src.model.validator:reward training epoch 904 | current loss  0.09740242
INFO:src.model.validator:reward training epoch 905 | current loss  0.09740224
INFO:src.model.validator:reward training epoch 906 | current loss  0.09740207
INFO:src.model.validator:reward training epoch 907 | current loss  0.09740191
INFO:src.model.validator:reward training epoch 908 | current loss  0.09740173
INFO:src.model.validator:reward training epoch 909 | current loss  0.09740155
INFO:src.model.validator:reward training epoch 910 | current loss  0.09740139
INFO:src.model.validator:reward training epoch 911 | current loss  0.09740123
INFO:src.model.validator:reward training epoch 912 | current loss  0.09740104
INFO:src.model.validator:reward training epoch 913 | current loss  0.09740090
INFO:src.model.validator:reward training epoch 914 | current loss  0.09740072
INFO:src.model.validator:reward training epoch 915 | current loss  0.09740057
INFO:src.model.validator:reward training epoch 916 | current loss  0.09740042
INFO:src.model.validator:reward training epoch 917 | current loss  0.09740026
INFO:src.model.validator:reward training epoch 918 | current loss  0.09740010
INFO:src.model.validator:reward training epoch 919 | current loss  0.09739994
INFO:src.model.validator:reward training epoch 920 | current loss  0.09739980
INFO:src.model.validator:reward training epoch 921 | current loss  0.09739963
INFO:src.model.validator:reward training epoch 922 | current loss  0.09739949
INFO:src.model.validator:reward training epoch 923 | current loss  0.09739934
INFO:src.model.validator:reward training epoch 924 | current loss  0.09739920
INFO:src.model.validator:reward training epoch 925 | current loss  0.09739905
INFO:src.model.validator:reward training epoch 926 | current loss  0.09739891
INFO:src.model.validator:reward training epoch 927 | current loss  0.09739878
INFO:src.model.validator:reward training epoch 928 | current loss  0.09739864
INFO:src.model.validator:reward training epoch 929 | current loss  0.09739849
INFO:src.model.validator:reward training epoch 930 | current loss  0.09739835
INFO:src.model.validator:reward training epoch 931 | current loss  0.09739821
INFO:src.model.validator:reward training epoch 932 | current loss  0.09739807
INFO:src.model.validator:reward training epoch 933 | current loss  0.09739795
INFO:src.model.validator:reward training epoch 934 | current loss  0.09739780
INFO:src.model.validator:reward training epoch 935 | current loss  0.09739768
INFO:src.model.validator:reward training epoch 936 | current loss  0.09739755
INFO:src.model.validator:reward training epoch 937 | current loss  0.09739742
INFO:src.model.validator:reward training epoch 938 | current loss  0.09739730
INFO:src.model.validator:reward training epoch 939 | current loss  0.09739717
INFO:src.model.validator:reward training epoch 940 | current loss  0.09739704
INFO:src.model.validator:reward training epoch 941 | current loss  0.09739692
INFO:src.model.validator:reward training epoch 942 | current loss  0.09739680
INFO:src.model.validator:reward training epoch 943 | current loss  0.09739667
INFO:src.model.validator:reward training epoch 944 | current loss  0.09739655
INFO:src.model.validator:reward training epoch 945 | current loss  0.09739645
INFO:src.model.validator:reward training epoch 946 | current loss  0.09739633
INFO:src.model.validator:reward training epoch 947 | current loss  0.09739620
INFO:src.model.validator:reward training epoch 948 | current loss  0.09739609
INFO:src.model.validator:reward training epoch 949 | current loss  0.09739598
INFO:src.model.validator:reward training epoch 950 | current loss  0.09739585
INFO:src.model.validator:reward training epoch 951 | current loss  0.09739575
INFO:src.model.validator:reward training epoch 952 | current loss  0.09739564
INFO:src.model.validator:reward training epoch 953 | current loss  0.09739552
INFO:src.model.validator:reward training epoch 954 | current loss  0.09739542
INFO:src.model.validator:reward training epoch 955 | current loss  0.09739530
INFO:src.model.validator:reward training epoch 956 | current loss  0.09739520
INFO:src.model.validator:reward training epoch 957 | current loss  0.09739511
INFO:src.model.validator:reward training epoch 958 | current loss  0.09739499
INFO:src.model.validator:reward training epoch 959 | current loss  0.09739491
INFO:src.model.validator:reward training epoch 960 | current loss  0.09739480
INFO:src.model.validator:reward training epoch 961 | current loss  0.09739469
INFO:src.model.validator:reward training epoch 962 | current loss  0.09739459
INFO:src.model.validator:reward training epoch 963 | current loss  0.09739450
INFO:src.model.validator:reward training epoch 964 | current loss  0.09739440
INFO:src.model.validator:reward training epoch 965 | current loss  0.09739430
INFO:src.model.validator:reward training epoch 966 | current loss  0.09739420
INFO:src.model.validator:reward training epoch 967 | current loss  0.09739412
INFO:src.model.validator:reward training epoch 968 | current loss  0.09739402
INFO:src.model.validator:reward training epoch 969 | current loss  0.09739392
INFO:src.model.validator:reward training epoch 970 | current loss  0.09739384
INFO:src.model.validator:reward training epoch 971 | current loss  0.09739373
INFO:src.model.validator:reward training epoch 972 | current loss  0.09739365
INFO:src.model.validator:reward training epoch 973 | current loss  0.09739356
INFO:src.model.validator:reward training epoch 974 | current loss  0.09739347
INFO:src.model.validator:reward training epoch 975 | current loss  0.09739339
INFO:src.model.validator:reward training epoch 976 | current loss  0.09739330
INFO:src.model.validator:reward training epoch 977 | current loss  0.09739319
INFO:src.model.validator:reward training epoch 978 | current loss  0.09739312
INFO:src.model.validator:reward training epoch 979 | current loss  0.09739304
INFO:src.model.validator:reward training epoch 980 | current loss  0.09739295
INFO:src.model.validator:reward training epoch 981 | current loss  0.09739286
INFO:src.model.validator:reward training epoch 982 | current loss  0.09739278
INFO:src.model.validator:reward training epoch 983 | current loss  0.09739270
INFO:src.model.validator:reward training epoch 984 | current loss  0.09739263
INFO:src.model.validator:reward training epoch 985 | current loss  0.09739254
INFO:src.model.validator:reward training epoch 986 | current loss  0.09739247
INFO:src.model.validator:reward training epoch 987 | current loss  0.09739240
INFO:src.model.validator:reward training epoch 988 | current loss  0.09739231
INFO:src.model.validator:reward training epoch 989 | current loss  0.09739223
INFO:src.model.validator:reward training epoch 990 | current loss  0.09739216
INFO:src.model.validator:reward training epoch 991 | current loss  0.09739208
INFO:src.model.validator:reward training epoch 992 | current loss  0.09739202
INFO:src.model.validator:reward training epoch 993 | current loss  0.09739194
INFO:src.model.validator:reward training epoch 994 | current loss  0.09739187
INFO:src.model.validator:reward training epoch 995 | current loss  0.09739179
INFO:src.model.validator:reward training epoch 996 | current loss  0.09739172
INFO:src.model.validator:reward training epoch 997 | current loss  0.09739164
INFO:src.model.validator:reward training epoch 998 | current loss  0.09739158
INFO:src.model.validator:reward training epoch 999 | current loss  0.09739152
INFO:src.model.validator:reward training complete | current loss  0.0974
-- validator reward test result --
confusion metrix
 [[19 20]
 [14 27]]
accuracy: 0.575
report
               precision    recall  f1-score   support

         0.0       0.58      0.49      0.53        39
         1.0       0.57      0.66      0.61        41

    accuracy                           0.57        80
   macro avg       0.58      0.57      0.57        80
weighted avg       0.58      0.57      0.57        80

INFO:src.model.crystalisland:50000 out of 145752 random data generated
INFO:src.model.crystalisland:100000 out of 145752 random data generated
INFO:src.model.crystalisland:finished creating random samples
INFO:src.model.crystalisland:finished creating random samples
INFO:src.model.validator:auth training epoch 0 | current loss  0.23422766
INFO:src.model.validator:auth training epoch 1 | current loss  0.20371471
INFO:src.model.validator:auth training epoch 2 | current loss  0.17752011
INFO:src.model.validator:auth training epoch 3 | current loss  0.15518324
INFO:src.model.validator:auth training epoch 4 | current loss  0.13621902
INFO:src.model.validator:auth training epoch 5 | current loss  0.12015907
INFO:src.model.validator:auth training epoch 6 | current loss  0.10644768
INFO:src.model.validator:auth training epoch 7 | current loss  0.09463812
INFO:src.model.validator:auth training epoch 8 | current loss  0.08445178
INFO:src.model.validator:auth training epoch 9 | current loss  0.07565968
INFO:src.model.validator:auth training epoch 10 | current loss  0.06804243
INFO:src.model.validator:auth training epoch 11 | current loss  0.06144315
INFO:src.model.validator:auth training epoch 12 | current loss  0.05575291
INFO:src.model.validator:auth training epoch 13 | current loss  0.05086118
INFO:src.model.validator:auth training epoch 14 | current loss  0.04666069
INFO:src.model.validator:auth training epoch 15 | current loss  0.04305873
INFO:src.model.validator:auth training epoch 16 | current loss  0.03997003
INFO:src.model.validator:auth training epoch 17 | current loss  0.03731137
INFO:src.model.validator:auth training epoch 18 | current loss  0.03500925
INFO:src.model.validator:auth training epoch 19 | current loss  0.03300677
INFO:src.model.validator:auth training epoch 20 | current loss  0.03125551
INFO:src.model.validator:auth training epoch 21 | current loss  0.02971510
INFO:src.model.validator:auth training epoch 22 | current loss  0.02835232
INFO:src.model.validator:auth training epoch 23 | current loss  0.02713890
INFO:src.model.validator:auth training epoch 24 | current loss  0.02605128
INFO:src.model.validator:auth training epoch 25 | current loss  0.02507003
INFO:src.model.validator:auth training epoch 26 | current loss  0.02417953
INFO:src.model.validator:auth training epoch 27 | current loss  0.02336719
INFO:src.model.validator:auth training epoch 28 | current loss  0.02262244
INFO:src.model.validator:auth training epoch 29 | current loss  0.02193628
INFO:src.model.validator:auth training epoch 30 | current loss  0.02130096
INFO:src.model.validator:auth training epoch 31 | current loss  0.02070999
INFO:src.model.validator:auth training epoch 32 | current loss  0.02015795
INFO:src.model.validator:auth training epoch 33 | current loss  0.01964036
INFO:src.model.validator:auth training epoch 34 | current loss  0.01915346
INFO:src.model.validator:auth training epoch 35 | current loss  0.01869421
INFO:src.model.validator:auth training epoch 36 | current loss  0.01826004
INFO:src.model.validator:auth training epoch 37 | current loss  0.01784877
INFO:src.model.validator:auth training epoch 38 | current loss  0.01745848
INFO:src.model.validator:auth training epoch 39 | current loss  0.01708752
INFO:src.model.validator:auth training epoch 40 | current loss  0.01673445
INFO:src.model.validator:auth training epoch 41 | current loss  0.01639810
INFO:src.model.validator:auth training epoch 42 | current loss  0.01607751
INFO:src.model.validator:auth training epoch 43 | current loss  0.01577168
INFO:src.model.validator:auth training epoch 44 | current loss  0.01547945
INFO:src.model.validator:auth training epoch 45 | current loss  0.01519997
INFO:src.model.validator:auth training epoch 46 | current loss  0.01493255
INFO:src.model.validator:auth training epoch 47 | current loss  0.01467648
INFO:src.model.validator:auth training epoch 48 | current loss  0.01443113
INFO:src.model.validator:auth training epoch 49 | current loss  0.01419593
INFO:src.model.validator:auth training epoch 50 | current loss  0.01397032
INFO:src.model.validator:auth training epoch 51 | current loss  0.01375371
INFO:src.model.validator:auth training epoch 52 | current loss  0.01354557
INFO:src.model.validator:auth training epoch 53 | current loss  0.01334540
INFO:src.model.validator:auth training epoch 54 | current loss  0.01315275
INFO:src.model.validator:auth training epoch 55 | current loss  0.01296714
INFO:src.model.validator:auth training epoch 56 | current loss  0.01278816
INFO:src.model.validator:auth training epoch 57 | current loss  0.01261550
INFO:src.model.validator:auth training epoch 58 | current loss  0.01244891
INFO:src.model.validator:auth training epoch 59 | current loss  0.01228814
INFO:src.model.validator:auth training epoch 60 | current loss  0.01213294
INFO:src.model.validator:auth training epoch 61 | current loss  0.01198310
INFO:src.model.validator:auth training epoch 62 | current loss  0.01183836
INFO:src.model.validator:auth training epoch 63 | current loss  0.01169834
INFO:src.model.validator:auth training epoch 64 | current loss  0.01156269
INFO:src.model.validator:auth training epoch 65 | current loss  0.01143115
INFO:src.model.validator:auth training epoch 66 | current loss  0.01130356
INFO:src.model.validator:auth training epoch 67 | current loss  0.01117975
INFO:src.model.validator:auth training epoch 68 | current loss  0.01105960
INFO:src.model.validator:auth training epoch 69 | current loss  0.01094299
INFO:src.model.validator:auth training epoch 70 | current loss  0.01082983
INFO:src.model.validator:auth training epoch 71 | current loss  0.01072001
INFO:src.model.validator:auth training epoch 72 | current loss  0.01061338
INFO:src.model.validator:auth training epoch 73 | current loss  0.01050980
INFO:src.model.validator:auth training epoch 74 | current loss  0.01040912
INFO:src.model.validator:auth training epoch 75 | current loss  0.01031119
INFO:src.model.validator:auth training epoch 76 | current loss  0.01021591
INFO:src.model.validator:auth training epoch 77 | current loss  0.01012317
INFO:src.model.validator:auth training epoch 78 | current loss  0.01003290
INFO:src.model.validator:auth training epoch 79 | current loss  0.00994501
INFO:src.model.validator:auth training epoch 80 | current loss  0.00985942
INFO:src.model.validator:auth training epoch 81 | current loss  0.00977604
INFO:src.model.validator:auth training epoch 82 | current loss  0.00969481
INFO:src.model.validator:auth training epoch 83 | current loss  0.00961565
INFO:src.model.validator:auth training epoch 84 | current loss  0.00953848
INFO:src.model.validator:auth training epoch 85 | current loss  0.00946325
INFO:src.model.validator:auth training epoch 86 | current loss  0.00938990
INFO:src.model.validator:auth training epoch 87 | current loss  0.00931841
INFO:src.model.validator:auth training epoch 88 | current loss  0.00924875
INFO:src.model.validator:auth training epoch 89 | current loss  0.00918089
INFO:src.model.validator:auth training epoch 90 | current loss  0.00911482
INFO:src.model.validator:auth training complete | current loss  0.0091
-- validator reward test result --
confusion metrix
 [[36409    38]
 [  718 35729]]
accuracy: 0.989628776030949
report
               precision    recall  f1-score   support

         0.0       0.98      1.00      0.99     36447
         1.0       1.00      0.98      0.99     36447

    accuracy                           0.99     72894
   macro avg       0.99      0.99      0.99     72894
weighted avg       0.99      0.99      0.99     72894

INFO:src.model.validator:validator models saved!
INFO:src.model.gail:args: {
  "seed": 1,
  "run_name": "seed_1",
  "gail_train_steps": 1000,
  "dryrun": false,
  "load_validator": false,
  "load_gail": false,
  "load_sim": true,
  "debug": false,
  "load_fqe": true,
  "load_bc": false,
  "state_dim": 27,
  "action_dim": 19,
  "is_random_planner": true,
  "student_data_loc": "../processed_data/student_trajectories.pkl",
  "narrative_data_loc": "../processed_data/narrative_trajectories.pkl",
  "device": "cpu",
  "units": 128,
  "lr_actor": 0.001,
  "lr_critic": 0.001,
  "lr_discriminator": 0.001,
  "scheduler_gamma": 0.99,
  "internal_epoch_pi": 20,
  "internal_epoch_d": 2,
  "discount_factor": 0.99,
  "clip_eps": 0.1,
  "max_episode_len": 1000,
  "update_steps": 1000,
  "simulate_episodes": 1000,
  "lr_validator": 0.001,
  "validator_train_steps": 1000,
  "validator_batch": 1000,
  "validator_auth_threshold": 0.95,
  "bcq_batch": 1000,
  "lr_bcq": 0.001,
  "bcq_threshold": 0.1,
  "bcq_train_steps": 100000,
  "bcq_update_frequency": 1000,
  "behavior_cloning_train_steps": 100000,
  "bc_batch": 1000,
  "fqe_train_steps": 100000,
  "lr_fqe": 0.001,
  "fqe_update_frequency": 1000,
  "fqe_batch": 1000,
  "np_state_dim": 31,
  "np_action_dim": 10,
  "np_discount": 0.99
}
INFO:src.model.crystalisland:finished creating random samples
INFO:src.model.gail:-- no gail with run_name seed_1 and name org--
INFO:src.model.gail:-- training gail --
INFO:src.model.gail:iter: 1000 | update: 1 | d_loss: 1.21 | pi_loss: -0.02 | d_exp:  0.3311 | d_nov:  0.5246 | d_rand:  0.5362
INFO:src.model.gail:iter: 2000 | update: 2 | d_loss: 1.19 | pi_loss:  0.02 | d_exp:  0.4131 | d_nov:  0.5952 | d_rand:  0.6589
INFO:src.model.gail:iter: 3000 | update: 3 | d_loss: 1.31 | pi_loss:  0.06 | d_exp:  0.5281 | d_nov:  0.6245 | d_rand:  0.7579
INFO:src.model.gail:iter: 4000 | update: 4 | d_loss: 1.36 | pi_loss:  0.05 | d_exp:  0.5575 | d_nov:  0.6082 | d_rand:  0.7922
INFO:src.model.gail:iter: 5000 | update: 5 | d_loss: 1.29 | pi_loss:  0.01 | d_exp:  0.4900 | d_nov:  0.5887 | d_rand:  0.7802
INFO:src.model.gail:iter: 6000 | update: 6 | d_loss: 1.15 | pi_loss:  0.07 | d_exp:  0.3844 | d_nov:  0.5858 | d_rand:  0.7459
INFO:src.model.gail:iter: 7000 | update: 7 | d_loss: 1.02 | pi_loss:  0.08 | d_exp:  0.3054 | d_nov:  0.6024 | d_rand:  0.7136
INFO:src.model.gail:iter: 8000 | update: 8 | d_loss: 0.88 | pi_loss:  0.08 | d_exp:  0.2670 | d_nov:  0.6743 | d_rand:  0.6967
INFO:src.model.gail:iter: 9000 | update: 9 | d_loss: 0.67 | pi_loss:  0.09 | d_exp:  0.2442 | d_nov:  0.7796 | d_rand:  0.6856
INFO:src.model.gail:iter: 10000 | update: 10 | d_loss: 0.51 | pi_loss:  0.08 | d_exp:  0.2153 | d_nov:  0.8480 | d_rand:  0.6651
INFO:src.model.gail:iter: 11000 | update: 11 | d_loss: 0.39 | pi_loss:  0.07 | d_exp:  0.1752 | d_nov:  0.8872 | d_rand:  0.6266
INFO:src.model.gail:iter: 12000 | update: 12 | d_loss: 0.30 | pi_loss:  0.07 | d_exp:  0.1334 | d_nov:  0.9075 | d_rand:  0.5724
INFO:src.model.gail:iter: 13000 | update: 13 | d_loss: 0.23 | pi_loss:  0.06 | d_exp:  0.0987 | d_nov:  0.9209 | d_rand:  0.5111
INFO:src.model.gail:iter: 14000 | update: 14 | d_loss: 0.18 | pi_loss:  0.06 | d_exp:  0.0736 | d_nov:  0.9314 | d_rand:  0.4521
INFO:src.model.gail:iter: 15000 | update: 15 | d_loss: 0.14 | pi_loss:  0.07 | d_exp:  0.0566 | d_nov:  0.9406 | d_rand:  0.4010
INFO:src.model.gail:iter: 16000 | update: 16 | d_loss: 0.12 | pi_loss:  0.03 | d_exp:  0.0452 | d_nov:  0.9497 | d_rand:  0.3592
INFO:src.model.gail:iter: 17000 | update: 17 | d_loss: 0.10 | pi_loss:  0.05 | d_exp:  0.0374 | d_nov:  0.9576 | d_rand:  0.3258
INFO:src.model.gail:iter: 18000 | update: 18 | d_loss: 0.08 | pi_loss:  0.03 | d_exp:  0.0318 | d_nov:  0.9656 | d_rand:  0.2989
INFO:src.model.gail:iter: 19000 | update: 19 | d_loss: 0.07 | pi_loss:  0.09 | d_exp:  0.0276 | d_nov:  0.9719 | d_rand:  0.2766
INFO:src.model.gail:iter: 20000 | update: 20 | d_loss: 0.06 | pi_loss:  0.04 | d_exp:  0.0243 | d_nov:  0.9763 | d_rand:  0.2577
INFO:src.model.gail:iter: 21000 | update: 21 | d_loss: 0.05 | pi_loss:  0.04 | d_exp:  0.0217 | d_nov:  0.9798 | d_rand:  0.2412
INFO:src.model.gail:iter: 22000 | update: 22 | d_loss: 0.04 | pi_loss:  0.07 | d_exp:  0.0195 | d_nov:  0.9821 | d_rand:  0.2267
INFO:src.model.gail:iter: 23000 | update: 23 | d_loss: 0.04 | pi_loss:  0.01 | d_exp:  0.0177 | d_nov:  0.9843 | d_rand:  0.2139
INFO:src.model.gail:iter: 24000 | update: 24 | d_loss: 0.03 | pi_loss:  0.05 | d_exp:  0.0162 | d_nov:  0.9858 | d_rand:  0.2024
INFO:src.model.gail:iter: 25000 | update: 25 | d_loss: 0.03 | pi_loss: -0.01 | d_exp:  0.0149 | d_nov:  0.9870 | d_rand:  0.1921
INFO:src.model.gail:iter: 26000 | update: 26 | d_loss: 0.03 | pi_loss:  0.01 | d_exp:  0.0138 | d_nov:  0.9879 | d_rand:  0.1830
INFO:src.model.gail:iter: 27000 | update: 27 | d_loss: 0.03 | pi_loss:  0.02 | d_exp:  0.0129 | d_nov:  0.9885 | d_rand:  0.1750
INFO:src.model.gail:iter: 28000 | update: 28 | d_loss: 0.03 | pi_loss:  0.01 | d_exp:  0.0120 | d_nov:  0.9896 | d_rand:  0.1677
INFO:src.model.gail:iter: 29000 | update: 29 | d_loss: 0.02 | pi_loss:  0.01 | d_exp:  0.0113 | d_nov:  0.9895 | d_rand:  0.1613
INFO:src.model.gail:iter: 30000 | update: 30 | d_loss: 0.02 | pi_loss:  0.01 | d_exp:  0.0107 | d_nov:  0.9892 | d_rand:  0.1562
INFO:src.model.gail:iter: 31000 | update: 31 | d_loss: 0.02 | pi_loss:  0.02 | d_exp:  0.0102 | d_nov:  0.9912 | d_rand:  0.1514
INFO:src.model.gail:iter: 32000 | update: 32 | d_loss: 0.02 | pi_loss:  0.03 | d_exp:  0.0097 | d_nov:  0.9893 | d_rand:  0.1472
INFO:src.model.gail:iter: 33000 | update: 33 | d_loss: 0.15 | pi_loss:  0.16 | d_exp:  0.0102 | d_nov:  0.9002 | d_rand:  0.1554
INFO:src.model.gail:iter: 34000 | update: 34 | d_loss: 0.29 | pi_loss:  0.08 | d_exp:  0.0118 | d_nov:  0.8562 | d_rand:  0.1804
INFO:src.model.gail:iter: 35000 | update: 35 | d_loss: 0.19 | pi_loss:  0.07 | d_exp:  0.0146 | d_nov:  0.9288 | d_rand:  0.2209
INFO:src.model.gail:iter: 36000 | update: 36 | d_loss: 0.09 | pi_loss:  0.04 | d_exp:  0.0181 | d_nov:  0.9677 | d_rand:  0.2709
INFO:src.model.gail:iter: 37000 | update: 37 | d_loss: 0.05 | pi_loss:  0.06 | d_exp:  0.0217 | d_nov:  0.9843 | d_rand:  0.3205
INFO:src.model.gail:iter: 38000 | update: 38 | d_loss: 0.04 | pi_loss:  0.04 | d_exp:  0.0246 | d_nov:  0.9918 | d_rand:  0.3604
INFO:src.model.gail:iter: 39000 | update: 39 | d_loss: 0.04 | pi_loss:  0.04 | d_exp:  0.0261 | d_nov:  0.9941 | d_rand:  0.3851
INFO:src.model.gail:iter: 40000 | update: 40 | d_loss: 0.03 | pi_loss:  0.04 | d_exp:  0.0262 | d_nov:  0.9954 | d_rand:  0.3933
INFO:src.model.gail:iter: 41000 | update: 41 | d_loss: 0.03 | pi_loss:  0.03 | d_exp:  0.0250 | d_nov:  0.9960 | d_rand:  0.3865
INFO:src.model.gail:iter: 42000 | update: 42 | d_loss: 0.03 | pi_loss:  0.03 | d_exp:  0.0229 | d_nov:  0.9963 | d_rand:  0.3681
INFO:src.model.gail:iter: 43000 | update: 43 | d_loss: 0.03 | pi_loss:  0.01 | d_exp:  0.0203 | d_nov:  0.9965 | d_rand:  0.3422
INFO:src.model.gail:iter: 44000 | update: 44 | d_loss: 0.03 | pi_loss:  0.02 | d_exp:  0.0177 | d_nov:  0.9964 | d_rand:  0.3126
INFO:src.model.gail:iter: 45000 | update: 45 | d_loss: 0.02 | pi_loss:  0.01 | d_exp:  0.0153 | d_nov:  0.9967 | d_rand:  0.2819
INFO:src.model.gail:iter: 46000 | update: 46 | d_loss: 0.02 | pi_loss:  0.05 | d_exp:  0.0132 | d_nov:  0.9932 | d_rand:  0.2532
INFO:src.model.gail:iter: 47000 | update: 47 | d_loss: 0.06 | pi_loss:  0.12 | d_exp:  0.0119 | d_nov:  0.9636 | d_rand:  0.2361
INFO:src.model.gail:iter: 48000 | update: 48 | d_loss: 0.75 | pi_loss:  0.07 | d_exp:  0.0146 | d_nov:  0.6788 | d_rand:  0.2783
INFO:src.model.gail:iter: 49000 | update: 49 | d_loss: 0.41 | pi_loss:  0.10 | d_exp:  0.0210 | d_nov:  0.8517 | d_rand:  0.3542
INFO:src.model.gail:iter: 50000 | update: 50 | d_loss: 0.15 | pi_loss:  0.08 | d_exp:  0.0302 | d_nov:  0.9511 | d_rand:  0.4344
INFO:src.model.gail:iter: 51000 | update: 51 | d_loss: 0.07 | pi_loss:  0.04 | d_exp:  0.0391 | d_nov:  0.9804 | d_rand:  0.4963
INFO:src.model.gail:iter: 52000 | update: 52 | d_loss: 0.06 | pi_loss:  0.06 | d_exp:  0.0435 | d_nov:  0.9916 | d_rand:  0.5305
INFO:src.model.gail:iter: 53000 | update: 53 | d_loss: 0.06 | pi_loss:  0.02 | d_exp:  0.0421 | d_nov:  0.9931 | d_rand:  0.5388
INFO:src.model.gail:iter: 54000 | update: 54 | d_loss: 0.05 | pi_loss:  0.04 | d_exp:  0.0374 | d_nov:  0.9940 | d_rand:  0.5267
INFO:src.model.gail:iter: 55000 | update: 55 | d_loss: 0.04 | pi_loss:  0.01 | d_exp:  0.0315 | d_nov:  0.9952 | d_rand:  0.4991
INFO:src.model.gail:iter: 56000 | update: 56 | d_loss: 0.04 | pi_loss:  0.03 | d_exp:  0.0254 | d_nov:  0.9959 | d_rand:  0.4606
INFO:src.model.gail:iter: 57000 | update: 57 | d_loss: 0.03 | pi_loss:  0.01 | d_exp:  0.0200 | d_nov:  0.9958 | d_rand:  0.4169
INFO:src.model.gail:iter: 58000 | update: 58 | d_loss: 0.02 | pi_loss:  0.02 | d_exp:  0.0156 | d_nov:  0.9962 | d_rand:  0.3724
INFO:src.model.gail:iter: 59000 | update: 59 | d_loss: 0.02 | pi_loss:  0.01 | d_exp:  0.0124 | d_nov:  0.9968 | d_rand:  0.3299
INFO:src.model.gail:iter: 60000 | update: 60 | d_loss: 0.02 | pi_loss:  0.01 | d_exp:  0.0101 | d_nov:  0.9960 | d_rand:  0.2927
INFO:src.model.gail:iter: 61000 | update: 61 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0085 | d_nov:  0.9966 | d_rand:  0.2615
INFO:src.model.gail:iter: 62000 | update: 62 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0073 | d_nov:  0.9963 | d_rand:  0.2366
INFO:src.model.gail:iter: 63000 | update: 63 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0064 | d_nov:  0.9970 | d_rand:  0.2162
INFO:src.model.gail:iter: 64000 | update: 64 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0058 | d_nov:  0.9965 | d_rand:  0.2001
INFO:src.model.gail:iter: 65000 | update: 65 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0052 | d_nov:  0.9965 | d_rand:  0.1879
INFO:src.model.gail:iter: 66000 | update: 66 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0048 | d_nov:  0.9971 | d_rand:  0.1783
INFO:src.model.gail:iter: 67000 | update: 67 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0044 | d_nov:  0.9971 | d_rand:  0.1705
INFO:src.model.gail:iter: 68000 | update: 68 | d_loss: 0.01 | pi_loss:  0.00 | d_exp:  0.0041 | d_nov:  0.9959 | d_rand:  0.1653
INFO:src.model.gail:iter: 69000 | update: 69 | d_loss: 0.01 | pi_loss:  0.05 | d_exp:  0.0039 | d_nov:  0.9933 | d_rand:  0.1621
INFO:src.model.gail:iter: 70000 | update: 70 | d_loss: 0.33 | pi_loss:  0.15 | d_exp:  0.0044 | d_nov:  0.8642 | d_rand:  0.1905
INFO:src.model.gail:iter: 71000 | update: 71 | d_loss: 2.82 | pi_loss:  0.13 | d_exp:  0.0066 | d_nov:  0.1916 | d_rand:  0.2674
INFO:src.model.gail:iter: 72000 | update: 72 | d_loss: 2.84 | pi_loss:  0.09 | d_exp:  0.0116 | d_nov:  0.1995 | d_rand:  0.3822
INFO:src.model.gail:iter: 73000 | update: 73 | d_loss: 1.29 | pi_loss:  0.11 | d_exp:  0.0224 | d_nov:  0.6388 | d_rand:  0.5156
INFO:src.model.gail:iter: 74000 | update: 74 | d_loss: 0.37 | pi_loss:  0.08 | d_exp:  0.0412 | d_nov:  0.8994 | d_rand:  0.6300
INFO:src.model.gail:iter: 75000 | update: 75 | d_loss: 0.14 | pi_loss:  0.07 | d_exp:  0.0665 | d_nov:  0.9695 | d_rand:  0.7063
INFO:src.model.gail:iter: 76000 | update: 76 | d_loss: 0.11 | pi_loss:  0.03 | d_exp:  0.0917 | d_nov:  0.9876 | d_rand:  0.7489
INFO:src.model.gail:iter: 77000 | update: 77 | d_loss: 0.12 | pi_loss:  0.02 | d_exp:  0.1086 | d_nov:  0.9938 | d_rand:  0.7654
INFO:src.model.gail:iter: 78000 | update: 78 | d_loss: 0.13 | pi_loss:  0.02 | d_exp:  0.1128 | d_nov:  0.9941 | d_rand:  0.7632
INFO:src.model.gail:iter: 79000 | update: 79 | d_loss: 0.13 | pi_loss:  0.02 | d_exp:  0.1057 | d_nov:  0.9950 | d_rand:  0.7467
INFO:src.model.gail:iter: 80000 | update: 80 | d_loss: 0.12 | pi_loss:  0.01 | d_exp:  0.0919 | d_nov:  0.9955 | d_rand:  0.7184
INFO:src.model.gail:iter: 81000 | update: 81 | d_loss: 0.10 | pi_loss:  0.00 | d_exp:  0.0759 | d_nov:  0.9961 | d_rand:  0.6803
INFO:src.model.gail:iter: 82000 | update: 82 | d_loss: 0.08 | pi_loss:  0.02 | d_exp:  0.0606 | d_nov:  0.9967 | d_rand:  0.6351
INFO:src.model.gail:iter: 83000 | update: 83 | d_loss: 0.07 | pi_loss:  0.01 | d_exp:  0.0473 | d_nov:  0.9962 | d_rand:  0.5866
INFO:src.model.gail:iter: 84000 | update: 84 | d_loss: 0.05 | pi_loss: -0.00 | d_exp:  0.0369 | d_nov:  0.9956 | d_rand:  0.5382
INFO:src.model.gail:iter: 85000 | update: 85 | d_loss: 0.04 | pi_loss:  0.00 | d_exp:  0.0291 | d_nov:  0.9960 | d_rand:  0.4921
INFO:src.model.gail:iter: 86000 | update: 86 | d_loss: 0.04 | pi_loss:  0.01 | d_exp:  0.0235 | d_nov:  0.9947 | d_rand:  0.4509
INFO:src.model.gail:iter: 87000 | update: 87 | d_loss: 0.03 | pi_loss: -0.03 | d_exp:  0.0196 | d_nov:  0.9942 | d_rand:  0.4165
INFO:src.model.gail:iter: 88000 | update: 88 | d_loss: 0.03 | pi_loss:  0.04 | d_exp:  0.0169 | d_nov:  0.9955 | d_rand:  0.3874
INFO:src.model.gail:iter: 89000 | update: 89 | d_loss: 0.03 | pi_loss: -0.01 | d_exp:  0.0149 | d_nov:  0.9941 | d_rand:  0.3639
INFO:src.model.gail:iter: 90000 | update: 90 | d_loss: 0.02 | pi_loss:  0.00 | d_exp:  0.0134 | d_nov:  0.9958 | d_rand:  0.3442
INFO:src.model.gail:iter: 91000 | update: 91 | d_loss: 0.02 | pi_loss: -0.00 | d_exp:  0.0123 | d_nov:  0.9949 | d_rand:  0.3279
INFO:src.model.gail:iter: 92000 | update: 92 | d_loss: 0.02 | pi_loss: -0.01 | d_exp:  0.0113 | d_nov:  0.9949 | d_rand:  0.3146
INFO:src.model.gail:iter: 93000 | update: 93 | d_loss: 0.02 | pi_loss: -0.01 | d_exp:  0.0105 | d_nov:  0.9944 | d_rand:  0.3042
INFO:src.model.gail:iter: 94000 | update: 94 | d_loss: 0.02 | pi_loss: -0.01 | d_exp:  0.0098 | d_nov:  0.9962 | d_rand:  0.2957
INFO:src.model.gail:iter: 95000 | update: 95 | d_loss: 0.02 | pi_loss: -0.03 | d_exp:  0.0092 | d_nov:  0.9956 | d_rand:  0.2884
INFO:src.model.gail:iter: 96000 | update: 96 | d_loss: 0.02 | pi_loss: -0.01 | d_exp:  0.0087 | d_nov:  0.9945 | d_rand:  0.2826
INFO:src.model.gail:iter: 97000 | update: 97 | d_loss: 1.28 | pi_loss:  0.15 | d_exp:  0.0103 | d_nov:  0.4504 | d_rand:  0.3229
INFO:src.model.gail:iter: 98000 | update: 98 | d_loss: 0.84 | pi_loss:  0.02 | d_exp:  0.0147 | d_nov:  0.6207 | d_rand:  0.4123
INFO:src.model.gail:iter: 99000 | update: 99 | d_loss: 0.14 | pi_loss:  0.03 | d_exp:  0.0212 | d_nov:  0.9278 | d_rand:  0.5063
INFO:src.model.gail:iter: 100000 | update: 100 | d_loss: 1.02 | pi_loss:  0.10 | d_exp:  0.0343 | d_nov:  0.6372 | d_rand:  0.6164
INFO:src.model.gail:iter: 101000 | update: 101 | d_loss: 1.02 | pi_loss:  0.14 | d_exp:  0.0628 | d_nov:  0.6332 | d_rand:  0.7192
INFO:src.model.gail:iter: 102000 | update: 102 | d_loss: 0.15 | pi_loss: -0.02 | d_exp:  0.1032 | d_nov:  0.9660 | d_rand:  0.7853
INFO:src.model.gail:iter: 103000 | update: 103 | d_loss: 0.14 | pi_loss:  0.02 | d_exp:  0.1406 | d_nov:  0.9944 | d_rand:  0.8205
INFO:src.model.gail:iter: 104000 | update: 104 | d_loss: 0.18 | pi_loss:  0.00 | d_exp:  0.1620 | d_nov:  0.9986 | d_rand:  0.8360
INFO:src.model.gail:iter: 105000 | update: 105 | d_loss: 0.21 | pi_loss: -0.03 | d_exp:  0.1631 | d_nov:  0.9975 | d_rand:  0.8391
INFO:src.model.gail:iter: 106000 | update: 106 | d_loss: 0.22 | pi_loss:  0.01 | d_exp:  0.1500 | d_nov:  0.9837 | d_rand:  0.8346
INFO:src.model.gail:iter: 107000 | update: 107 | d_loss: 0.29 | pi_loss:  0.11 | d_exp:  0.1342 | d_nov:  0.9165 | d_rand:  0.8291
INFO:src.model.gail:iter: 108000 | update: 108 | d_loss: 0.68 | pi_loss:  0.14 | d_exp:  0.1306 | d_nov:  0.6644 | d_rand:  0.8271
INFO:src.model.gail:iter: 109000 | update: 109 | d_loss: 0.85 | pi_loss:  0.14 | d_exp:  0.1433 | d_nov:  0.6476 | d_rand:  0.8300
INFO:src.model.gail:iter: 110000 | update: 110 | d_loss: 0.55 | pi_loss:  0.12 | d_exp:  0.1635 | d_nov:  0.8259 | d_rand:  0.8334
INFO:src.model.gail:iter: 111000 | update: 111 | d_loss: 0.34 | pi_loss:  0.11 | d_exp:  0.1779 | d_nov:  0.9206 | d_rand:  0.8331
INFO:src.model.gail:iter: 112000 | update: 112 | d_loss: 0.29 | pi_loss:  0.09 | d_exp:  0.1786 | d_nov:  0.9600 | d_rand:  0.8276
INFO:src.model.gail:iter: 113000 | update: 113 | d_loss: 0.25 | pi_loss:  0.09 | d_exp:  0.1655 | d_nov:  0.9719 | d_rand:  0.8171
INFO:src.model.gail:iter: 114000 | update: 114 | d_loss: 0.22 | pi_loss:  0.10 | d_exp:  0.1433 | d_nov:  0.9762 | d_rand:  0.8017
INFO:src.model.gail:iter: 115000 | update: 115 | d_loss: 0.19 | pi_loss:  0.08 | d_exp:  0.1186 | d_nov:  0.9763 | d_rand:  0.7823
INFO:src.model.gail:iter: 116000 | update: 116 | d_loss: 0.16 | pi_loss:  0.07 | d_exp:  0.0956 | d_nov:  0.9760 | d_rand:  0.7602
INFO:src.model.gail:iter: 117000 | update: 117 | d_loss: 0.13 | pi_loss:  0.05 | d_exp:  0.0764 | d_nov:  0.9796 | d_rand:  0.7363
INFO:src.model.gail:iter: 118000 | update: 118 | d_loss: 0.11 | pi_loss:  0.03 | d_exp:  0.0614 | d_nov:  0.9796 | d_rand:  0.7123
INFO:src.model.gail:iter: 119000 | update: 119 | d_loss: 0.08 | pi_loss:  0.03 | d_exp:  0.0500 | d_nov:  0.9836 | d_rand:  0.6884
INFO:src.model.gail:iter: 120000 | update: 120 | d_loss: 0.07 | pi_loss:  0.02 | d_exp:  0.0414 | d_nov:  0.9825 | d_rand:  0.6657
INFO:src.model.gail:iter: 121000 | update: 121 | d_loss: 0.06 | pi_loss:  0.03 | d_exp:  0.0350 | d_nov:  0.9836 | d_rand:  0.6446
INFO:src.model.gail:iter: 122000 | update: 122 | d_loss: 0.05 | pi_loss:  0.03 | d_exp:  0.0302 | d_nov:  0.9837 | d_rand:  0.6252
INFO:src.model.gail:iter: 123000 | update: 123 | d_loss: 0.05 | pi_loss:  0.04 | d_exp:  0.0264 | d_nov:  0.9842 | d_rand:  0.6074
INFO:src.model.gail:iter: 124000 | update: 124 | d_loss: 0.05 | pi_loss:  0.02 | d_exp:  0.0235 | d_nov:  0.9825 | d_rand:  0.5917
INFO:src.model.gail:iter: 125000 | update: 125 | d_loss: 0.04 | pi_loss:  0.03 | d_exp:  0.0212 | d_nov:  0.9826 | d_rand:  0.5782
INFO:src.model.gail:iter: 126000 | update: 126 | d_loss: 0.04 | pi_loss:  0.02 | d_exp:  0.0195 | d_nov:  0.9823 | d_rand:  0.5674
INFO:src.model.gail:iter: 127000 | update: 127 | d_loss: 0.04 | pi_loss:  0.00 | d_exp:  0.0180 | d_nov:  0.9836 | d_rand:  0.5590
INFO:src.model.gail:iter: 128000 | update: 128 | d_loss: 0.03 | pi_loss:  0.02 | d_exp:  0.0169 | d_nov:  0.9864 | d_rand:  0.5520
INFO:src.model.gail:iter: 129000 | update: 129 | d_loss: 0.03 | pi_loss:  0.02 | d_exp:  0.0159 | d_nov:  0.9877 | d_rand:  0.5455
INFO:src.model.gail:iter: 130000 | update: 130 | d_loss: 0.03 | pi_loss:  0.03 | d_exp:  0.0150 | d_nov:  0.9861 | d_rand:  0.5403
INFO:src.model.gail:iter: 131000 | update: 131 | d_loss: 0.03 | pi_loss:  0.01 | d_exp:  0.0142 | d_nov:  0.9874 | d_rand:  0.5360
INFO:src.model.gail:iter: 132000 | update: 132 | d_loss: 0.03 | pi_loss:  0.02 | d_exp:  0.0136 | d_nov:  0.9873 | d_rand:  0.5324
INFO:src.model.gail:iter: 133000 | update: 133 | d_loss: 0.03 | pi_loss:  0.02 | d_exp:  0.0130 | d_nov:  0.9874 | d_rand:  0.5291
INFO:src.model.gail:iter: 134000 | update: 134 | d_loss: 0.03 | pi_loss:  0.02 | d_exp:  0.0124 | d_nov:  0.9880 | d_rand:  0.5261
INFO:src.model.gail:iter: 135000 | update: 135 | d_loss: 0.02 | pi_loss:  0.04 | d_exp:  0.0118 | d_nov:  0.9896 | d_rand:  0.5227
INFO:src.model.gail:iter: 136000 | update: 136 | d_loss: 0.02 | pi_loss:  0.03 | d_exp:  0.0112 | d_nov:  0.9883 | d_rand:  0.5190
INFO:src.model.gail:iter: 137000 | update: 137 | d_loss: 0.03 | pi_loss:  0.03 | d_exp:  0.0107 | d_nov:  0.9879 | d_rand:  0.5156
INFO:src.model.gail:iter: 138000 | update: 138 | d_loss: 0.02 | pi_loss:  0.02 | d_exp:  0.0102 | d_nov:  0.9882 | d_rand:  0.5127
INFO:src.model.gail:iter: 139000 | update: 139 | d_loss: 0.02 | pi_loss:  0.03 | d_exp:  0.0098 | d_nov:  0.9883 | d_rand:  0.5102
INFO:src.model.gail:iter: 140000 | update: 140 | d_loss: 0.02 | pi_loss:  0.03 | d_exp:  0.0095 | d_nov:  0.9890 | d_rand:  0.5080
INFO:src.model.gail:iter: 141000 | update: 141 | d_loss: 0.02 | pi_loss:  0.02 | d_exp:  0.0091 | d_nov:  0.9886 | d_rand:  0.5059
INFO:src.model.gail:iter: 142000 | update: 142 | d_loss: 0.02 | pi_loss:  0.03 | d_exp:  0.0088 | d_nov:  0.9898 | d_rand:  0.5038
INFO:src.model.gail:iter: 143000 | update: 143 | d_loss: 0.02 | pi_loss:  0.04 | d_exp:  0.0086 | d_nov:  0.9897 | d_rand:  0.5016
INFO:src.model.gail:iter: 144000 | update: 144 | d_loss: 0.02 | pi_loss:  0.02 | d_exp:  0.0083 | d_nov:  0.9892 | d_rand:  0.4994
INFO:src.model.gail:iter: 145000 | update: 145 | d_loss: 0.02 | pi_loss:  0.03 | d_exp:  0.0081 | d_nov:  0.9889 | d_rand:  0.4977
INFO:src.model.gail:iter: 146000 | update: 146 | d_loss: 0.02 | pi_loss:  0.03 | d_exp:  0.0079 | d_nov:  0.9909 | d_rand:  0.4959
INFO:src.model.gail:iter: 147000 | update: 147 | d_loss: 0.02 | pi_loss:  0.02 | d_exp:  0.0078 | d_nov:  0.9911 | d_rand:  0.4941
INFO:src.model.gail:iter: 148000 | update: 148 | d_loss: 0.02 | pi_loss:  0.01 | d_exp:  0.0076 | d_nov:  0.9922 | d_rand:  0.4924
INFO:src.model.gail:iter: 149000 | update: 149 | d_loss: 0.02 | pi_loss:  0.02 | d_exp:  0.0075 | d_nov:  0.9925 | d_rand:  0.4908
INFO:src.model.gail:iter: 150000 | update: 150 | d_loss: 0.02 | pi_loss:  0.01 | d_exp:  0.0074 | d_nov:  0.9930 | d_rand:  0.4890
INFO:src.model.gail:iter: 151000 | update: 151 | d_loss: 0.02 | pi_loss:  0.01 | d_exp:  0.0073 | d_nov:  0.9922 | d_rand:  0.4873
INFO:src.model.gail:iter: 152000 | update: 152 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0072 | d_nov:  0.9936 | d_rand:  0.4853
INFO:src.model.gail:iter: 153000 | update: 153 | d_loss: 0.02 | pi_loss:  0.02 | d_exp:  0.0071 | d_nov:  0.9923 | d_rand:  0.4834
INFO:src.model.gail:iter: 154000 | update: 154 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0070 | d_nov:  0.9935 | d_rand:  0.4815
INFO:src.model.gail:iter: 155000 | update: 155 | d_loss: 0.01 | pi_loss:  0.02 | d_exp:  0.0069 | d_nov:  0.9934 | d_rand:  0.4794
INFO:src.model.gail:iter: 156000 | update: 156 | d_loss: 0.02 | pi_loss: -0.00 | d_exp:  0.0068 | d_nov:  0.9913 | d_rand:  0.4777
INFO:src.model.gail:iter: 157000 | update: 157 | d_loss: 0.02 | pi_loss:  0.01 | d_exp:  0.0067 | d_nov:  0.9926 | d_rand:  0.4762
INFO:src.model.gail:iter: 158000 | update: 158 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0066 | d_nov:  0.9930 | d_rand:  0.4748
INFO:src.model.gail:iter: 159000 | update: 159 | d_loss: 0.01 | pi_loss:  0.02 | d_exp:  0.0065 | d_nov:  0.9938 | d_rand:  0.4730
INFO:src.model.gail:iter: 160000 | update: 160 | d_loss: 0.01 | pi_loss:  0.03 | d_exp:  0.0065 | d_nov:  0.9943 | d_rand:  0.4710
INFO:src.model.gail:iter: 161000 | update: 161 | d_loss: 0.01 | pi_loss:  0.02 | d_exp:  0.0064 | d_nov:  0.9932 | d_rand:  0.4690
INFO:src.model.gail:iter: 162000 | update: 162 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0063 | d_nov:  0.9936 | d_rand:  0.4670
INFO:src.model.gail:iter: 163000 | update: 163 | d_loss: 0.01 | pi_loss:  0.02 | d_exp:  0.0062 | d_nov:  0.9937 | d_rand:  0.4649
INFO:src.model.gail:iter: 164000 | update: 164 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0061 | d_nov:  0.9939 | d_rand:  0.4629
INFO:src.model.gail:iter: 165000 | update: 165 | d_loss: 0.01 | pi_loss:  0.02 | d_exp:  0.0060 | d_nov:  0.9939 | d_rand:  0.4607
INFO:src.model.gail:iter: 166000 | update: 166 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0058 | d_nov:  0.9935 | d_rand:  0.4587
INFO:src.model.gail:iter: 167000 | update: 167 | d_loss: 0.01 | pi_loss:  0.02 | d_exp:  0.0057 | d_nov:  0.9940 | d_rand:  0.4566
INFO:src.model.gail:iter: 168000 | update: 168 | d_loss: 0.01 | pi_loss:  0.02 | d_exp:  0.0056 | d_nov:  0.9939 | d_rand:  0.4544
INFO:src.model.gail:iter: 169000 | update: 169 | d_loss: 0.01 | pi_loss:  0.02 | d_exp:  0.0055 | d_nov:  0.9938 | d_rand:  0.4523
INFO:src.model.gail:iter: 170000 | update: 170 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0055 | d_nov:  0.9939 | d_rand:  0.4502
INFO:src.model.gail:iter: 171000 | update: 171 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0054 | d_nov:  0.9937 | d_rand:  0.4481
INFO:src.model.gail:iter: 172000 | update: 172 | d_loss: 0.01 | pi_loss:  0.03 | d_exp:  0.0053 | d_nov:  0.9949 | d_rand:  0.4458
INFO:src.model.gail:iter: 173000 | update: 173 | d_loss: 0.01 | pi_loss:  0.02 | d_exp:  0.0052 | d_nov:  0.9937 | d_rand:  0.4436
INFO:src.model.gail:iter: 174000 | update: 174 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0051 | d_nov:  0.9941 | d_rand:  0.4413
INFO:src.model.gail:iter: 175000 | update: 175 | d_loss: 0.02 | pi_loss:  0.01 | d_exp:  0.0050 | d_nov:  0.9913 | d_rand:  0.4399
INFO:src.model.gail:iter: 176000 | update: 176 | d_loss: 0.01 | pi_loss:  0.01 | d_exp:  0.0050 | d_nov:  0.9939 | d_rand:  0.4387
INFO:src.model.gail:iter: 177000 | update: 177 | d_loss: 0.01 | pi_loss:  0.03 | d_exp:  0.0049 | d_nov:  0.9927 | d_rand:  0.4374
INFO:src.model.gail:iter: 178000 | update: 178 | d_loss: 0.12 | pi_loss:  0.16 | d_exp:  0.0050 | d_nov:  0.9103 | d_rand:  0.4390
INFO:src.model.gail:iter: 179000 | update: 179 | d_loss: 1.14 | pi_loss:  0.13 | d_exp:  0.0056 | d_nov:  0.4294 | d_rand:  0.4601
INFO:src.model.gail:iter: 180000 | update: 180 | d_loss: 3.28 | pi_loss:  0.14 | d_exp:  0.0075 | d_nov:  0.0791 | d_rand:  0.5029
INFO:src.model.gail:iter: 181000 | update: 181 | d_loss: 3.68 | pi_loss:  0.14 | d_exp:  0.0114 | d_nov:  0.0452 | d_rand:  0.5602
INFO:src.model.gail:iter: 182000 | update: 182 | d_loss: 2.84 | pi_loss:  0.07 | d_exp:  0.0193 | d_nov:  0.1263 | d_rand:  0.6242
INFO:src.model.gail:iter: 183000 | update: 183 | d_loss: 2.16 | pi_loss:  0.04 | d_exp:  0.0351 | d_nov:  0.2465 | d_rand:  0.6873
INFO:src.model.gail:iter: 184000 | update: 184 | d_loss: 1.16 | pi_loss:  0.04 | d_exp:  0.0647 | d_nov:  0.5275 | d_rand:  0.7425
INFO:src.model.gail:iter: 185000 | update: 185 | d_loss: 0.59 | pi_loss:  0.03 | d_exp:  0.1119 | d_nov:  0.7632 | d_rand:  0.7846
INFO:src.model.gail:iter: 186000 | update: 186 | d_loss: 0.35 | pi_loss:  0.06 | d_exp:  0.1706 | d_nov:  0.8876 | d_rand:  0.8128
INFO:src.model.gail:iter: 187000 | update: 187 | d_loss: 0.37 | pi_loss:  0.00 | d_exp:  0.2282 | d_nov:  0.9160 | d_rand:  0.8301
INFO:src.model.gail:iter: 188000 | update: 188 | d_loss: 0.39 | pi_loss: -0.01 | d_exp:  0.2714 | d_nov:  0.9531 | d_rand:  0.8393
INFO:src.model.gail:iter: 189000 | update: 189 | d_loss: 0.46 | pi_loss:  0.02 | d_exp:  0.2942 | d_nov:  0.9417 | d_rand:  0.8424
INFO:src.model.gail:iter: 190000 | update: 190 | d_loss: 0.50 | pi_loss:  0.01 | d_exp:  0.2971 | d_nov:  0.9303 | d_rand:  0.8411
INFO:src.model.gail:iter: 191000 | update: 191 | d_loss: 0.49 | pi_loss:  0.02 | d_exp:  0.2840 | d_nov:  0.9300 | d_rand:  0.8359
INFO:src.model.gail:iter: 192000 | update: 192 | d_loss: 0.45 | pi_loss: -0.01 | d_exp:  0.2599 | d_nov:  0.9332 | d_rand:  0.8269
INFO:src.model.gail:iter: 193000 | update: 193 | d_loss: 0.44 | pi_loss: -0.00 | d_exp:  0.2310 | d_nov:  0.9104 | d_rand:  0.8158
INFO:src.model.gail:iter: 194000 | update: 194 | d_loss: 0.43 | pi_loss: -0.01 | d_exp:  0.2021 | d_nov:  0.8918 | d_rand:  0.8040
INFO:src.model.gail:iter: 195000 | update: 195 | d_loss: 0.41 | pi_loss:  0.01 | d_exp:  0.1767 | d_nov:  0.8688 | d_rand:  0.7918
INFO:src.model.gail:iter: 196000 | update: 196 | d_loss: 0.71 | pi_loss:  0.18 | d_exp:  0.1609 | d_nov:  0.6724 | d_rand:  0.7821
INFO:src.model.gail:iter: 197000 | update: 197 | d_loss: 1.09 | pi_loss:  0.11 | d_exp:  0.1586 | d_nov:  0.5095 | d_rand:  0.7784
INFO:src.model.gail:iter: 198000 | update: 198 | d_loss: 0.48 | pi_loss: -0.14 | d_exp:  0.1592 | d_nov:  0.8227 | d_rand:  0.7772
INFO:src.model.gail:iter: 199000 | update: 199 | d_loss: 0.65 | pi_loss:  0.04 | d_exp:  0.1631 | d_nov:  0.7364 | d_rand:  0.7776
INFO:src.model.gail:iter: 200000 | update: 200 | d_loss: 0.81 | pi_loss:  0.10 | d_exp:  0.1705 | d_nov:  0.6572 | d_rand:  0.7787
INFO:src.model.gail:iter: 201000 | update: 201 | d_loss: 0.30 | pi_loss:  0.00 | d_exp:  0.1732 | d_nov:  0.9527 | d_rand:  0.7766
INFO:src.model.gail:iter: 202000 | update: 202 | d_loss: 1.07 | pi_loss:  0.11 | d_exp:  0.1782 | d_nov:  0.5118 | d_rand:  0.7745
INFO:src.model.gail:iter: 203000 | update: 203 | d_loss: 0.29 | pi_loss:  0.07 | d_exp:  0.1783 | d_nov:  0.9709 | d_rand:  0.7693
INFO:src.model.gail:iter: 204000 | update: 204 | d_loss: 0.29 | pi_loss: -0.05 | d_exp:  0.1721 | d_nov:  0.9661 | d_rand:  0.7606
INFO:src.model.gail:iter: 205000 | update: 205 | d_loss: 0.24 | pi_loss: -0.01 | d_exp:  0.1612 | d_nov:  0.9979 | d_rand:  0.7486
INFO:src.model.gail:iter: 206000 | update: 206 | d_loss: 0.23 | pi_loss:  0.11 | d_exp:  0.1479 | d_nov:  0.9916 | d_rand:  0.7342
INFO:src.model.gail:iter: 207000 | update: 207 | d_loss: 0.75 | pi_loss:  0.22 | d_exp:  0.1400 | d_nov:  0.6348 | d_rand:  0.7220
INFO:src.model.gail:iter: 208000 | update: 208 | d_loss: 1.71 | pi_loss:  0.09 | d_exp:  0.1458 | d_nov:  0.2977 | d_rand:  0.7183
INFO:src.model.gail:iter: 209000 | update: 209 | d_loss: 2.64 | pi_loss:  0.03 | d_exp:  0.1651 | d_nov:  0.1493 | d_rand:  0.7237
INFO:src.model.gail:iter: 210000 | update: 210 | d_loss: 3.31 | pi_loss:  0.04 | d_exp:  0.1992 | d_nov:  0.0886 | d_rand:  0.7347
INFO:src.model.gail:iter: 211000 | update: 211 | d_loss: 3.23 | pi_loss:  0.03 | d_exp:  0.2464 | d_nov:  0.0867 | d_rand:  0.7488
INFO:src.model.gail:iter: 212000 | update: 212 | d_loss: 2.54 | pi_loss: -0.05 | d_exp:  0.3045 | d_nov:  0.1831 | d_rand:  0.7636
INFO:src.model.gail:iter: 213000 | update: 213 | d_loss: 1.92 | pi_loss:  0.13 | d_exp:  0.3688 | d_nov:  0.3496 | d_rand:  0.7776
INFO:src.model.gail:iter: 214000 | update: 214 | d_loss: 0.93 | pi_loss: -0.01 | d_exp:  0.4218 | d_nov:  0.7947 | d_rand:  0.7872
INFO:src.model.gail:iter: 215000 | update: 215 | d_loss: 0.78 | pi_loss: -0.08 | d_exp:  0.4523 | d_nov:  0.9640 | d_rand:  0.7905
INFO:src.model.gail:iter: 216000 | update: 216 | d_loss: 0.82 | pi_loss: -0.08 | d_exp:  0.4609 | d_nov:  0.9653 | d_rand:  0.7885
INFO:src.model.gail:iter: 217000 | update: 217 | d_loss: 0.82 | pi_loss: -0.02 | d_exp:  0.4509 | d_nov:  0.9787 | d_rand:  0.7825
INFO:src.model.gail:iter: 218000 | update: 218 | d_loss: 0.82 | pi_loss: -0.04 | d_exp:  0.4277 | d_nov:  0.9445 | d_rand:  0.7736
INFO:src.model.gail:iter: 219000 | update: 219 | d_loss: 1.05 | pi_loss:  0.04 | d_exp:  0.3998 | d_nov:  0.7599 | d_rand:  0.7651
INFO:src.model.gail:iter: 220000 | update: 220 | d_loss: 1.58 | pi_loss:  0.15 | d_exp:  0.3776 | d_nov:  0.4169 | d_rand:  0.7590
INFO:src.model.gail:iter: 221000 | update: 221 | d_loss: 1.08 | pi_loss: -0.06 | d_exp:  0.3580 | d_nov:  0.6981 | d_rand:  0.7547
INFO:src.model.gail:iter: 222000 | update: 222 | d_loss: 1.05 | pi_loss: -0.03 | d_exp:  0.3393 | d_nov:  0.7073 | d_rand:  0.7510
INFO:src.model.gail:iter: 223000 | update: 223 | d_loss: 1.47 | pi_loss:  0.06 | d_exp:  0.3281 | d_nov:  0.4386 | d_rand:  0.7491
INFO:src.model.gail:iter: 224000 | update: 224 | d_loss: 2.09 | pi_loss:  0.15 | d_exp:  0.3279 | d_nov:  0.2172 | d_rand:  0.7500
INFO:src.model.gail:iter: 225000 | update: 225 | d_loss: 2.08 | pi_loss:  0.15 | d_exp:  0.3371 | d_nov:  0.2305 | d_rand:  0.7530
INFO:src.model.gail:iter: 226000 | update: 226 | d_loss: 2.01 | pi_loss:  0.04 | d_exp:  0.3536 | d_nov:  0.2530 | d_rand:  0.7573
INFO:src.model.gail:iter: 227000 | update: 227 | d_loss: 2.05 | pi_loss:  0.07 | d_exp:  0.3758 | d_nov:  0.2713 | d_rand:  0.7628
INFO:src.model.gail:iter: 228000 | update: 228 | d_loss: 1.95 | pi_loss:  0.05 | d_exp:  0.4019 | d_nov:  0.3003 | d_rand:  0.7689
INFO:src.model.gail:iter: 229000 | update: 229 | d_loss: 1.73 | pi_loss:  0.13 | d_exp:  0.4293 | d_nov:  0.3838 | d_rand:  0.7746
INFO:src.model.gail:iter: 230000 | update: 230 | d_loss: 1.62 | pi_loss: -0.05 | d_exp:  0.4558 | d_nov:  0.4570 | d_rand:  0.7794
INFO:src.model.gail:iter: 231000 | update: 231 | d_loss: 1.43 | pi_loss:  0.06 | d_exp:  0.4774 | d_nov:  0.5360 | d_rand:  0.7824
INFO:src.model.gail:iter: 232000 | update: 232 | d_loss: 1.34 | pi_loss: -0.02 | d_exp:  0.4918 | d_nov:  0.6122 | d_rand:  0.7843
INFO:src.model.gail:iter: 233000 | update: 233 | d_loss: 1.49 | pi_loss:  0.01 | d_exp:  0.5006 | d_nov:  0.5537 | d_rand:  0.7856
INFO:src.model.gail:iter: 234000 | update: 234 | d_loss: 1.45 | pi_loss:  0.02 | d_exp:  0.5047 | d_nov:  0.5869 | d_rand:  0.7866
INFO:src.model.gail:iter: 235000 | update: 235 | d_loss: 1.53 | pi_loss:  0.01 | d_exp:  0.5042 | d_nov:  0.5785 | d_rand:  0.7872
INFO:src.model.gail:iter: 236000 | update: 236 | d_loss: 1.53 | pi_loss: -0.05 | d_exp:  0.5011 | d_nov:  0.5395 | d_rand:  0.7873
INFO:src.model.gail:iter: 237000 | update: 237 | d_loss: 1.64 | pi_loss:  0.13 | d_exp:  0.4977 | d_nov:  0.4651 | d_rand:  0.7869
INFO:src.model.gail:iter: 238000 | update: 238 | d_loss: 1.48 | pi_loss:  0.06 | d_exp:  0.4928 | d_nov:  0.5709 | d_rand:  0.7865
INFO:src.model.gail:iter: 239000 | update: 239 | d_loss: 1.31 | pi_loss:  0.08 | d_exp:  0.4855 | d_nov:  0.5994 | d_rand:  0.7855
INFO:src.model.gail:iter: 240000 | update: 240 | d_loss: 1.32 | pi_loss:  0.12 | d_exp:  0.4764 | d_nov:  0.6043 | d_rand:  0.7840
INFO:src.model.gail:iter: 241000 | update: 241 | d_loss: 1.26 | pi_loss:  0.06 | d_exp:  0.4658 | d_nov:  0.6110 | d_rand:  0.7820
INFO:src.model.gail:iter: 242000 | update: 242 | d_loss: 1.20 | pi_loss:  0.02 | d_exp:  0.4540 | d_nov:  0.6283 | d_rand:  0.7794
INFO:src.model.gail:iter: 243000 | update: 243 | d_loss: 1.15 | pi_loss:  0.01 | d_exp:  0.4410 | d_nov:  0.6488 | d_rand:  0.7766
INFO:src.model.gail:iter: 244000 | update: 244 | d_loss: 1.15 | pi_loss: -0.01 | d_exp:  0.4273 | d_nov:  0.6461 | d_rand:  0.7738
INFO:src.model.gail:iter: 245000 | update: 245 | d_loss: 1.06 | pi_loss:  0.00 | d_exp:  0.4129 | d_nov:  0.6689 | d_rand:  0.7706
INFO:src.model.gail:iter: 246000 | update: 246 | d_loss: 1.12 | pi_loss: -0.03 | d_exp:  0.3994 | d_nov:  0.6208 | d_rand:  0.7674
INFO:src.model.gail:iter: 247000 | update: 247 | d_loss: 1.05 | pi_loss:  0.00 | d_exp:  0.3867 | d_nov:  0.6378 | d_rand:  0.7642
INFO:src.model.gail:iter: 248000 | update: 248 | d_loss: 0.99 | pi_loss:  0.02 | d_exp:  0.3745 | d_nov:  0.6573 | d_rand:  0.7609
INFO:src.model.gail:iter: 249000 | update: 249 | d_loss: 1.05 | pi_loss: -0.01 | d_exp:  0.3636 | d_nov:  0.6420 | d_rand:  0.7579
INFO:src.model.gail:iter: 250000 | update: 250 | d_loss: 1.07 | pi_loss: -0.00 | d_exp:  0.3539 | d_nov:  0.6112 | d_rand:  0.7551
INFO:src.model.gail:iter: 251000 | update: 251 | d_loss: 1.06 | pi_loss:  0.10 | d_exp:  0.3470 | d_nov:  0.6002 | d_rand:  0.7527
INFO:src.model.gail:iter: 252000 | update: 252 | d_loss: 1.11 | pi_loss:  0.05 | d_exp:  0.3425 | d_nov:  0.5586 | d_rand:  0.7507
INFO:src.model.gail:iter: 253000 | update: 253 | d_loss: 1.06 | pi_loss: -0.05 | d_exp:  0.3385 | d_nov:  0.6160 | d_rand:  0.7493
INFO:src.model.gail:iter: 254000 | update: 254 | d_loss: 1.05 | pi_loss:  0.10 | d_exp:  0.3356 | d_nov:  0.5866 | d_rand:  0.7481
INFO:src.model.gail:iter: 255000 | update: 255 | d_loss: 0.90 | pi_loss: -0.03 | d_exp:  0.3327 | d_nov:  0.6735 | d_rand:  0.7470
INFO:src.model.gail:iter: 256000 | update: 256 | d_loss: 1.03 | pi_loss:  0.05 | d_exp:  0.3306 | d_nov:  0.5908 | d_rand:  0.7463
INFO:src.model.gail:iter: 257000 | update: 257 | d_loss: 0.96 | pi_loss:  0.04 | d_exp:  0.3294 | d_nov:  0.6223 | d_rand:  0.7456
INFO:src.model.gail:iter: 258000 | update: 258 | d_loss: 0.99 | pi_loss:  0.04 | d_exp:  0.3287 | d_nov:  0.6067 | d_rand:  0.7451
INFO:src.model.gail:iter: 259000 | update: 259 | d_loss: 0.93 | pi_loss:  0.05 | d_exp:  0.3280 | d_nov:  0.6477 | d_rand:  0.7447
INFO:src.model.gail:iter: 260000 | update: 260 | d_loss: 0.92 | pi_loss:  0.04 | d_exp:  0.3270 | d_nov:  0.6505 | d_rand:  0.7441
INFO:src.model.gail:iter: 261000 | update: 261 | d_loss: 0.86 | pi_loss:  0.03 | d_exp:  0.3254 | d_nov:  0.6848 | d_rand:  0.7432
INFO:src.model.gail:iter: 262000 | update: 262 | d_loss: 0.89 | pi_loss:  0.02 | d_exp:  0.3234 | d_nov:  0.6718 | d_rand:  0.7421
INFO:src.model.gail:iter: 263000 | update: 263 | d_loss: 0.73 | pi_loss:  0.03 | d_exp:  0.3199 | d_nov:  0.7633 | d_rand:  0.7407
INFO:src.model.gail:iter: 264000 | update: 264 | d_loss: 0.76 | pi_loss: -0.01 | d_exp:  0.3150 | d_nov:  0.7488 | d_rand:  0.7390
INFO:src.model.gail:iter: 265000 | update: 265 | d_loss: 0.75 | pi_loss: -0.02 | d_exp:  0.3091 | d_nov:  0.7620 | d_rand:  0.7373
INFO:src.model.gail:iter: 266000 | update: 266 | d_loss: 0.71 | pi_loss: -0.01 | d_exp:  0.3026 | d_nov:  0.7627 | d_rand:  0.7355
INFO:src.model.gail:iter: 267000 | update: 267 | d_loss: 0.73 | pi_loss: -0.02 | d_exp:  0.2959 | d_nov:  0.7534 | d_rand:  0.7338
INFO:src.model.gail:iter: 268000 | update: 268 | d_loss: 0.70 | pi_loss:  0.01 | d_exp:  0.2896 | d_nov:  0.7547 | d_rand:  0.7321
INFO:src.model.gail:iter: 269000 | update: 269 | d_loss: 0.65 | pi_loss: -0.02 | d_exp:  0.2831 | d_nov:  0.7931 | d_rand:  0.7304
INFO:src.model.gail:iter: 270000 | update: 270 | d_loss: 0.65 | pi_loss:  0.03 | d_exp:  0.2765 | d_nov:  0.7804 | d_rand:  0.7285
INFO:src.model.gail:iter: 271000 | update: 271 | d_loss: 0.61 | pi_loss: -0.01 | d_exp:  0.2698 | d_nov:  0.8003 | d_rand:  0.7265
INFO:src.model.gail:iter: 272000 | update: 272 | d_loss: 0.61 | pi_loss: -0.01 | d_exp:  0.2631 | d_nov:  0.7888 | d_rand:  0.7245
INFO:src.model.gail:iter: 273000 | update: 273 | d_loss: 0.58 | pi_loss: -0.01 | d_exp:  0.2564 | d_nov:  0.8085 | d_rand:  0.7224
INFO:src.model.gail:iter: 274000 | update: 274 | d_loss: 0.59 | pi_loss: -0.03 | d_exp:  0.2499 | d_nov:  0.7986 | d_rand:  0.7204
INFO:src.model.gail:iter: 275000 | update: 275 | d_loss: 0.57 | pi_loss: -0.02 | d_exp:  0.2435 | d_nov:  0.8062 | d_rand:  0.7184
INFO:src.model.gail:iter: 276000 | update: 276 | d_loss: 0.59 | pi_loss:  0.00 | d_exp:  0.2379 | d_nov:  0.7877 | d_rand:  0.7167
INFO:src.model.gail:iter: 277000 | update: 277 | d_loss: 0.61 | pi_loss:  0.02 | d_exp:  0.2333 | d_nov:  0.7604 | d_rand:  0.7154
INFO:src.model.gail:iter: 278000 | update: 278 | d_loss: 0.59 | pi_loss: -0.03 | d_exp:  0.2296 | d_nov:  0.7777 | d_rand:  0.7145
INFO:src.model.gail:iter: 279000 | update: 279 | d_loss: 0.57 | pi_loss: -0.01 | d_exp:  0.2268 | d_nov:  0.7804 | d_rand:  0.7139
INFO:src.model.gail:iter: 280000 | update: 280 | d_loss: 0.55 | pi_loss:  0.04 | d_exp:  0.2243 | d_nov:  0.7978 | d_rand:  0.7133
INFO:src.model.gail:iter: 281000 | update: 281 | d_loss: 0.64 | pi_loss:  0.00 | d_exp:  0.2231 | d_nov:  0.7303 | d_rand:  0.7131
INFO:src.model.gail:iter: 282000 | update: 282 | d_loss: 0.60 | pi_loss:  0.03 | d_exp:  0.2228 | d_nov:  0.7620 | d_rand:  0.7130
INFO:src.model.gail:iter: 283000 | update: 283 | d_loss: 0.81 | pi_loss:  0.10 | d_exp:  0.2242 | d_nov:  0.6199 | d_rand:  0.7138
INFO:src.model.gail:iter: 284000 | update: 284 | d_loss: 0.79 | pi_loss:  0.12 | d_exp:  0.2277 | d_nov:  0.6244 | d_rand:  0.7152
INFO:src.model.gail:iter: 285000 | update: 285 | d_loss: 0.63 | pi_loss:  0.01 | d_exp:  0.2315 | d_nov:  0.7443 | d_rand:  0.7168
INFO:src.model.gail:iter: 286000 | update: 286 | d_loss: 0.46 | pi_loss: -0.10 | d_exp:  0.2336 | d_nov:  0.8700 | d_rand:  0.7180
INFO:src.model.gail:iter: 287000 | update: 287 | d_loss: 0.63 | pi_loss:  0.18 | d_exp:  0.2350 | d_nov:  0.7440 | d_rand:  0.7193
INFO:src.model.gail:iter: 288000 | update: 288 | d_loss: 0.46 | pi_loss:  0.02 | d_exp:  0.2348 | d_nov:  0.8781 | d_rand:  0.7201
INFO:src.model.gail:iter: 289000 | update: 289 | d_loss: 0.99 | pi_loss:  0.13 | d_exp:  0.2365 | d_nov:  0.5255 | d_rand:  0.7216
INFO:src.model.gail:iter: 290000 | update: 290 | d_loss: 0.91 | pi_loss:  0.10 | d_exp:  0.2403 | d_nov:  0.5710 | d_rand:  0.7240
INFO:src.model.gail:iter: 291000 | update: 291 | d_loss: 0.83 | pi_loss:  0.12 | d_exp:  0.2456 | d_nov:  0.6262 | d_rand:  0.7266
INFO:src.model.gail:iter: 292000 | update: 292 | d_loss: 0.61 | pi_loss:  0.15 | d_exp:  0.2501 | d_nov:  0.7736 | d_rand:  0.7287
INFO:src.model.gail:iter: 293000 | update: 293 | d_loss: 0.79 | pi_loss:  0.08 | d_exp:  0.2551 | d_nov:  0.6611 | d_rand:  0.7306
INFO:src.model.gail:iter: 294000 | update: 294 | d_loss: 0.63 | pi_loss:  0.02 | d_exp:  0.2601 | d_nov:  0.7691 | d_rand:  0.7319
INFO:src.model.gail:iter: 295000 | update: 295 | d_loss: 0.56 | pi_loss: -0.03 | d_exp:  0.2634 | d_nov:  0.8181 | d_rand:  0.7326
INFO:src.model.gail:iter: 296000 | update: 296 | d_loss: 0.63 | pi_loss:  0.17 | d_exp:  0.2655 | d_nov:  0.7682 | d_rand:  0.7327
INFO:src.model.gail:iter: 297000 | update: 297 | d_loss: 0.51 | pi_loss: -0.17 | d_exp:  0.2657 | d_nov:  0.8585 | d_rand:  0.7320
INFO:src.model.gail:iter: 298000 | update: 298 | d_loss: 0.49 | pi_loss:  0.09 | d_exp:  0.2637 | d_nov:  0.8794 | d_rand:  0.7307
INFO:src.model.gail:iter: 299000 | update: 299 | d_loss: 0.44 | pi_loss:  0.11 | d_exp:  0.2594 | d_nov:  0.9059 | d_rand:  0.7286
INFO:src.model.gail:iter: 300000 | update: 300 | d_loss: 0.38 | pi_loss: -0.25 | d_exp:  0.2528 | d_nov:  0.9636 | d_rand:  0.7257
INFO:src.model.gail:iter: 301000 | update: 301 | d_loss: 0.38 | pi_loss:  0.10 | d_exp:  0.2446 | d_nov:  0.9472 | d_rand:  0.7222
INFO:src.model.gail:iter: 302000 | update: 302 | d_loss: 0.43 | pi_loss:  0.27 | d_exp:  0.2357 | d_nov:  0.8950 | d_rand:  0.7184
INFO:src.model.gail:iter: 303000 | update: 303 | d_loss: 0.46 | pi_loss:  0.25 | d_exp:  0.2273 | d_nov:  0.8498 | d_rand:  0.7147
INFO:src.model.gail:iter: 304000 | update: 304 | d_loss: 0.64 | pi_loss:  0.03 | d_exp:  0.2209 | d_nov:  0.7161 | d_rand:  0.7116
INFO:src.model.gail:iter: 305000 | update: 305 | d_loss: 0.59 | pi_loss:  0.02 | d_exp:  0.2161 | d_nov:  0.7515 | d_rand:  0.7092
INFO:src.model.gail:iter: 306000 | update: 306 | d_loss: 0.53 | pi_loss: -0.03 | d_exp:  0.2122 | d_nov:  0.7895 | d_rand:  0.7073
INFO:src.model.gail:iter: 307000 | update: 307 | d_loss: 0.51 | pi_loss:  0.00 | d_exp:  0.2090 | d_nov:  0.8069 | d_rand:  0.7057
INFO:src.model.gail:iter: 308000 | update: 308 | d_loss: 0.50 | pi_loss: -0.03 | d_exp:  0.2061 | d_nov:  0.8054 | d_rand:  0.7044
INFO:src.model.gail:iter: 309000 | update: 309 | d_loss: 0.61 | pi_loss:  0.25 | d_exp:  0.2044 | d_nov:  0.7197 | d_rand:  0.7035
INFO:src.model.gail:iter: 310000 | update: 310 | d_loss: 0.49 | pi_loss: -0.12 | d_exp:  0.2032 | d_nov:  0.8041 | d_rand:  0.7030
INFO:src.model.gail:iter: 311000 | update: 311 | d_loss: 0.63 | pi_loss: -0.11 | d_exp:  0.2030 | d_nov:  0.7046 | d_rand:  0.7029
INFO:src.model.gail:iter: 312000 | update: 312 | d_loss: 0.58 | pi_loss:  0.19 | d_exp:  0.2034 | d_nov:  0.7356 | d_rand:  0.7030
INFO:src.model.gail:iter: 313000 | update: 313 | d_loss: 0.59 | pi_loss:  0.02 | d_exp:  0.2043 | d_nov:  0.7351 | d_rand:  0.7034
INFO:src.model.gail:iter: 314000 | update: 314 | d_loss: 0.57 | pi_loss:  0.05 | d_exp:  0.2057 | d_nov:  0.7541 | d_rand:  0.7040
INFO:src.model.gail:iter: 315000 | update: 315 | d_loss: 0.68 | pi_loss: -0.00 | d_exp:  0.2085 | d_nov:  0.6827 | d_rand:  0.7051
INFO:src.model.gail:iter: 316000 | update: 316 | d_loss: 0.52 | pi_loss:  0.21 | d_exp:  0.2114 | d_nov:  0.7932 | d_rand:  0.7061
INFO:src.model.gail:iter: 317000 | update: 317 | d_loss: 0.73 | pi_loss: -0.20 | d_exp:  0.2154 | d_nov:  0.6671 | d_rand:  0.7075
INFO:src.model.gail:iter: 318000 | update: 318 | d_loss: 0.56 | pi_loss:  0.16 | d_exp:  0.2194 | d_nov:  0.7869 | d_rand:  0.7088
INFO:src.model.gail:iter: 319000 | update: 319 | d_loss: 0.45 | pi_loss: -0.02 | d_exp:  0.2222 | d_nov:  0.8499 | d_rand:  0.7096
INFO:src.model.gail:iter: 320000 | update: 320 | d_loss: 0.52 | pi_loss:  0.04 | d_exp:  0.2243 | d_nov:  0.8118 | d_rand:  0.7100
INFO:src.model.gail:iter: 321000 | update: 321 | d_loss: 0.51 | pi_loss:  0.09 | d_exp:  0.2257 | d_nov:  0.8254 | d_rand:  0.7102
INFO:src.model.gail:iter: 322000 | update: 322 | d_loss: 0.88 | pi_loss: -0.03 | d_exp:  0.2288 | d_nov:  0.5698 | d_rand:  0.7110
INFO:src.model.gail:iter: 323000 | update: 323 | d_loss: 0.69 | pi_loss:  0.03 | d_exp:  0.2329 | d_nov:  0.6978 | d_rand:  0.7121
INFO:src.model.gail:iter: 324000 | update: 324 | d_loss: 0.79 | pi_loss:  0.19 | d_exp:  0.2380 | d_nov:  0.6474 | d_rand:  0.7134
INFO:src.model.gail:iter: 325000 | update: 325 | d_loss: 1.00 | pi_loss:  0.13 | d_exp:  0.2447 | d_nov:  0.5201 | d_rand:  0.7152
INFO:src.model.gail:iter: 326000 | update: 326 | d_loss: 0.95 | pi_loss:  0.08 | d_exp:  0.2525 | d_nov:  0.5562 | d_rand:  0.7173
INFO:src.model.gail:iter: 327000 | update: 327 | d_loss: 1.13 | pi_loss:  0.12 | d_exp:  0.2624 | d_nov:  0.4771 | d_rand:  0.7199
INFO:src.model.gail:iter: 328000 | update: 328 | d_loss: 1.18 | pi_loss:  0.10 | d_exp:  0.2737 | d_nov:  0.4627 | d_rand:  0.7228
INFO:src.model.gail:iter: 329000 | update: 329 | d_loss: 1.19 | pi_loss:  0.05 | d_exp:  0.2861 | d_nov:  0.4680 | d_rand:  0.7259
INFO:src.model.gail:iter: 330000 | update: 330 | d_loss: 1.07 | pi_loss: -0.00 | d_exp:  0.2987 | d_nov:  0.5371 | d_rand:  0.7289
INFO:src.model.gail:iter: 331000 | update: 331 | d_loss: 0.77 | pi_loss:  0.04 | d_exp:  0.3092 | d_nov:  0.7189 | d_rand:  0.7315
INFO:src.model.gail:iter: 332000 | update: 332 | d_loss: 0.76 | pi_loss:  0.03 | d_exp:  0.3171 | d_nov:  0.7346 | d_rand:  0.7333
INFO:src.model.gail:iter: 333000 | update: 333 | d_loss: 0.87 | pi_loss:  0.15 | d_exp:  0.3235 | d_nov:  0.6762 | d_rand:  0.7346
INFO:src.model.gail:iter: 334000 | update: 334 | d_loss: 1.02 | pi_loss:  0.04 | d_exp:  0.3294 | d_nov:  0.5882 | d_rand:  0.7358
INFO:src.model.gail:iter: 335000 | update: 335 | d_loss: 1.05 | pi_loss:  0.08 | d_exp:  0.3351 | d_nov:  0.5714 | d_rand:  0.7370
INFO:src.model.gail:iter: 336000 | update: 336 | d_loss: 1.13 | pi_loss:  0.10 | d_exp:  0.3406 | d_nov:  0.5302 | d_rand:  0.7381
INFO:src.model.gail:iter: 337000 | update: 337 | d_loss: 1.11 | pi_loss:  0.09 | d_exp:  0.3461 | d_nov:  0.5505 | d_rand:  0.7393
INFO:src.model.gail:iter: 338000 | update: 338 | d_loss: 0.94 | pi_loss:  0.05 | d_exp:  0.3504 | d_nov:  0.6518 | d_rand:  0.7400
INFO:src.model.gail:iter: 339000 | update: 339 | d_loss: 0.95 | pi_loss:  0.05 | d_exp:  0.3535 | d_nov:  0.6481 | d_rand:  0.7404
INFO:src.model.gail:iter: 340000 | update: 340 | d_loss: 0.96 | pi_loss:  0.14 | d_exp:  0.3556 | d_nov:  0.6389 | d_rand:  0.7405
INFO:src.model.gail:iter: 341000 | update: 341 | d_loss: 0.89 | pi_loss:  0.05 | d_exp:  0.3562 | d_nov:  0.6915 | d_rand:  0.7404
INFO:src.model.gail:iter: 342000 | update: 342 | d_loss: 0.88 | pi_loss:  0.13 | d_exp:  0.3555 | d_nov:  0.6931 | d_rand:  0.7398
INFO:src.model.gail:iter: 343000 | update: 343 | d_loss: 0.91 | pi_loss:  0.04 | d_exp:  0.3540 | d_nov:  0.6721 | d_rand:  0.7391
INFO:src.model.gail:iter: 344000 | update: 344 | d_loss: 0.89 | pi_loss:  0.06 | d_exp:  0.3519 | d_nov:  0.6824 | d_rand:  0.7382
INFO:src.model.gail:iter: 345000 | update: 345 | d_loss: 0.95 | pi_loss:  0.08 | d_exp:  0.3495 | d_nov:  0.6567 | d_rand:  0.7374
INFO:src.model.gail:iter: 346000 | update: 346 | d_loss: 0.86 | pi_loss:  0.05 | d_exp:  0.3467 | d_nov:  0.6959 | d_rand:  0.7365
INFO:src.model.gail:iter: 347000 | update: 347 | d_loss: 0.81 | pi_loss:  0.02 | d_exp:  0.3432 | d_nov:  0.7312 | d_rand:  0.7354
INFO:src.model.gail:iter: 348000 | update: 348 | d_loss: 0.86 | pi_loss:  0.09 | d_exp:  0.3394 | d_nov:  0.6929 | d_rand:  0.7343
INFO:src.model.gail:iter: 349000 | update: 349 | d_loss: 0.81 | pi_loss:  0.08 | d_exp:  0.3355 | d_nov:  0.7149 | d_rand:  0.7331
INFO:src.model.gail:iter: 350000 | update: 350 | d_loss: 0.80 | pi_loss: -0.01 | d_exp:  0.3314 | d_nov:  0.7292 | d_rand:  0.7317
INFO:src.model.gail:iter: 351000 | update: 351 | d_loss: 0.66 | pi_loss:  0.12 | d_exp:  0.3264 | d_nov:  0.8176 | d_rand:  0.7301
INFO:src.model.gail:iter: 352000 | update: 352 | d_loss: 0.82 | pi_loss: -0.01 | d_exp:  0.3216 | d_nov:  0.6954 | d_rand:  0.7285
INFO:src.model.gail:iter: 353000 | update: 353 | d_loss: 0.77 | pi_loss:  0.05 | d_exp:  0.3171 | d_nov:  0.7286 | d_rand:  0.7271
INFO:src.model.gail:iter: 354000 | update: 354 | d_loss: 0.67 | pi_loss: -0.01 | d_exp:  0.3123 | d_nov:  0.7857 | d_rand:  0.7255
INFO:src.model.gail:iter: 355000 | update: 355 | d_loss: 0.67 | pi_loss:  0.15 | d_exp:  0.3073 | d_nov:  0.8023 | d_rand:  0.7239
INFO:src.model.gail:iter: 356000 | update: 356 | d_loss: 0.70 | pi_loss:  0.01 | d_exp:  0.3023 | d_nov:  0.7618 | d_rand:  0.7224
INFO:src.model.gail:iter: 357000 | update: 357 | d_loss: 0.63 | pi_loss: -0.11 | d_exp:  0.2972 | d_nov:  0.8153 | d_rand:  0.7208
INFO:src.model.gail:iter: 358000 | update: 358 | d_loss: 0.67 | pi_loss:  0.00 | d_exp:  0.2922 | d_nov:  0.7668 | d_rand:  0.7192
INFO:src.model.gail:iter: 359000 | update: 359 | d_loss: 0.63 | pi_loss:  0.23 | d_exp:  0.2873 | d_nov:  0.7936 | d_rand:  0.7177
INFO:src.model.gail:iter: 360000 | update: 360 | d_loss: 0.52 | pi_loss: -0.12 | d_exp:  0.2820 | d_nov:  0.8750 | d_rand:  0.7161
INFO:src.model.gail:iter: 361000 | update: 361 | d_loss: 0.51 | pi_loss:  0.13 | d_exp:  0.2762 | d_nov:  0.8791 | d_rand:  0.7143
INFO:src.model.gail:iter: 362000 | update: 362 | d_loss: 0.56 | pi_loss: -0.14 | d_exp:  0.2705 | d_nov:  0.8288 | d_rand:  0.7125
INFO:src.model.gail:iter: 363000 | update: 363 | d_loss: 0.65 | pi_loss:  0.13 | d_exp:  0.2654 | d_nov:  0.7533 | d_rand:  0.7110
INFO:src.model.gail:iter: 364000 | update: 364 | d_loss: 0.74 | pi_loss:  0.15 | d_exp:  0.2615 | d_nov:  0.6818 | d_rand:  0.7098
INFO:src.model.gail:iter: 365000 | update: 365 | d_loss: 0.58 | pi_loss: -0.11 | d_exp:  0.2580 | d_nov:  0.8000 | d_rand:  0.7087
INFO:src.model.gail:iter: 366000 | update: 366 | d_loss: 0.68 | pi_loss:  0.15 | d_exp:  0.2550 | d_nov:  0.7199 | d_rand:  0.7079
INFO:src.model.gail:iter: 367000 | update: 367 | d_loss: 0.65 | pi_loss: -0.05 | d_exp:  0.2526 | d_nov:  0.7328 | d_rand:  0.7072
INFO:src.model.gail:iter: 368000 | update: 368 | d_loss: 0.63 | pi_loss:  0.04 | d_exp:  0.2507 | d_nov:  0.7498 | d_rand:  0.7067
INFO:src.model.gail:iter: 369000 | update: 369 | d_loss: 0.67 | pi_loss:  0.10 | d_exp:  0.2492 | d_nov:  0.7144 | d_rand:  0.7063
INFO:src.model.gail:iter: 370000 | update: 370 | d_loss: 0.57 | pi_loss:  0.06 | d_exp:  0.2477 | d_nov:  0.7907 | d_rand:  0.7059
INFO:src.model.gail:iter: 371000 | update: 371 | d_loss: 0.64 | pi_loss:  0.28 | d_exp:  0.2463 | d_nov:  0.7386 | d_rand:  0.7056
INFO:src.model.gail:iter: 372000 | update: 372 | d_loss: 0.58 | pi_loss: -0.05 | d_exp:  0.2450 | d_nov:  0.7875 | d_rand:  0.7053
INFO:src.model.gail:iter: 373000 | update: 373 | d_loss: 0.53 | pi_loss: -0.14 | d_exp:  0.2433 | d_nov:  0.8228 | d_rand:  0.7049
INFO:src.model.gail:iter: 374000 | update: 374 | d_loss: 0.51 | pi_loss:  0.07 | d_exp:  0.2414 | d_nov:  0.8267 | d_rand:  0.7043
INFO:src.model.gail:iter: 375000 | update: 375 | d_loss: 0.52 | pi_loss:  0.06 | d_exp:  0.2393 | d_nov:  0.8178 | d_rand:  0.7037
INFO:src.model.gail:iter: 376000 | update: 376 | d_loss: 0.51 | pi_loss: -0.09 | d_exp:  0.2371 | d_nov:  0.8226 | d_rand:  0.7030
INFO:src.model.gail:iter: 377000 | update: 377 | d_loss: 0.50 | pi_loss: -0.00 | d_exp:  0.2347 | d_nov:  0.8381 | d_rand:  0.7023
INFO:src.model.gail:iter: 378000 | update: 378 | d_loss: 0.50 | pi_loss: -0.01 | d_exp:  0.2323 | d_nov:  0.8284 | d_rand:  0.7015
INFO:src.model.gail:iter: 379000 | update: 379 | d_loss: 0.44 | pi_loss:  0.04 | d_exp:  0.2296 | d_nov:  0.8751 | d_rand:  0.7006
INFO:src.model.gail:iter: 380000 | update: 380 | d_loss: 0.46 | pi_loss:  0.01 | d_exp:  0.2267 | d_nov:  0.8588 | d_rand:  0.6996
INFO:src.model.gail:iter: 381000 | update: 381 | d_loss: 0.51 | pi_loss:  0.06 | d_exp:  0.2241 | d_nov:  0.8125 | d_rand:  0.6987
INFO:src.model.gail:iter: 382000 | update: 382 | d_loss: 0.66 | pi_loss:  0.21 | d_exp:  0.2221 | d_nov:  0.6967 | d_rand:  0.6981
INFO:src.model.gail:iter: 383000 | update: 383 | d_loss: 0.48 | pi_loss: -0.10 | d_exp:  0.2204 | d_nov:  0.8188 | d_rand:  0.6975
INFO:src.model.gail:iter: 384000 | update: 384 | d_loss: 0.50 | pi_loss: -0.08 | d_exp:  0.2186 | d_nov:  0.8194 | d_rand:  0.6968
INFO:src.model.gail:iter: 385000 | update: 385 | d_loss: 0.50 | pi_loss:  0.09 | d_exp:  0.2169 | d_nov:  0.8082 | d_rand:  0.6962
INFO:src.model.gail:iter: 386000 | update: 386 | d_loss: 0.54 | pi_loss:  0.13 | d_exp:  0.2154 | d_nov:  0.7721 | d_rand:  0.6957
INFO:src.model.gail:iter: 387000 | update: 387 | d_loss: 0.55 | pi_loss:  0.15 | d_exp:  0.2142 | d_nov:  0.7671 | d_rand:  0.6953
INFO:src.model.gail:iter: 388000 | update: 388 | d_loss: 0.52 | pi_loss: -0.04 | d_exp:  0.2132 | d_nov:  0.7960 | d_rand:  0.6949
INFO:src.model.gail:iter: 389000 | update: 389 | d_loss: 0.68 | pi_loss: -0.07 | d_exp:  0.2127 | d_nov:  0.6805 | d_rand:  0.6949
INFO:src.model.gail:iter: 390000 | update: 390 | d_loss: 0.64 | pi_loss:  0.04 | d_exp:  0.2127 | d_nov:  0.7007 | d_rand:  0.6950
INFO:src.model.gail:iter: 391000 | update: 391 | d_loss: 0.64 | pi_loss:  0.21 | d_exp:  0.2131 | d_nov:  0.7115 | d_rand:  0.6953
INFO:src.model.gail:iter: 392000 | update: 392 | d_loss: 0.63 | pi_loss:  0.09 | d_exp:  0.2138 | d_nov:  0.7061 | d_rand:  0.6957
INFO:src.model.gail:iter: 393000 | update: 393 | d_loss: 0.54 | pi_loss: -0.20 | d_exp:  0.2145 | d_nov:  0.7752 | d_rand:  0.6961
INFO:src.model.gail:iter: 394000 | update: 394 | d_loss: 0.61 | pi_loss:  0.12 | d_exp:  0.2153 | d_nov:  0.7233 | d_rand:  0.6965
INFO:src.model.gail:iter: 395000 | update: 395 | d_loss: 0.53 | pi_loss: -0.01 | d_exp:  0.2159 | d_nov:  0.7806 | d_rand:  0.6968
INFO:src.model.gail:iter: 396000 | update: 396 | d_loss: 0.57 | pi_loss: -0.00 | d_exp:  0.2165 | d_nov:  0.7548 | d_rand:  0.6970
INFO:src.model.gail:iter: 397000 | update: 397 | d_loss: 0.59 | pi_loss:  0.06 | d_exp:  0.2171 | d_nov:  0.7391 | d_rand:  0.6972
INFO:src.model.gail:iter: 398000 | update: 398 | d_loss: 0.48 | pi_loss:  0.03 | d_exp:  0.2174 | d_nov:  0.8261 | d_rand:  0.6974
INFO:src.model.gail:iter: 399000 | update: 399 | d_loss: 0.66 | pi_loss:  0.14 | d_exp:  0.2179 | d_nov:  0.7066 | d_rand:  0.6975
INFO:src.model.gail:iter: 400000 | update: 400 | d_loss: 0.59 | pi_loss:  0.14 | d_exp:  0.2184 | d_nov:  0.7400 | d_rand:  0.6977
INFO:src.model.gail:iter: 401000 | update: 401 | d_loss: 0.67 | pi_loss:  0.04 | d_exp:  0.2192 | d_nov:  0.7126 | d_rand:  0.6980
INFO:src.model.gail:iter: 402000 | update: 402 | d_loss: 0.65 | pi_loss: -0.18 | d_exp:  0.2203 | d_nov:  0.7084 | d_rand:  0.6985
INFO:src.model.gail:iter: 403000 | update: 403 | d_loss: 0.56 | pi_loss:  0.02 | d_exp:  0.2212 | d_nov:  0.7695 | d_rand:  0.6988
INFO:src.model.gail:iter: 404000 | update: 404 | d_loss: 0.53 | pi_loss:  0.10 | d_exp:  0.2219 | d_nov:  0.7953 | d_rand:  0.6991
INFO:src.model.gail:iter: 405000 | update: 405 | d_loss: 0.64 | pi_loss:  0.24 | d_exp:  0.2227 | d_nov:  0.7095 | d_rand:  0.6994
INFO:src.model.gail:iter: 406000 | update: 406 | d_loss: 0.67 | pi_loss: -0.07 | d_exp:  0.2238 | d_nov:  0.6893 | d_rand:  0.6998
INFO:src.model.gail:iter: 407000 | update: 407 | d_loss: 0.59 | pi_loss: -0.06 | d_exp:  0.2249 | d_nov:  0.7481 | d_rand:  0.7002
INFO:src.model.gail:iter: 408000 | update: 408 | d_loss: 0.57 | pi_loss:  0.03 | d_exp:  0.2258 | d_nov:  0.7636 | d_rand:  0.7005
INFO:src.model.gail:iter: 409000 | update: 409 | d_loss: 0.59 | pi_loss:  0.05 | d_exp:  0.2265 | d_nov:  0.7583 | d_rand:  0.7007
INFO:src.model.gail:iter: 410000 | update: 410 | d_loss: 0.61 | pi_loss:  0.03 | d_exp:  0.2272 | d_nov:  0.7391 | d_rand:  0.7009
INFO:src.model.gail:iter: 411000 | update: 411 | d_loss: 0.56 | pi_loss:  0.05 | d_exp:  0.2278 | d_nov:  0.7720 | d_rand:  0.7011
INFO:src.model.gail:iter: 412000 | update: 412 | d_loss: 0.62 | pi_loss:  0.06 | d_exp:  0.2284 | d_nov:  0.7395 | d_rand:  0.7013
INFO:src.model.gail:iter: 413000 | update: 413 | d_loss: 0.59 | pi_loss:  0.13 | d_exp:  0.2289 | d_nov:  0.7516 | d_rand:  0.7015
INFO:src.model.gail:iter: 414000 | update: 414 | d_loss: 0.57 | pi_loss: -0.20 | d_exp:  0.2293 | d_nov:  0.7666 | d_rand:  0.7016
INFO:src.model.gail:iter: 415000 | update: 415 | d_loss: 0.58 | pi_loss:  0.01 | d_exp:  0.2295 | d_nov:  0.7640 | d_rand:  0.7017
INFO:src.model.gail:iter: 416000 | update: 416 | d_loss: 0.56 | pi_loss: -0.02 | d_exp:  0.2297 | d_nov:  0.7730 | d_rand:  0.7018
INFO:src.model.gail:iter: 417000 | update: 417 | d_loss: 0.56 | pi_loss:  0.07 | d_exp:  0.2297 | d_nov:  0.7805 | d_rand:  0.7017
INFO:src.model.gail:iter: 418000 | update: 418 | d_loss: 0.52 | pi_loss: -0.06 | d_exp:  0.2294 | d_nov:  0.8171 | d_rand:  0.7016
INFO:src.model.gail:iter: 419000 | update: 419 | d_loss: 0.52 | pi_loss:  0.12 | d_exp:  0.2290 | d_nov:  0.8001 | d_rand:  0.7014
INFO:src.model.gail:iter: 420000 | update: 420 | d_loss: 0.57 | pi_loss:  0.08 | d_exp:  0.2286 | d_nov:  0.7688 | d_rand:  0.7012
INFO:src.model.gail:iter: 421000 | update: 421 | d_loss: 0.55 | pi_loss: -0.03 | d_exp:  0.2282 | d_nov:  0.7810 | d_rand:  0.7010
INFO:src.model.gail:iter: 422000 | update: 422 | d_loss: 0.60 | pi_loss:  0.09 | d_exp:  0.2278 | d_nov:  0.7602 | d_rand:  0.7009
INFO:src.model.gail:iter: 423000 | update: 423 | d_loss: 0.52 | pi_loss:  0.06 | d_exp:  0.2274 | d_nov:  0.8080 | d_rand:  0.7008
INFO:src.model.gail:iter: 424000 | update: 424 | d_loss: 0.49 | pi_loss: -0.09 | d_exp:  0.2268 | d_nov:  0.8192 | d_rand:  0.7006
INFO:src.model.gail:iter: 425000 | update: 425 | d_loss: 0.59 | pi_loss: -0.04 | d_exp:  0.2262 | d_nov:  0.7615 | d_rand:  0.7005
INFO:src.model.gail:iter: 426000 | update: 426 | d_loss: 0.58 | pi_loss:  0.18 | d_exp:  0.2258 | d_nov:  0.7605 | d_rand:  0.7004
INFO:src.model.gail:iter: 427000 | update: 427 | d_loss: 0.57 | pi_loss: -0.30 | d_exp:  0.2254 | d_nov:  0.7745 | d_rand:  0.7003
INFO:src.model.gail:iter: 428000 | update: 428 | d_loss: 0.59 | pi_loss: -0.06 | d_exp:  0.2252 | d_nov:  0.7520 | d_rand:  0.7003
INFO:src.model.gail:iter: 429000 | update: 429 | d_loss: 0.67 | pi_loss:  0.20 | d_exp:  0.2252 | d_nov:  0.7095 | d_rand:  0.7004
INFO:src.model.gail:iter: 430000 | update: 430 | d_loss: 0.55 | pi_loss: -0.07 | d_exp:  0.2253 | d_nov:  0.7835 | d_rand:  0.7004
INFO:src.model.gail:iter: 431000 | update: 431 | d_loss: 0.55 | pi_loss:  0.09 | d_exp:  0.2254 | d_nov:  0.7769 | d_rand:  0.7003
INFO:src.model.gail:iter: 432000 | update: 432 | d_loss: 0.56 | pi_loss: -0.07 | d_exp:  0.2253 | d_nov:  0.7769 | d_rand:  0.7003
INFO:src.model.gail:iter: 433000 | update: 433 | d_loss: 0.66 | pi_loss:  0.04 | d_exp:  0.2254 | d_nov:  0.7227 | d_rand:  0.7003
INFO:src.model.gail:iter: 434000 | update: 434 | d_loss: 0.52 | pi_loss:  0.13 | d_exp:  0.2255 | d_nov:  0.8047 | d_rand:  0.7003
INFO:src.model.gail:iter: 435000 | update: 435 | d_loss: 0.50 | pi_loss: -0.01 | d_exp:  0.2253 | d_nov:  0.8232 | d_rand:  0.7002
INFO:src.model.gail:iter: 436000 | update: 436 | d_loss: 0.54 | pi_loss: -0.02 | d_exp:  0.2249 | d_nov:  0.8070 | d_rand:  0.7000
INFO:src.model.gail:iter: 437000 | update: 437 | d_loss: 0.44 | pi_loss: -0.06 | d_exp:  0.2243 | d_nov:  0.8616 | d_rand:  0.6998
INFO:src.model.gail:iter: 438000 | update: 438 | d_loss: 0.41 | pi_loss: -0.00 | d_exp:  0.2233 | d_nov:  0.8869 | d_rand:  0.6993
INFO:src.model.gail:iter: 439000 | update: 439 | d_loss: 0.45 | pi_loss:  0.02 | d_exp:  0.2221 | d_nov:  0.8520 | d_rand:  0.6988
INFO:src.model.gail:iter: 440000 | update: 440 | d_loss: 0.53 | pi_loss: -0.03 | d_exp:  0.2210 | d_nov:  0.8058 | d_rand:  0.6983
INFO:src.model.gail:iter: 441000 | update: 441 | d_loss: 0.55 | pi_loss:  0.01 | d_exp:  0.2200 | d_nov:  0.7777 | d_rand:  0.6979
INFO:src.model.gail:iter: 442000 | update: 442 | d_loss: 0.50 | pi_loss:  0.09 | d_exp:  0.2191 | d_nov:  0.8131 | d_rand:  0.6975
INFO:src.model.gail:iter: 443000 | update: 443 | d_loss: 0.46 | pi_loss: -0.05 | d_exp:  0.2181 | d_nov:  0.8445 | d_rand:  0.6971
INFO:src.model.gail:iter: 444000 | update: 444 | d_loss: 0.51 | pi_loss: -0.03 | d_exp:  0.2171 | d_nov:  0.8024 | d_rand:  0.6967
INFO:src.model.gail:iter: 445000 | update: 445 | d_loss: 0.52 | pi_loss: -0.02 | d_exp:  0.2162 | d_nov:  0.8069 | d_rand:  0.6963
INFO:src.model.gail:iter: 446000 | update: 446 | d_loss: 0.47 | pi_loss:  0.06 | d_exp:  0.2153 | d_nov:  0.8350 | d_rand:  0.6959
INFO:src.model.gail:iter: 447000 | update: 447 | d_loss: 0.51 | pi_loss:  0.00 | d_exp:  0.2145 | d_nov:  0.8165 | d_rand:  0.6956
INFO:src.model.gail:iter: 448000 | update: 448 | d_loss: 0.46 | pi_loss:  0.09 | d_exp:  0.2135 | d_nov:  0.8382 | d_rand:  0.6953
INFO:src.model.gail:iter: 449000 | update: 449 | d_loss: 0.47 | pi_loss: -0.05 | d_exp:  0.2126 | d_nov:  0.8407 | d_rand:  0.6949
INFO:src.model.gail:iter: 450000 | update: 450 | d_loss: 0.50 | pi_loss: -0.12 | d_exp:  0.2117 | d_nov:  0.8130 | d_rand:  0.6946
INFO:src.model.gail:iter: 451000 | update: 451 | d_loss: 0.62 | pi_loss:  0.10 | d_exp:  0.2111 | d_nov:  0.7253 | d_rand:  0.6944
INFO:src.model.gail:iter: 452000 | update: 452 | d_loss: 0.46 | pi_loss: -0.09 | d_exp:  0.2105 | d_nov:  0.8370 | d_rand:  0.6942
INFO:src.model.gail:iter: 453000 | update: 453 | d_loss: 0.57 | pi_loss: -0.04 | d_exp:  0.2101 | d_nov:  0.7605 | d_rand:  0.6941
INFO:src.model.gail:iter: 454000 | update: 454 | d_loss: 0.53 | pi_loss: -0.01 | d_exp:  0.2098 | d_nov:  0.7913 | d_rand:  0.6939
INFO:src.model.gail:iter: 455000 | update: 455 | d_loss: 0.50 | pi_loss:  0.03 | d_exp:  0.2095 | d_nov:  0.8169 | d_rand:  0.6938
INFO:src.model.gail:iter: 456000 | update: 456 | d_loss: 0.48 | pi_loss: -0.07 | d_exp:  0.2091 | d_nov:  0.8243 | d_rand:  0.6936
INFO:src.model.gail:iter: 457000 | update: 457 | d_loss: 0.54 | pi_loss:  0.21 | d_exp:  0.2087 | d_nov:  0.8067 | d_rand:  0.6935
INFO:src.model.gail:iter: 458000 | update: 458 | d_loss: 0.41 | pi_loss: -0.01 | d_exp:  0.2081 | d_nov:  0.8716 | d_rand:  0.6932
INFO:src.model.gail:iter: 459000 | update: 459 | d_loss: 0.44 | pi_loss: -0.09 | d_exp:  0.2074 | d_nov:  0.8442 | d_rand:  0.6929
INFO:src.model.gail:iter: 460000 | update: 460 | d_loss: 0.49 | pi_loss: -0.01 | d_exp:  0.2068 | d_nov:  0.8145 | d_rand:  0.6927
INFO:src.model.gail:iter: 461000 | update: 461 | d_loss: 0.57 | pi_loss:  0.17 | d_exp:  0.2062 | d_nov:  0.7708 | d_rand:  0.6925
INFO:src.model.gail:iter: 462000 | update: 462 | d_loss: 0.45 | pi_loss:  0.05 | d_exp:  0.2057 | d_nov:  0.8335 | d_rand:  0.6923
INFO:src.model.gail:iter: 463000 | update: 463 | d_loss: 0.59 | pi_loss: -0.22 | d_exp:  0.2053 | d_nov:  0.7644 | d_rand:  0.6922
INFO:src.model.gail:iter: 464000 | update: 464 | d_loss: 0.62 | pi_loss: -0.04 | d_exp:  0.2052 | d_nov:  0.7351 | d_rand:  0.6922
INFO:src.model.gail:iter: 465000 | update: 465 | d_loss: 0.57 | pi_loss:  0.15 | d_exp:  0.2053 | d_nov:  0.7520 | d_rand:  0.6923
INFO:src.model.gail:iter: 466000 | update: 466 | d_loss: 0.47 | pi_loss: -0.14 | d_exp:  0.2053 | d_nov:  0.8192 | d_rand:  0.6923
INFO:src.model.gail:iter: 467000 | update: 467 | d_loss: 0.51 | pi_loss: -0.07 | d_exp:  0.2053 | d_nov:  0.7916 | d_rand:  0.6923
INFO:src.model.gail:iter: 468000 | update: 468 | d_loss: 0.51 | pi_loss:  0.08 | d_exp:  0.2052 | d_nov:  0.8011 | d_rand:  0.6923
INFO:src.model.gail:iter: 469000 | update: 469 | d_loss: 0.47 | pi_loss:  0.09 | d_exp:  0.2051 | d_nov:  0.8200 | d_rand:  0.6922
INFO:src.model.gail:iter: 470000 | update: 470 | d_loss: 0.55 | pi_loss: -0.03 | d_exp:  0.2050 | d_nov:  0.7823 | d_rand:  0.6922
INFO:src.model.gail:iter: 471000 | update: 471 | d_loss: 0.61 | pi_loss:  0.10 | d_exp:  0.2051 | d_nov:  0.7346 | d_rand:  0.6922
INFO:src.model.gail:iter: 472000 | update: 472 | d_loss: 0.58 | pi_loss: -0.06 | d_exp:  0.2053 | d_nov:  0.7481 | d_rand:  0.6923
INFO:src.model.gail:iter: 473000 | update: 473 | d_loss: 0.51 | pi_loss:  0.07 | d_exp:  0.2055 | d_nov:  0.7995 | d_rand:  0.6923
INFO:src.model.gail:iter: 474000 | update: 474 | d_loss: 0.56 | pi_loss:  0.04 | d_exp:  0.2056 | d_nov:  0.7785 | d_rand:  0.6924
INFO:src.model.gail:iter: 475000 | update: 475 | d_loss: 0.58 | pi_loss: -0.16 | d_exp:  0.2059 | d_nov:  0.7510 | d_rand:  0.6925
INFO:src.model.gail:iter: 476000 | update: 476 | d_loss: 0.55 | pi_loss:  0.10 | d_exp:  0.2062 | d_nov:  0.7706 | d_rand:  0.6926
INFO:src.model.gail:iter: 477000 | update: 477 | d_loss: 0.54 | pi_loss:  0.12 | d_exp:  0.2064 | d_nov:  0.7763 | d_rand:  0.6928
INFO:src.model.gail:iter: 478000 | update: 478 | d_loss: 0.53 | pi_loss: -0.23 | d_exp:  0.2066 | d_nov:  0.7919 | d_rand:  0.6929
INFO:src.model.gail:iter: 479000 | update: 479 | d_loss: 0.59 | pi_loss:  0.14 | d_exp:  0.2069 | d_nov:  0.7510 | d_rand:  0.6930
INFO:src.model.gail:iter: 480000 | update: 480 | d_loss: 0.57 | pi_loss: -0.10 | d_exp:  0.2072 | d_nov:  0.7537 | d_rand:  0.6931
INFO:src.model.gail:iter: 481000 | update: 481 | d_loss: 0.46 | pi_loss:  0.12 | d_exp:  0.2073 | d_nov:  0.8303 | d_rand:  0.6932
INFO:src.model.gail:iter: 482000 | update: 482 | d_loss: 0.50 | pi_loss: -0.23 | d_exp:  0.2074 | d_nov:  0.8060 | d_rand:  0.6932
INFO:src.model.gail:iter: 483000 | update: 483 | d_loss: 0.51 | pi_loss: -0.16 | d_exp:  0.2074 | d_nov:  0.7973 | d_rand:  0.6932
INFO:src.model.gail:iter: 484000 | update: 484 | d_loss: 0.56 | pi_loss:  0.06 | d_exp:  0.2074 | d_nov:  0.7717 | d_rand:  0.6932
INFO:src.model.gail:iter: 485000 | update: 485 | d_loss: 0.58 | pi_loss:  0.35 | d_exp:  0.2075 | d_nov:  0.7484 | d_rand:  0.6932
INFO:src.model.gail:iter: 486000 | update: 486 | d_loss: 0.63 | pi_loss: -0.20 | d_exp:  0.2077 | d_nov:  0.7346 | d_rand:  0.6933
INFO:src.model.gail:iter: 487000 | update: 487 | d_loss: 0.56 | pi_loss: -0.09 | d_exp:  0.2080 | d_nov:  0.7584 | d_rand:  0.6934
INFO:src.model.gail:iter: 488000 | update: 488 | d_loss: 0.57 | pi_loss: -0.03 | d_exp:  0.2083 | d_nov:  0.7636 | d_rand:  0.6935
INFO:src.model.gail:iter: 489000 | update: 489 | d_loss: 0.58 | pi_loss:  0.16 | d_exp:  0.2086 | d_nov:  0.7493 | d_rand:  0.6937
INFO:src.model.gail:iter: 490000 | update: 490 | d_loss: 0.47 | pi_loss:  0.08 | d_exp:  0.2088 | d_nov:  0.8164 | d_rand:  0.6938
INFO:src.model.gail:iter: 491000 | update: 491 | d_loss: 0.50 | pi_loss: -0.08 | d_exp:  0.2089 | d_nov:  0.8038 | d_rand:  0.6938
INFO:src.model.gail:iter: 492000 | update: 492 | d_loss: 0.60 | pi_loss:  0.10 | d_exp:  0.2091 | d_nov:  0.7385 | d_rand:  0.6939
INFO:src.model.gail:iter: 493000 | update: 493 | d_loss: 0.54 | pi_loss: -0.13 | d_exp:  0.2093 | d_nov:  0.7824 | d_rand:  0.6939
INFO:src.model.gail:iter: 494000 | update: 494 | d_loss: 0.69 | pi_loss:  0.14 | d_exp:  0.2097 | d_nov:  0.7098 | d_rand:  0.6940
INFO:src.model.gail:iter: 495000 | update: 495 | d_loss: 0.57 | pi_loss: -0.04 | d_exp:  0.2100 | d_nov:  0.7613 | d_rand:  0.6942
INFO:src.model.gail:iter: 496000 | update: 496 | d_loss: 0.55 | pi_loss:  0.10 | d_exp:  0.2104 | d_nov:  0.7658 | d_rand:  0.6943
INFO:src.model.gail:iter: 497000 | update: 497 | d_loss: 0.46 | pi_loss: -0.12 | d_exp:  0.2106 | d_nov:  0.8371 | d_rand:  0.6944
INFO:src.model.gail:iter: 498000 | update: 498 | d_loss: 0.47 | pi_loss: -0.03 | d_exp:  0.2106 | d_nov:  0.8207 | d_rand:  0.6943
INFO:src.model.gail:iter: 499000 | update: 499 | d_loss: 0.56 | pi_loss:  0.04 | d_exp:  0.2107 | d_nov:  0.7690 | d_rand:  0.6943
INFO:src.model.gail:iter: 500000 | update: 500 | d_loss: 0.62 | pi_loss: -0.01 | d_exp:  0.2108 | d_nov:  0.7302 | d_rand:  0.6944
INFO:src.model.gail:iter: 501000 | update: 501 | d_loss: 0.58 | pi_loss:  0.09 | d_exp:  0.2111 | d_nov:  0.7549 | d_rand:  0.6944
INFO:src.model.gail:iter: 502000 | update: 502 | d_loss: 0.60 | pi_loss:  0.51 | d_exp:  0.2114 | d_nov:  0.7259 | d_rand:  0.6945
INFO:src.model.gail:iter: 503000 | update: 503 | d_loss: 0.52 | pi_loss: -0.06 | d_exp:  0.2116 | d_nov:  0.8008 | d_rand:  0.6946
INFO:src.model.gail:iter: 504000 | update: 504 | d_loss: 0.54 | pi_loss: -0.27 | d_exp:  0.2118 | d_nov:  0.7881 | d_rand:  0.6946
INFO:src.model.gail:iter: 505000 | update: 505 | d_loss: 0.52 | pi_loss: -0.04 | d_exp:  0.2119 | d_nov:  0.7996 | d_rand:  0.6947
INFO:src.model.gail:iter: 506000 | update: 506 | d_loss: 0.48 | pi_loss: -0.03 | d_exp:  0.2120 | d_nov:  0.8198 | d_rand:  0.6947
INFO:src.model.gail:iter: 507000 | update: 507 | d_loss: 0.50 | pi_loss: -0.03 | d_exp:  0.2120 | d_nov:  0.8065 | d_rand:  0.6946
INFO:src.model.gail:iter: 508000 | update: 508 | d_loss: 0.50 | pi_loss: -0.09 | d_exp:  0.2119 | d_nov:  0.8105 | d_rand:  0.6946
INFO:src.model.gail:iter: 509000 | update: 509 | d_loss: 0.54 | pi_loss:  0.01 | d_exp:  0.2118 | d_nov:  0.7990 | d_rand:  0.6945
INFO:src.model.gail:iter: 510000 | update: 510 | d_loss: 0.57 | pi_loss:  0.18 | d_exp:  0.2117 | d_nov:  0.7738 | d_rand:  0.6944
INFO:src.model.gail:iter: 511000 | update: 511 | d_loss: 0.54 | pi_loss:  0.09 | d_exp:  0.2117 | d_nov:  0.7813 | d_rand:  0.6944
INFO:src.model.gail:iter: 512000 | update: 512 | d_loss: 0.46 | pi_loss: -0.14 | d_exp:  0.2116 | d_nov:  0.8367 | d_rand:  0.6943
INFO:src.model.gail:iter: 513000 | update: 513 | d_loss: 0.57 | pi_loss: -0.11 | d_exp:  0.2116 | d_nov:  0.7640 | d_rand:  0.6943
INFO:src.model.gail:iter: 514000 | update: 514 | d_loss: 0.60 | pi_loss:  0.09 | d_exp:  0.2116 | d_nov:  0.7405 | d_rand:  0.6942
INFO:src.model.gail:iter: 515000 | update: 515 | d_loss: 0.52 | pi_loss:  0.12 | d_exp:  0.2117 | d_nov:  0.8014 | d_rand:  0.6942
INFO:src.model.gail:iter: 516000 | update: 516 | d_loss: 0.49 | pi_loss: -0.03 | d_exp:  0.2116 | d_nov:  0.8232 | d_rand:  0.6942
INFO:src.model.gail:iter: 517000 | update: 517 | d_loss: 0.60 | pi_loss:  0.01 | d_exp:  0.2116 | d_nov:  0.7404 | d_rand:  0.6942
INFO:src.model.gail:iter: 518000 | update: 518 | d_loss: 0.55 | pi_loss: -0.18 | d_exp:  0.2117 | d_nov:  0.7798 | d_rand:  0.6942
INFO:src.model.gail:iter: 519000 | update: 519 | d_loss: 0.49 | pi_loss: -0.09 | d_exp:  0.2117 | d_nov:  0.8217 | d_rand:  0.6942
INFO:src.model.gail:iter: 520000 | update: 520 | d_loss: 0.45 | pi_loss:  0.07 | d_exp:  0.2116 | d_nov:  0.8557 | d_rand:  0.6941
INFO:src.model.gail:iter: 521000 | update: 521 | d_loss: 0.54 | pi_loss: -0.07 | d_exp:  0.2115 | d_nov:  0.7936 | d_rand:  0.6941
INFO:src.model.gail:iter: 522000 | update: 522 | d_loss: 0.63 | pi_loss:  0.64 | d_exp:  0.2115 | d_nov:  0.7080 | d_rand:  0.6941
INFO:src.model.gail:iter: 523000 | update: 523 | d_loss: 0.47 | pi_loss: -0.14 | d_exp:  0.2114 | d_nov:  0.8289 | d_rand:  0.6940
INFO:src.model.gail:iter: 524000 | update: 524 | d_loss: 0.46 | pi_loss: -0.06 | d_exp:  0.2113 | d_nov:  0.8324 | d_rand:  0.6939
INFO:src.model.gail:iter: 525000 | update: 525 | d_loss: 0.47 | pi_loss:  0.05 | d_exp:  0.2111 | d_nov:  0.8278 | d_rand:  0.6939
INFO:src.model.gail:iter: 526000 | update: 526 | d_loss: 0.42 | pi_loss: -0.24 | d_exp:  0.2108 | d_nov:  0.8765 | d_rand:  0.6937
INFO:src.model.gail:iter: 527000 | update: 527 | d_loss: 0.50 | pi_loss:  0.05 | d_exp:  0.2105 | d_nov:  0.8086 | d_rand:  0.6936
INFO:src.model.gail:iter: 528000 | update: 528 | d_loss: 0.51 | pi_loss: -0.08 | d_exp:  0.2102 | d_nov:  0.8117 | d_rand:  0.6934
INFO:src.model.gail:iter: 529000 | update: 529 | d_loss: 0.52 | pi_loss: -0.10 | d_exp:  0.2100 | d_nov:  0.8057 | d_rand:  0.6933
INFO:src.model.gail:iter: 530000 | update: 530 | d_loss: 0.57 | pi_loss:  0.09 | d_exp:  0.2098 | d_nov:  0.7602 | d_rand:  0.6932
INFO:src.model.gail:iter: 531000 | update: 531 | d_loss: 0.44 | pi_loss: -0.09 | d_exp:  0.2096 | d_nov:  0.8454 | d_rand:  0.6931
INFO:src.model.gail:iter: 532000 | update: 532 | d_loss: 0.50 | pi_loss:  0.02 | d_exp:  0.2093 | d_nov:  0.8125 | d_rand:  0.6931
INFO:src.model.gail:iter: 533000 | update: 533 | d_loss: 0.59 | pi_loss:  0.44 | d_exp:  0.2092 | d_nov:  0.7369 | d_rand:  0.6930
INFO:src.model.gail:iter: 534000 | update: 534 | d_loss: 0.46 | pi_loss: -0.07 | d_exp:  0.2091 | d_nov:  0.8408 | d_rand:  0.6930
INFO:src.model.gail:iter: 535000 | update: 535 | d_loss: 0.50 | pi_loss: -0.11 | d_exp:  0.2089 | d_nov:  0.8144 | d_rand:  0.6929
INFO:src.model.gail:iter: 536000 | update: 536 | d_loss: 0.45 | pi_loss: -0.11 | d_exp:  0.2086 | d_nov:  0.8480 | d_rand:  0.6928
INFO:src.model.gail:iter: 537000 | update: 537 | d_loss: 0.48 | pi_loss: -0.01 | d_exp:  0.2084 | d_nov:  0.8193 | d_rand:  0.6928
INFO:src.model.gail:iter: 538000 | update: 538 | d_loss: 0.52 | pi_loss: -0.08 | d_exp:  0.2081 | d_nov:  0.8056 | d_rand:  0.6927
INFO:src.model.gail:iter: 539000 | update: 539 | d_loss: 0.52 | pi_loss:  0.05 | d_exp:  0.2080 | d_nov:  0.7862 | d_rand:  0.6926
INFO:src.model.gail:iter: 540000 | update: 540 | d_loss: 0.50 | pi_loss: -0.15 | d_exp:  0.2078 | d_nov:  0.8071 | d_rand:  0.6925
INFO:src.model.gail:iter: 541000 | update: 541 | d_loss: 0.42 | pi_loss: -0.15 | d_exp:  0.2076 | d_nov:  0.8588 | d_rand:  0.6924
INFO:src.model.gail:iter: 542000 | update: 542 | d_loss: 0.48 | pi_loss:  0.19 | d_exp:  0.2073 | d_nov:  0.8253 | d_rand:  0.6923
INFO:src.model.gail:iter: 543000 | update: 543 | d_loss: 0.42 | pi_loss: -0.04 | d_exp:  0.2070 | d_nov:  0.8658 | d_rand:  0.6922
INFO:src.model.gail:iter: 544000 | update: 544 | d_loss: 0.49 | pi_loss: -0.10 | d_exp:  0.2067 | d_nov:  0.8187 | d_rand:  0.6921
INFO:src.model.gail:iter: 545000 | update: 545 | d_loss: 0.56 | pi_loss:  0.15 | d_exp:  0.2065 | d_nov:  0.7610 | d_rand:  0.6920
INFO:src.model.gail:iter: 546000 | update: 546 | d_loss: 0.48 | pi_loss: -0.10 | d_exp:  0.2063 | d_nov:  0.8194 | d_rand:  0.6919
INFO:src.model.gail:iter: 547000 | update: 547 | d_loss: 0.48 | pi_loss:  0.03 | d_exp:  0.2061 | d_nov:  0.8284 | d_rand:  0.6918
INFO:src.model.gail:iter: 548000 | update: 548 | d_loss: 0.64 | pi_loss:  0.07 | d_exp:  0.2060 | d_nov:  0.7092 | d_rand:  0.6918
INFO:src.model.gail:iter: 549000 | update: 549 | d_loss: 0.64 | pi_loss:  0.28 | d_exp:  0.2061 | d_nov:  0.6938 | d_rand:  0.6919
INFO:src.model.gail:iter: 550000 | update: 550 | d_loss: 0.49 | pi_loss: -0.14 | d_exp:  0.2062 | d_nov:  0.8238 | d_rand:  0.6919
INFO:src.model.gail:iter: 551000 | update: 551 | d_loss: 0.43 | pi_loss:  0.02 | d_exp:  0.2062 | d_nov:  0.8521 | d_rand:  0.6919
INFO:src.model.gail:iter: 552000 | update: 552 | d_loss: 0.45 | pi_loss: -0.03 | d_exp:  0.2061 | d_nov:  0.8446 | d_rand:  0.6919
INFO:src.model.gail:iter: 553000 | update: 553 | d_loss: 0.55 | pi_loss:  0.08 | d_exp:  0.2060 | d_nov:  0.7688 | d_rand:  0.6919
INFO:src.model.gail:iter: 554000 | update: 554 | d_loss: 0.59 | pi_loss:  0.27 | d_exp:  0.2061 | d_nov:  0.7390 | d_rand:  0.6919
INFO:src.model.gail:iter: 555000 | update: 555 | d_loss: 0.53 | pi_loss: -0.21 | d_exp:  0.2061 | d_nov:  0.7999 | d_rand:  0.6919
INFO:src.model.gail:iter: 556000 | update: 556 | d_loss: 0.54 | pi_loss:  0.10 | d_exp:  0.2062 | d_nov:  0.7622 | d_rand:  0.6919
INFO:src.model.gail:iter: 557000 | update: 557 | d_loss: 0.54 | pi_loss: -0.02 | d_exp:  0.2063 | d_nov:  0.7759 | d_rand:  0.6919
INFO:src.model.gail:iter: 558000 | update: 558 | d_loss: 0.63 | pi_loss:  0.05 | d_exp:  0.2064 | d_nov:  0.7225 | d_rand:  0.6920
INFO:src.model.gail:iter: 559000 | update: 559 | d_loss: 0.46 | pi_loss: -0.14 | d_exp:  0.2065 | d_nov:  0.8335 | d_rand:  0.6921
INFO:src.model.gail:iter: 560000 | update: 560 | d_loss: 0.44 | pi_loss: -0.27 | d_exp:  0.2065 | d_nov:  0.8513 | d_rand:  0.6921
INFO:src.model.gail:iter: 561000 | update: 561 | d_loss: 0.45 | pi_loss: -0.02 | d_exp:  0.2065 | d_nov:  0.8396 | d_rand:  0.6921
INFO:src.model.gail:iter: 562000 | update: 562 | d_loss: 0.46 | pi_loss: -0.16 | d_exp:  0.2063 | d_nov:  0.8260 | d_rand:  0.6920
INFO:src.model.gail:iter: 563000 | update: 563 | d_loss: 0.57 | pi_loss: -0.09 | d_exp:  0.2063 | d_nov:  0.7850 | d_rand:  0.6920
INFO:src.model.gail:iter: 564000 | update: 564 | d_loss: 0.50 | pi_loss:  0.10 | d_exp:  0.2062 | d_nov:  0.7955 | d_rand:  0.6919
INFO:src.model.gail:iter: 565000 | update: 565 | d_loss: 0.47 | pi_loss:  0.10 | d_exp:  0.2061 | d_nov:  0.8315 | d_rand:  0.6919
INFO:src.model.gail:iter: 566000 | update: 566 | d_loss: 0.45 | pi_loss: -0.19 | d_exp:  0.2059 | d_nov:  0.8432 | d_rand:  0.6918
INFO:src.model.gail:iter: 567000 | update: 567 | d_loss: 0.54 | pi_loss:  0.25 | d_exp:  0.2059 | d_nov:  0.7763 | d_rand:  0.6918
INFO:src.model.gail:iter: 568000 | update: 568 | d_loss: 0.46 | pi_loss:  0.20 | d_exp:  0.2057 | d_nov:  0.8330 | d_rand:  0.6917
INFO:src.model.gail:iter: 569000 | update: 569 | d_loss: 0.39 | pi_loss: -0.18 | d_exp:  0.2056 | d_nov:  0.8908 | d_rand:  0.6916
INFO:src.model.gail:iter: 570000 | update: 570 | d_loss: 0.49 | pi_loss: -0.07 | d_exp:  0.2053 | d_nov:  0.8328 | d_rand:  0.6915
INFO:src.model.gail:iter: 571000 | update: 571 | d_loss: 0.57 | pi_loss:  0.23 | d_exp:  0.2052 | d_nov:  0.7619 | d_rand:  0.6915
INFO:src.model.gail:iter: 572000 | update: 572 | d_loss: 0.54 | pi_loss: -0.07 | d_exp:  0.2051 | d_nov:  0.7822 | d_rand:  0.6914
INFO:src.model.gail:iter: 573000 | update: 573 | d_loss: 0.54 | pi_loss: -0.02 | d_exp:  0.2050 | d_nov:  0.7689 | d_rand:  0.6914
INFO:src.model.gail:iter: 574000 | update: 574 | d_loss: 0.50 | pi_loss: -0.09 | d_exp:  0.2050 | d_nov:  0.8055 | d_rand:  0.6914
INFO:src.model.gail:iter: 575000 | update: 575 | d_loss: 0.72 | pi_loss:  0.32 | d_exp:  0.2051 | d_nov:  0.6477 | d_rand:  0.6914
INFO:src.model.gail:iter: 576000 | update: 576 | d_loss: 0.50 | pi_loss: -0.03 | d_exp:  0.2052 | d_nov:  0.8042 | d_rand:  0.6914
INFO:src.model.gail:iter: 577000 | update: 577 | d_loss: 0.60 | pi_loss: -0.03 | d_exp:  0.2053 | d_nov:  0.7507 | d_rand:  0.6915
INFO:src.model.gail:iter: 578000 | update: 578 | d_loss: 0.46 | pi_loss: -0.02 | d_exp:  0.2054 | d_nov:  0.8309 | d_rand:  0.6915
INFO:src.model.gail:iter: 579000 | update: 579 | d_loss: 0.44 | pi_loss:  0.07 | d_exp:  0.2054 | d_nov:  0.8398 | d_rand:  0.6915
INFO:src.model.gail:iter: 580000 | update: 580 | d_loss: 0.41 | pi_loss:  0.11 | d_exp:  0.2053 | d_nov:  0.8675 | d_rand:  0.6915
INFO:src.model.gail:iter: 581000 | update: 581 | d_loss: 0.45 | pi_loss: -0.16 | d_exp:  0.2052 | d_nov:  0.8363 | d_rand:  0.6914
INFO:src.model.gail:iter: 582000 | update: 582 | d_loss: 0.47 | pi_loss: -0.12 | d_exp:  0.2050 | d_nov:  0.8266 | d_rand:  0.6913
INFO:src.model.gail:iter: 583000 | update: 583 | d_loss: 0.43 | pi_loss: -0.11 | d_exp:  0.2049 | d_nov:  0.8542 | d_rand:  0.6912
INFO:src.model.gail:iter: 584000 | update: 584 | d_loss: 0.46 | pi_loss: -0.11 | d_exp:  0.2047 | d_nov:  0.8286 | d_rand:  0.6911
INFO:src.model.gail:iter: 585000 | update: 585 | d_loss: 0.49 | pi_loss:  0.16 | d_exp:  0.2045 | d_nov:  0.8115 | d_rand:  0.6911
INFO:src.model.gail:iter: 586000 | update: 586 | d_loss: 0.61 | pi_loss:  0.28 | d_exp:  0.2044 | d_nov:  0.7156 | d_rand:  0.6910
INFO:src.model.gail:iter: 587000 | update: 587 | d_loss: 0.62 | pi_loss:  0.17 | d_exp:  0.2045 | d_nov:  0.7129 | d_rand:  0.6910
INFO:src.model.gail:iter: 588000 | update: 588 | d_loss: 0.53 | pi_loss:  0.04 | d_exp:  0.2045 | d_nov:  0.7903 | d_rand:  0.6910
INFO:src.model.gail:iter: 589000 | update: 589 | d_loss: 0.56 | pi_loss:  0.00 | d_exp:  0.2046 | d_nov:  0.7860 | d_rand:  0.6910
INFO:src.model.gail:iter: 590000 | update: 590 | d_loss: 0.55 | pi_loss:  0.06 | d_exp:  0.2046 | d_nov:  0.7595 | d_rand:  0.6911
INFO:src.model.gail:iter: 591000 | update: 591 | d_loss: 0.55 | pi_loss:  0.30 | d_exp:  0.2047 | d_nov:  0.7837 | d_rand:  0.6911
INFO:src.model.gail:iter: 592000 | update: 592 | d_loss: 0.65 | pi_loss:  0.09 | d_exp:  0.2048 | d_nov:  0.6963 | d_rand:  0.6911
INFO:src.model.gail:iter: 593000 | update: 593 | d_loss: 0.62 | pi_loss: -0.07 | d_exp:  0.2050 | d_nov:  0.7313 | d_rand:  0.6912
INFO:src.model.gail:iter: 594000 | update: 594 | d_loss: 0.73 | pi_loss:  0.34 | d_exp:  0.2053 | d_nov:  0.6344 | d_rand:  0.6913
INFO:src.model.gail:iter: 595000 | update: 595 | d_loss: 0.67 | pi_loss:  0.19 | d_exp:  0.2057 | d_nov:  0.6764 | d_rand:  0.6915
INFO:src.model.gail:iter: 596000 | update: 596 | d_loss: 0.58 | pi_loss: -0.19 | d_exp:  0.2060 | d_nov:  0.7384 | d_rand:  0.6916
INFO:src.model.gail:iter: 597000 | update: 597 | d_loss: 0.73 | pi_loss:  0.33 | d_exp:  0.2065 | d_nov:  0.6389 | d_rand:  0.6917
INFO:src.model.gail:iter: 598000 | update: 598 | d_loss: 0.63 | pi_loss: -0.19 | d_exp:  0.2069 | d_nov:  0.7510 | d_rand:  0.6919
INFO:src.model.gail:iter: 599000 | update: 599 | d_loss: 0.59 | pi_loss: -0.09 | d_exp:  0.2073 | d_nov:  0.7434 | d_rand:  0.6920
INFO:src.model.gail:iter: 600000 | update: 600 | d_loss: 0.52 | pi_loss: -0.17 | d_exp:  0.2076 | d_nov:  0.8007 | d_rand:  0.6921
INFO:src.model.gail:iter: 601000 | update: 601 | d_loss: 0.52 | pi_loss:  0.02 | d_exp:  0.2079 | d_nov:  0.7958 | d_rand:  0.6922
INFO:src.model.gail:iter: 602000 | update: 602 | d_loss: 0.51 | pi_loss: -0.22 | d_exp:  0.2081 | d_nov:  0.8037 | d_rand:  0.6923
INFO:src.model.gail:iter: 603000 | update: 603 | d_loss: 0.65 | pi_loss: -0.09 | d_exp:  0.2083 | d_nov:  0.6865 | d_rand:  0.6924
INFO:src.model.gail:iter: 604000 | update: 604 | d_loss: 0.55 | pi_loss:  0.00 | d_exp:  0.2085 | d_nov:  0.7655 | d_rand:  0.6924
INFO:src.model.gail:iter: 605000 | update: 605 | d_loss: 0.54 | pi_loss: -0.00 | d_exp:  0.2087 | d_nov:  0.7756 | d_rand:  0.6925
INFO:src.model.gail:iter: 606000 | update: 606 | d_loss: 0.51 | pi_loss: -0.05 | d_exp:  0.2089 | d_nov:  0.7904 | d_rand:  0.6926
INFO:src.model.gail:iter: 607000 | update: 607 | d_loss: 0.57 | pi_loss:  0.13 | d_exp:  0.2091 | d_nov:  0.7485 | d_rand:  0.6926
INFO:src.model.gail:iter: 608000 | update: 608 | d_loss: 0.58 | pi_loss:  0.11 | d_exp:  0.2093 | d_nov:  0.7425 | d_rand:  0.6927
INFO:src.model.gail:iter: 609000 | update: 609 | d_loss: 0.63 | pi_loss:  0.04 | d_exp:  0.2094 | d_nov:  0.7433 | d_rand:  0.6927
INFO:src.model.gail:iter: 610000 | update: 610 | d_loss: 0.61 | pi_loss:  0.08 | d_exp:  0.2097 | d_nov:  0.7274 | d_rand:  0.6928
INFO:src.model.gail:iter: 611000 | update: 611 | d_loss: 0.50 | pi_loss: -0.02 | d_exp:  0.2098 | d_nov:  0.8062 | d_rand:  0.6929
INFO:src.model.gail:iter: 612000 | update: 612 | d_loss: 0.55 | pi_loss: -0.04 | d_exp:  0.2100 | d_nov:  0.7732 | d_rand:  0.6929
INFO:src.model.gail:iter: 613000 | update: 613 | d_loss: 0.56 | pi_loss: -0.03 | d_exp:  0.2101 | d_nov:  0.7940 | d_rand:  0.6929
INFO:src.model.gail:iter: 614000 | update: 614 | d_loss: 0.62 | pi_loss:  0.19 | d_exp:  0.2102 | d_nov:  0.7160 | d_rand:  0.6930
INFO:src.model.gail:iter: 615000 | update: 615 | d_loss: 0.66 | pi_loss: -0.03 | d_exp:  0.2104 | d_nov:  0.6997 | d_rand:  0.6931
INFO:src.model.gail:iter: 616000 | update: 616 | d_loss: 0.58 | pi_loss: -0.00 | d_exp:  0.2106 | d_nov:  0.7492 | d_rand:  0.6931
INFO:src.model.gail:iter: 617000 | update: 617 | d_loss: 0.49 | pi_loss:  0.08 | d_exp:  0.2108 | d_nov:  0.8188 | d_rand:  0.6932
INFO:src.model.gail:iter: 618000 | update: 618 | d_loss: 0.60 | pi_loss:  0.21 | d_exp:  0.2109 | d_nov:  0.7327 | d_rand:  0.6932
INFO:src.model.gail:iter: 619000 | update: 619 | d_loss: 0.63 | pi_loss:  0.12 | d_exp:  0.2111 | d_nov:  0.7317 | d_rand:  0.6933
INFO:src.model.gail:iter: 620000 | update: 620 | d_loss: 0.57 | pi_loss: -0.07 | d_exp:  0.2113 | d_nov:  0.7518 | d_rand:  0.6933
INFO:src.model.gail:iter: 621000 | update: 621 | d_loss: 0.53 | pi_loss: -0.02 | d_exp:  0.2114 | d_nov:  0.7859 | d_rand:  0.6934
INFO:src.model.gail:iter: 622000 | update: 622 | d_loss: 0.57 | pi_loss:  0.14 | d_exp:  0.2116 | d_nov:  0.7516 | d_rand:  0.6934
INFO:src.model.gail:iter: 623000 | update: 623 | d_loss: 0.60 | pi_loss:  0.11 | d_exp:  0.2117 | d_nov:  0.7274 | d_rand:  0.6935
INFO:src.model.gail:iter: 624000 | update: 624 | d_loss: 0.80 | pi_loss:  0.01 | d_exp:  0.2120 | d_nov:  0.6063 | d_rand:  0.6936
INFO:src.model.gail:iter: 625000 | update: 625 | d_loss: 0.71 | pi_loss: -0.03 | d_exp:  0.2123 | d_nov:  0.6700 | d_rand:  0.6937
INFO:src.model.gail:iter: 626000 | update: 626 | d_loss: 0.68 | pi_loss:  0.08 | d_exp:  0.2127 | d_nov:  0.6682 | d_rand:  0.6938
INFO:src.model.gail:iter: 627000 | update: 627 | d_loss: 0.48 | pi_loss:  0.02 | d_exp:  0.2130 | d_nov:  0.8215 | d_rand:  0.6939
INFO:src.model.gail:iter: 628000 | update: 628 | d_loss: 0.63 | pi_loss:  0.00 | d_exp:  0.2133 | d_nov:  0.7063 | d_rand:  0.6939
INFO:src.model.gail:iter: 629000 | update: 629 | d_loss: 0.57 | pi_loss:  0.11 | d_exp:  0.2135 | d_nov:  0.7543 | d_rand:  0.6940
INFO:src.model.gail:iter: 630000 | update: 630 | d_loss: 0.56 | pi_loss:  0.04 | d_exp:  0.2137 | d_nov:  0.7663 | d_rand:  0.6941
INFO:src.model.gail:iter: 631000 | update: 631 | d_loss: 0.71 | pi_loss:  0.43 | d_exp:  0.2140 | d_nov:  0.6648 | d_rand:  0.6942
INFO:src.model.gail:iter: 632000 | update: 632 | d_loss: 0.61 | pi_loss: -0.00 | d_exp:  0.2142 | d_nov:  0.7360 | d_rand:  0.6942
INFO:src.model.gail:iter: 633000 | update: 633 | d_loss: 0.62 | pi_loss:  0.33 | d_exp:  0.2145 | d_nov:  0.7201 | d_rand:  0.6943
INFO:src.model.gail:iter: 634000 | update: 634 | d_loss: 0.56 | pi_loss: -0.10 | d_exp:  0.2147 | d_nov:  0.7693 | d_rand:  0.6944
INFO:src.model.gail:iter: 635000 | update: 635 | d_loss: 0.47 | pi_loss: -0.25 | d_exp:  0.2149 | d_nov:  0.8381 | d_rand:  0.6944
INFO:src.model.gail:iter: 636000 | update: 636 | d_loss: 0.59 | pi_loss: -0.09 | d_exp:  0.2150 | d_nov:  0.7467 | d_rand:  0.6945
INFO:src.model.gail:iter: 637000 | update: 637 | d_loss: 0.85 | pi_loss:  0.61 | d_exp:  0.2152 | d_nov:  0.5896 | d_rand:  0.6945
INFO:src.model.gail:iter: 638000 | update: 638 | d_loss: 0.68 | pi_loss:  0.21 | d_exp:  0.2155 | d_nov:  0.6797 | d_rand:  0.6946
INFO:src.model.gail:iter: 639000 | update: 639 | d_loss: 0.61 | pi_loss: -0.01 | d_exp:  0.2158 | d_nov:  0.7206 | d_rand:  0.6947
INFO:src.model.gail:iter: 640000 | update: 640 | d_loss: 0.69 | pi_loss:  0.10 | d_exp:  0.2160 | d_nov:  0.7043 | d_rand:  0.6948
INFO:src.model.gail:iter: 641000 | update: 641 | d_loss: 0.73 | pi_loss:  0.22 | d_exp:  0.2163 | d_nov:  0.6507 | d_rand:  0.6949
INFO:src.model.gail:iter: 642000 | update: 642 | d_loss: 0.63 | pi_loss:  0.22 | d_exp:  0.2166 | d_nov:  0.7216 | d_rand:  0.6950
INFO:src.model.gail:iter: 643000 | update: 643 | d_loss: 0.69 | pi_loss:  0.18 | d_exp:  0.2169 | d_nov:  0.6722 | d_rand:  0.6952
INFO:src.model.gail:iter: 644000 | update: 644 | d_loss: 0.62 | pi_loss:  0.04 | d_exp:  0.2172 | d_nov:  0.7208 | d_rand:  0.6953
INFO:src.model.gail:iter: 645000 | update: 645 | d_loss: 0.53 | pi_loss: -0.11 | d_exp:  0.2174 | d_nov:  0.8028 | d_rand:  0.6953
INFO:src.model.gail:iter: 646000 | update: 646 | d_loss: 0.65 | pi_loss:  0.06 | d_exp:  0.2177 | d_nov:  0.6997 | d_rand:  0.6954
INFO:src.model.gail:iter: 647000 | update: 647 | d_loss: 0.53 | pi_loss: -0.22 | d_exp:  0.2178 | d_nov:  0.7904 | d_rand:  0.6955
INFO:src.model.gail:iter: 648000 | update: 648 | d_loss: 0.62 | pi_loss:  0.23 | d_exp:  0.2180 | d_nov:  0.7180 | d_rand:  0.6955
INFO:src.model.gail:iter: 649000 | update: 649 | d_loss: 0.58 | pi_loss: -0.08 | d_exp:  0.2182 | d_nov:  0.7689 | d_rand:  0.6956
INFO:src.model.gail:iter: 650000 | update: 650 | d_loss: 0.80 | pi_loss:  0.45 | d_exp:  0.2184 | d_nov:  0.6264 | d_rand:  0.6956
INFO:src.model.gail:iter: 651000 | update: 651 | d_loss: 0.88 | pi_loss: -0.09 | d_exp:  0.2187 | d_nov:  0.5684 | d_rand:  0.6957
INFO:src.model.gail:iter: 652000 | update: 652 | d_loss: 0.76 | pi_loss: -0.14 | d_exp:  0.2190 | d_nov:  0.6441 | d_rand:  0.6959
INFO:src.model.gail:iter: 653000 | update: 653 | d_loss: 0.50 | pi_loss:  0.06 | d_exp:  0.2193 | d_nov:  0.8142 | d_rand:  0.6960
INFO:src.model.gail:iter: 654000 | update: 654 | d_loss: 0.65 | pi_loss: -0.02 | d_exp:  0.2195 | d_nov:  0.7260 | d_rand:  0.6960
INFO:src.model.gail:iter: 655000 | update: 655 | d_loss: 0.64 | pi_loss:  0.03 | d_exp:  0.2198 | d_nov:  0.7092 | d_rand:  0.6961
INFO:src.model.gail:iter: 656000 | update: 656 | d_loss: 0.68 | pi_loss: -0.08 | d_exp:  0.2200 | d_nov:  0.6868 | d_rand:  0.6962
INFO:src.model.gail:iter: 657000 | update: 657 | d_loss: 0.61 | pi_loss:  0.17 | d_exp:  0.2203 | d_nov:  0.7287 | d_rand:  0.6963
INFO:src.model.gail:iter: 658000 | update: 658 | d_loss: 0.60 | pi_loss: -0.00 | d_exp:  0.2205 | d_nov:  0.7437 | d_rand:  0.6964
INFO:src.model.gail:iter: 659000 | update: 659 | d_loss: 0.67 | pi_loss: -0.07 | d_exp:  0.2207 | d_nov:  0.7101 | d_rand:  0.6964
INFO:src.model.gail:iter: 660000 | update: 660 | d_loss: 0.79 | pi_loss: -0.06 | d_exp:  0.2209 | d_nov:  0.6098 | d_rand:  0.6965
INFO:src.model.gail:iter: 661000 | update: 661 | d_loss: 0.56 | pi_loss: -0.34 | d_exp:  0.2211 | d_nov:  0.7860 | d_rand:  0.6966
INFO:src.model.gail:iter: 662000 | update: 662 | d_loss: 0.63 | pi_loss: -0.00 | d_exp:  0.2213 | d_nov:  0.7372 | d_rand:  0.6967
INFO:src.model.gail:iter: 663000 | update: 663 | d_loss: 0.76 | pi_loss:  0.06 | d_exp:  0.2215 | d_nov:  0.6290 | d_rand:  0.6967
INFO:src.model.gail:iter: 664000 | update: 664 | d_loss: 0.56 | pi_loss: -0.08 | d_exp:  0.2217 | d_nov:  0.7663 | d_rand:  0.6968
INFO:src.model.gail:iter: 665000 | update: 665 | d_loss: 0.73 | pi_loss:  0.08 | d_exp:  0.2220 | d_nov:  0.6500 | d_rand:  0.6969
INFO:src.model.gail:iter: 666000 | update: 666 | d_loss: 0.51 | pi_loss: -0.09 | d_exp:  0.2221 | d_nov:  0.8311 | d_rand:  0.6969
INFO:src.model.gail:iter: 667000 | update: 667 | d_loss: 0.64 | pi_loss:  0.17 | d_exp:  0.2223 | d_nov:  0.7167 | d_rand:  0.6970
INFO:src.model.gail:iter: 668000 | update: 668 | d_loss: 0.66 | pi_loss:  0.20 | d_exp:  0.2224 | d_nov:  0.7124 | d_rand:  0.6970
INFO:src.model.gail:iter: 669000 | update: 669 | d_loss: 0.70 | pi_loss:  0.20 | d_exp:  0.2226 | d_nov:  0.6839 | d_rand:  0.6971
INFO:src.model.gail:iter: 670000 | update: 670 | d_loss: 0.50 | pi_loss:  0.13 | d_exp:  0.2227 | d_nov:  0.8165 | d_rand:  0.6971
INFO:src.model.gail:iter: 671000 | update: 671 | d_loss: 0.48 | pi_loss: -0.18 | d_exp:  0.2228 | d_nov:  0.8455 | d_rand:  0.6971
INFO:src.model.gail:iter: 672000 | update: 672 | d_loss: 0.55 | pi_loss:  0.07 | d_exp:  0.2228 | d_nov:  0.7782 | d_rand:  0.6971
INFO:src.model.gail:iter: 673000 | update: 673 | d_loss: 0.65 | pi_loss:  0.12 | d_exp:  0.2229 | d_nov:  0.7020 | d_rand:  0.6971
INFO:src.model.gail:iter: 674000 | update: 674 | d_loss: 0.63 | pi_loss:  0.07 | d_exp:  0.2230 | d_nov:  0.7219 | d_rand:  0.6971
INFO:src.model.gail:iter: 675000 | update: 675 | d_loss: 0.69 | pi_loss:  0.03 | d_exp:  0.2231 | d_nov:  0.6941 | d_rand:  0.6972
INFO:src.model.gail:iter: 676000 | update: 676 | d_loss: 0.72 | pi_loss:  0.25 | d_exp:  0.2232 | d_nov:  0.6543 | d_rand:  0.6972
INFO:src.model.gail:iter: 677000 | update: 677 | d_loss: 0.49 | pi_loss: -0.03 | d_exp:  0.2233 | d_nov:  0.8269 | d_rand:  0.6972
INFO:src.model.gail:iter: 678000 | update: 678 | d_loss: 0.49 | pi_loss:  0.05 | d_exp:  0.2234 | d_nov:  0.8313 | d_rand:  0.6973
INFO:src.model.gail:iter: 679000 | update: 679 | d_loss: 0.54 | pi_loss:  0.06 | d_exp:  0.2234 | d_nov:  0.8039 | d_rand:  0.6973
INFO:src.model.gail:iter: 680000 | update: 680 | d_loss: 0.66 | pi_loss: -0.06 | d_exp:  0.2234 | d_nov:  0.7323 | d_rand:  0.6973
INFO:src.model.gail:iter: 681000 | update: 681 | d_loss: 0.81 | pi_loss:  0.13 | d_exp:  0.2235 | d_nov:  0.6070 | d_rand:  0.6973
INFO:src.model.gail:iter: 682000 | update: 682 | d_loss: 0.60 | pi_loss: -0.25 | d_exp:  0.2237 | d_nov:  0.7595 | d_rand:  0.6973
INFO:src.model.gail:iter: 683000 | update: 683 | d_loss: 0.53 | pi_loss: -0.12 | d_exp:  0.2237 | d_nov:  0.8022 | d_rand:  0.6974
INFO:src.model.gail:iter: 684000 | update: 684 | d_loss: 0.66 | pi_loss:  0.09 | d_exp:  0.2238 | d_nov:  0.7162 | d_rand:  0.6974
INFO:src.model.gail:iter: 685000 | update: 685 | d_loss: 0.67 | pi_loss:  0.07 | d_exp:  0.2239 | d_nov:  0.6907 | d_rand:  0.6974
INFO:src.model.gail:iter: 686000 | update: 686 | d_loss: 0.64 | pi_loss:  0.05 | d_exp:  0.2240 | d_nov:  0.7320 | d_rand:  0.6975
INFO:src.model.gail:iter: 687000 | update: 687 | d_loss: 0.86 | pi_loss:  0.26 | d_exp:  0.2242 | d_nov:  0.5905 | d_rand:  0.6975
INFO:src.model.gail:iter: 688000 | update: 688 | d_loss: 0.51 | pi_loss: -0.15 | d_exp:  0.2243 | d_nov:  0.8100 | d_rand:  0.6976
INFO:src.model.gail:iter: 689000 | update: 689 | d_loss: 0.53 | pi_loss: -0.21 | d_exp:  0.2244 | d_nov:  0.8070 | d_rand:  0.6976
INFO:src.model.gail:iter: 690000 | update: 690 | d_loss: 0.61 | pi_loss:  0.02 | d_exp:  0.2245 | d_nov:  0.7522 | d_rand:  0.6976
INFO:src.model.gail:iter: 691000 | update: 691 | d_loss: 0.80 | pi_loss:  0.16 | d_exp:  0.2246 | d_nov:  0.6070 | d_rand:  0.6977
INFO:src.model.gail:iter: 692000 | update: 692 | d_loss: 0.58 | pi_loss:  0.24 | d_exp:  0.2247 | d_nov:  0.7694 | d_rand:  0.6977
INFO:src.model.gail:iter: 693000 | update: 693 | d_loss: 0.70 | pi_loss:  0.36 | d_exp:  0.2249 | d_nov:  0.6888 | d_rand:  0.6978
INFO:src.model.gail:iter: 694000 | update: 694 | d_loss: 0.51 | pi_loss: -0.05 | d_exp:  0.2250 | d_nov:  0.8142 | d_rand:  0.6978
INFO:src.model.gail:iter: 695000 | update: 695 | d_loss: 0.65 | pi_loss: -0.22 | d_exp:  0.2251 | d_nov:  0.7298 | d_rand:  0.6978
INFO:src.model.gail:iter: 696000 | update: 696 | d_loss: 0.72 | pi_loss: -0.07 | d_exp:  0.2252 | d_nov:  0.6571 | d_rand:  0.6979
INFO:src.model.gail:iter: 697000 | update: 697 | d_loss: 0.59 | pi_loss:  0.11 | d_exp:  0.2253 | d_nov:  0.7500 | d_rand:  0.6979
INFO:src.model.gail:iter: 698000 | update: 698 | d_loss: 0.66 | pi_loss:  0.04 | d_exp:  0.2254 | d_nov:  0.7021 | d_rand:  0.6979
INFO:src.model.gail:iter: 699000 | update: 699 | d_loss: 0.64 | pi_loss: -0.02 | d_exp:  0.2255 | d_nov:  0.7196 | d_rand:  0.6980
INFO:src.model.gail:iter: 700000 | update: 700 | d_loss: 0.63 | pi_loss:  0.13 | d_exp:  0.2256 | d_nov:  0.7320 | d_rand:  0.6980
INFO:src.model.gail:iter: 701000 | update: 701 | d_loss: 0.49 | pi_loss:  0.06 | d_exp:  0.2257 | d_nov:  0.8358 | d_rand:  0.6980
INFO:src.model.gail:iter: 702000 | update: 702 | d_loss: 0.63 | pi_loss:  0.22 | d_exp:  0.2258 | d_nov:  0.7560 | d_rand:  0.6980
INFO:src.model.gail:iter: 703000 | update: 703 | d_loss: 0.64 | pi_loss:  0.01 | d_exp:  0.2259 | d_nov:  0.7114 | d_rand:  0.6981
INFO:src.model.gail:iter: 704000 | update: 704 | d_loss: 0.60 | pi_loss:  0.04 | d_exp:  0.2259 | d_nov:  0.7546 | d_rand:  0.6981
INFO:src.model.gail:iter: 705000 | update: 705 | d_loss: 0.67 | pi_loss: -0.03 | d_exp:  0.2260 | d_nov:  0.7068 | d_rand:  0.6981
INFO:src.model.gail:iter: 706000 | update: 706 | d_loss: 0.63 | pi_loss:  0.12 | d_exp:  0.2261 | d_nov:  0.7268 | d_rand:  0.6981
INFO:src.model.gail:iter: 707000 | update: 707 | d_loss: 0.71 | pi_loss: -0.14 | d_exp:  0.2262 | d_nov:  0.6768 | d_rand:  0.6982
INFO:src.model.gail:iter: 708000 | update: 708 | d_loss: 0.64 | pi_loss:  0.14 | d_exp:  0.2263 | d_nov:  0.7214 | d_rand:  0.6982
INFO:src.model.gail:iter: 709000 | update: 709 | d_loss: 0.63 | pi_loss:  0.05 | d_exp:  0.2264 | d_nov:  0.7238 | d_rand:  0.6982
INFO:src.model.gail:iter: 710000 | update: 710 | d_loss: 0.54 | pi_loss: -0.10 | d_exp:  0.2265 | d_nov:  0.7914 | d_rand:  0.6982
INFO:src.model.gail:iter: 711000 | update: 711 | d_loss: 0.67 | pi_loss:  0.10 | d_exp:  0.2265 | d_nov:  0.6926 | d_rand:  0.6983
INFO:src.model.gail:iter: 712000 | update: 712 | d_loss: 0.62 | pi_loss:  0.33 | d_exp:  0.2266 | d_nov:  0.7283 | d_rand:  0.6983
INFO:src.model.gail:iter: 713000 | update: 713 | d_loss: 0.54 | pi_loss: -0.21 | d_exp:  0.2267 | d_nov:  0.7976 | d_rand:  0.6983
INFO:src.model.gail:iter: 714000 | update: 714 | d_loss: 0.73 | pi_loss:  0.29 | d_exp:  0.2268 | d_nov:  0.6572 | d_rand:  0.6983
INFO:src.model.gail:iter: 715000 | update: 715 | d_loss: 0.70 | pi_loss:  0.36 | d_exp:  0.2269 | d_nov:  0.6758 | d_rand:  0.6984
INFO:src.model.gail:iter: 716000 | update: 716 | d_loss: 0.66 | pi_loss:  0.05 | d_exp:  0.2270 | d_nov:  0.7004 | d_rand:  0.6984
INFO:src.model.gail:iter: 717000 | update: 717 | d_loss: 0.60 | pi_loss: -0.01 | d_exp:  0.2270 | d_nov:  0.7448 | d_rand:  0.6984
INFO:src.model.gail:iter: 718000 | update: 718 | d_loss: 0.63 | pi_loss: -0.05 | d_exp:  0.2271 | d_nov:  0.7415 | d_rand:  0.6985
INFO:src.model.gail:iter: 719000 | update: 719 | d_loss: 0.59 | pi_loss:  0.18 | d_exp:  0.2272 | d_nov:  0.7465 | d_rand:  0.6985
INFO:src.model.gail:iter: 720000 | update: 720 | d_loss: 0.62 | pi_loss: -0.08 | d_exp:  0.2273 | d_nov:  0.7356 | d_rand:  0.6985
INFO:src.model.gail:iter: 721000 | update: 721 | d_loss: 0.60 | pi_loss:  0.09 | d_exp:  0.2274 | d_nov:  0.7477 | d_rand:  0.6985
INFO:src.model.gail:iter: 722000 | update: 722 | d_loss: 0.65 | pi_loss: -0.15 | d_exp:  0.2274 | d_nov:  0.7261 | d_rand:  0.6985
INFO:src.model.gail:iter: 723000 | update: 723 | d_loss: 0.67 | pi_loss:  0.17 | d_exp:  0.2275 | d_nov:  0.7061 | d_rand:  0.6986
INFO:src.model.gail:iter: 724000 | update: 724 | d_loss: 0.64 | pi_loss:  0.16 | d_exp:  0.2276 | d_nov:  0.7236 | d_rand:  0.6986
INFO:src.model.gail:iter: 725000 | update: 725 | d_loss: 0.58 | pi_loss:  0.03 | d_exp:  0.2276 | d_nov:  0.7633 | d_rand:  0.6986
INFO:src.model.gail:iter: 726000 | update: 726 | d_loss: 0.60 | pi_loss: -0.19 | d_exp:  0.2277 | d_nov:  0.7516 | d_rand:  0.6986
INFO:src.model.gail:iter: 727000 | update: 727 | d_loss: 0.77 | pi_loss: -0.03 | d_exp:  0.2278 | d_nov:  0.6403 | d_rand:  0.6987
INFO:src.model.gail:iter: 728000 | update: 728 | d_loss: 0.68 | pi_loss: -0.16 | d_exp:  0.2279 | d_nov:  0.6898 | d_rand:  0.6987
INFO:src.model.gail:iter: 729000 | update: 729 | d_loss: 0.69 | pi_loss:  0.07 | d_exp:  0.2280 | d_nov:  0.6791 | d_rand:  0.6987
INFO:src.model.gail:iter: 730000 | update: 730 | d_loss: 0.60 | pi_loss: -0.15 | d_exp:  0.2281 | d_nov:  0.7654 | d_rand:  0.6987
INFO:src.model.gail:iter: 731000 | update: 731 | d_loss: 0.69 | pi_loss:  0.15 | d_exp:  0.2282 | d_nov:  0.6837 | d_rand:  0.6988
INFO:src.model.gail:iter: 732000 | update: 732 | d_loss: 0.63 | pi_loss:  0.00 | d_exp:  0.2283 | d_nov:  0.7449 | d_rand:  0.6988
INFO:src.model.gail:iter: 733000 | update: 733 | d_loss: 0.71 | pi_loss: -0.08 | d_exp:  0.2283 | d_nov:  0.6685 | d_rand:  0.6988
INFO:src.model.gail:iter: 734000 | update: 734 | d_loss: 0.85 | pi_loss:  0.24 | d_exp:  0.2285 | d_nov:  0.5823 | d_rand:  0.6989
INFO:src.model.gail:iter: 735000 | update: 735 | d_loss: 0.57 | pi_loss:  0.25 | d_exp:  0.2286 | d_nov:  0.7705 | d_rand:  0.6989
INFO:src.model.gail:iter: 736000 | update: 736 | d_loss: 0.70 | pi_loss:  0.41 | d_exp:  0.2287 | d_nov:  0.6931 | d_rand:  0.6989
INFO:src.model.gail:iter: 737000 | update: 737 | d_loss: 0.67 | pi_loss: -0.07 | d_exp:  0.2288 | d_nov:  0.7023 | d_rand:  0.6990
INFO:src.model.gail:iter: 738000 | update: 738 | d_loss: 0.53 | pi_loss: -0.28 | d_exp:  0.2289 | d_nov:  0.8054 | d_rand:  0.6990
INFO:src.model.gail:iter: 739000 | update: 739 | d_loss: 0.52 | pi_loss: -0.31 | d_exp:  0.2289 | d_nov:  0.8202 | d_rand:  0.6990
INFO:src.model.gail:iter: 740000 | update: 740 | d_loss: 0.56 | pi_loss: -0.14 | d_exp:  0.2290 | d_nov:  0.7718 | d_rand:  0.6990
INFO:src.model.gail:iter: 741000 | update: 741 | d_loss: 0.70 | pi_loss: -0.17 | d_exp:  0.2290 | d_nov:  0.6883 | d_rand:  0.6990
INFO:src.model.gail:iter: 742000 | update: 742 | d_loss: 0.70 | pi_loss: -0.19 | d_exp:  0.2291 | d_nov:  0.6864 | d_rand:  0.6991
INFO:src.model.gail:iter: 743000 | update: 743 | d_loss: 0.57 | pi_loss: -0.15 | d_exp:  0.2291 | d_nov:  0.7718 | d_rand:  0.6991
INFO:src.model.gail:iter: 744000 | update: 744 | d_loss: 0.68 | pi_loss:  0.01 | d_exp:  0.2292 | d_nov:  0.6891 | d_rand:  0.6991
INFO:src.model.gail:iter: 745000 | update: 745 | d_loss: 0.69 | pi_loss:  0.31 | d_exp:  0.2293 | d_nov:  0.6995 | d_rand:  0.6991
INFO:src.model.gail:iter: 746000 | update: 746 | d_loss: 0.65 | pi_loss: -0.15 | d_exp:  0.2293 | d_nov:  0.7387 | d_rand:  0.6991
INFO:src.model.gail:iter: 747000 | update: 747 | d_loss: 0.83 | pi_loss:  0.02 | d_exp:  0.2294 | d_nov:  0.6037 | d_rand:  0.6992
INFO:src.model.gail:iter: 748000 | update: 748 | d_loss: 0.67 | pi_loss:  0.49 | d_exp:  0.2295 | d_nov:  0.7031 | d_rand:  0.6992
INFO:src.model.gail:iter: 749000 | update: 749 | d_loss: 0.74 | pi_loss:  0.04 | d_exp:  0.2296 | d_nov:  0.6481 | d_rand:  0.6992
INFO:src.model.gail:iter: 750000 | update: 750 | d_loss: 0.53 | pi_loss: -0.00 | d_exp:  0.2297 | d_nov:  0.8096 | d_rand:  0.6993
INFO:src.model.gail:iter: 751000 | update: 751 | d_loss: 0.69 | pi_loss:  0.00 | d_exp:  0.2298 | d_nov:  0.7142 | d_rand:  0.6993
INFO:src.model.gail:iter: 752000 | update: 752 | d_loss: 0.68 | pi_loss: -0.16 | d_exp:  0.2299 | d_nov:  0.6905 | d_rand:  0.6993
INFO:src.model.gail:iter: 753000 | update: 753 | d_loss: 0.54 | pi_loss: -0.23 | d_exp:  0.2300 | d_nov:  0.8032 | d_rand:  0.6993
INFO:src.model.gail:iter: 754000 | update: 754 | d_loss: 0.67 | pi_loss: -0.10 | d_exp:  0.2300 | d_nov:  0.7020 | d_rand:  0.6993
INFO:src.model.gail:iter: 755000 | update: 755 | d_loss: 0.73 | pi_loss:  0.06 | d_exp:  0.2301 | d_nov:  0.6620 | d_rand:  0.6994
INFO:src.model.gail:iter: 756000 | update: 756 | d_loss: 0.66 | pi_loss: -0.04 | d_exp:  0.2302 | d_nov:  0.7135 | d_rand:  0.6994
INFO:src.model.gail:iter: 757000 | update: 757 | d_loss: 0.63 | pi_loss: -0.06 | d_exp:  0.2302 | d_nov:  0.7238 | d_rand:  0.6994
INFO:src.model.gail:iter: 758000 | update: 758 | d_loss: 0.58 | pi_loss: -0.00 | d_exp:  0.2303 | d_nov:  0.7752 | d_rand:  0.6994
INFO:src.model.gail:iter: 759000 | update: 759 | d_loss: 0.52 | pi_loss: -0.04 | d_exp:  0.2303 | d_nov:  0.8139 | d_rand:  0.6994
INFO:src.model.gail:iter: 760000 | update: 760 | d_loss: 0.61 | pi_loss: -0.02 | d_exp:  0.2304 | d_nov:  0.7373 | d_rand:  0.6994
INFO:src.model.gail:iter: 761000 | update: 761 | d_loss: 0.60 | pi_loss:  0.10 | d_exp:  0.2304 | d_nov:  0.7454 | d_rand:  0.6994
INFO:src.model.gail:iter: 762000 | update: 762 | d_loss: 0.83 | pi_loss:  0.20 | d_exp:  0.2305 | d_nov:  0.6738 | d_rand:  0.6995
INFO:src.model.gail:iter: 763000 | update: 763 | d_loss: 0.90 | pi_loss:  0.41 | d_exp:  0.2305 | d_nov:  0.5608 | d_rand:  0.6995
INFO:src.model.gail:iter: 764000 | update: 764 | d_loss: 0.71 | pi_loss: -0.09 | d_exp:  0.2306 | d_nov:  0.7108 | d_rand:  0.6995
INFO:src.model.gail:iter: 765000 | update: 765 | d_loss: 0.70 | pi_loss: -0.01 | d_exp:  0.2307 | d_nov:  0.6833 | d_rand:  0.6995
INFO:src.model.gail:iter: 766000 | update: 766 | d_loss: 0.64 | pi_loss:  0.21 | d_exp:  0.2308 | d_nov:  0.7260 | d_rand:  0.6996
INFO:src.model.gail:iter: 767000 | update: 767 | d_loss: 0.59 | pi_loss:  0.26 | d_exp:  0.2308 | d_nov:  0.7568 | d_rand:  0.6996
INFO:src.model.gail:iter: 768000 | update: 768 | d_loss: 0.66 | pi_loss:  0.26 | d_exp:  0.2309 | d_nov:  0.7088 | d_rand:  0.6996
INFO:src.model.gail:iter: 769000 | update: 769 | d_loss: 0.60 | pi_loss: -0.24 | d_exp:  0.2310 | d_nov:  0.7453 | d_rand:  0.6996
INFO:src.model.gail:iter: 770000 | update: 770 | d_loss: 0.72 | pi_loss:  0.05 | d_exp:  0.2310 | d_nov:  0.6758 | d_rand:  0.6996
INFO:src.model.gail:iter: 771000 | update: 771 | d_loss: 0.55 | pi_loss: -0.13 | d_exp:  0.2311 | d_nov:  0.7957 | d_rand:  0.6996
INFO:src.model.gail:iter: 772000 | update: 772 | d_loss: 0.59 | pi_loss:  0.09 | d_exp:  0.2311 | d_nov:  0.7546 | d_rand:  0.6997
INFO:src.model.gail:iter: 773000 | update: 773 | d_loss: 0.50 | pi_loss: -0.24 | d_exp:  0.2311 | d_nov:  0.8218 | d_rand:  0.6997
INFO:src.model.gail:iter: 774000 | update: 774 | d_loss: 0.53 | pi_loss: -0.19 | d_exp:  0.2311 | d_nov:  0.8035 | d_rand:  0.6997
INFO:src.model.gail:iter: 775000 | update: 775 | d_loss: 0.55 | pi_loss: -0.06 | d_exp:  0.2311 | d_nov:  0.7855 | d_rand:  0.6997
INFO:src.model.gail:iter: 776000 | update: 776 | d_loss: 0.61 | pi_loss: -0.01 | d_exp:  0.2311 | d_nov:  0.7504 | d_rand:  0.6997
INFO:src.model.gail:iter: 777000 | update: 777 | d_loss: 0.60 | pi_loss:  0.01 | d_exp:  0.2311 | d_nov:  0.7510 | d_rand:  0.6996
INFO:src.model.gail:iter: 778000 | update: 778 | d_loss: 0.62 | pi_loss:  0.09 | d_exp:  0.2312 | d_nov:  0.7334 | d_rand:  0.6996
INFO:src.model.gail:iter: 779000 | update: 779 | d_loss: 0.60 | pi_loss: -0.03 | d_exp:  0.2312 | d_nov:  0.7606 | d_rand:  0.6996
INFO:src.model.gail:iter: 780000 | update: 780 | d_loss: 0.70 | pi_loss:  0.10 | d_exp:  0.2312 | d_nov:  0.6713 | d_rand:  0.6996
INFO:src.model.gail:iter: 781000 | update: 781 | d_loss: 0.58 | pi_loss: -0.19 | d_exp:  0.2312 | d_nov:  0.7728 | d_rand:  0.6997
INFO:src.model.gail:iter: 782000 | update: 782 | d_loss: 0.53 | pi_loss:  0.14 | d_exp:  0.2312 | d_nov:  0.8087 | d_rand:  0.6997
INFO:src.model.gail:iter: 783000 | update: 783 | d_loss: 0.64 | pi_loss:  0.03 | d_exp:  0.2312 | d_nov:  0.7194 | d_rand:  0.6997
INFO:src.model.gail:iter: 784000 | update: 784 | d_loss: 0.77 | pi_loss:  0.48 | d_exp:  0.2313 | d_nov:  0.6814 | d_rand:  0.6997
INFO:src.model.gail:iter: 785000 | update: 785 | d_loss: 0.81 | pi_loss:  0.24 | d_exp:  0.2313 | d_nov:  0.6223 | d_rand:  0.6997
INFO:src.model.gail:iter: 786000 | update: 786 | d_loss: 0.54 | pi_loss: -0.02 | d_exp:  0.2313 | d_nov:  0.8070 | d_rand:  0.6997
INFO:src.model.gail:iter: 787000 | update: 787 | d_loss: 0.56 | pi_loss: -0.07 | d_exp:  0.2314 | d_nov:  0.7817 | d_rand:  0.6997
INFO:src.model.gail:iter: 788000 | update: 788 | d_loss: 0.54 | pi_loss:  0.03 | d_exp:  0.2314 | d_nov:  0.7904 | d_rand:  0.6997
INFO:src.model.gail:iter: 789000 | update: 789 | d_loss: 0.59 | pi_loss: -0.24 | d_exp:  0.2314 | d_nov:  0.7697 | d_rand:  0.6997
INFO:src.model.gail:iter: 790000 | update: 790 | d_loss: 0.64 | pi_loss: -0.07 | d_exp:  0.2314 | d_nov:  0.7212 | d_rand:  0.6997
INFO:src.model.gail:iter: 791000 | update: 791 | d_loss: 0.65 | pi_loss: -0.01 | d_exp:  0.2315 | d_nov:  0.7179 | d_rand:  0.6997
INFO:src.model.gail:iter: 792000 | update: 792 | d_loss: 0.69 | pi_loss: -0.08 | d_exp:  0.2315 | d_nov:  0.6897 | d_rand:  0.6997
INFO:src.model.gail:iter: 793000 | update: 793 | d_loss: 0.72 | pi_loss: -0.17 | d_exp:  0.2315 | d_nov:  0.6884 | d_rand:  0.6997
INFO:src.model.gail:iter: 794000 | update: 794 | d_loss: 0.50 | pi_loss: -0.16 | d_exp:  0.2315 | d_nov:  0.8342 | d_rand:  0.6997
INFO:src.model.gail:iter: 795000 | update: 795 | d_loss: 0.69 | pi_loss: -0.16 | d_exp:  0.2316 | d_nov:  0.7183 | d_rand:  0.6997
INFO:src.model.gail:iter: 796000 | update: 796 | d_loss: 0.81 | pi_loss:  0.24 | d_exp:  0.2316 | d_nov:  0.6194 | d_rand:  0.6997
INFO:src.model.gail:iter: 797000 | update: 797 | d_loss: 0.59 | pi_loss:  0.44 | d_exp:  0.2316 | d_nov:  0.7615 | d_rand:  0.6997
INFO:src.model.gail:iter: 798000 | update: 798 | d_loss: 0.60 | pi_loss:  0.15 | d_exp:  0.2317 | d_nov:  0.7480 | d_rand:  0.6998
INFO:src.model.gail:iter: 799000 | update: 799 | d_loss: 0.61 | pi_loss: -0.04 | d_exp:  0.2317 | d_nov:  0.7491 | d_rand:  0.6998
INFO:src.model.gail:iter: 800000 | update: 800 | d_loss: 0.65 | pi_loss:  0.08 | d_exp:  0.2317 | d_nov:  0.7139 | d_rand:  0.6998
INFO:src.model.gail:iter: 801000 | update: 801 | d_loss: 0.65 | pi_loss: -0.00 | d_exp:  0.2318 | d_nov:  0.7181 | d_rand:  0.6998
INFO:src.model.gail:iter: 802000 | update: 802 | d_loss: 0.75 | pi_loss:  0.00 | d_exp:  0.2318 | d_nov:  0.6565 | d_rand:  0.6998
INFO:src.model.gail:iter: 803000 | update: 803 | d_loss: 0.97 | pi_loss:  0.42 | d_exp:  0.2319 | d_nov:  0.5330 | d_rand:  0.6998
INFO:src.model.gail:iter: 804000 | update: 804 | d_loss: 0.65 | pi_loss:  0.27 | d_exp:  0.2319 | d_nov:  0.7244 | d_rand:  0.6998
INFO:src.model.gail:iter: 805000 | update: 805 | d_loss: 0.64 | pi_loss: -0.01 | d_exp:  0.2320 | d_nov:  0.7179 | d_rand:  0.6998
INFO:src.model.gail:iter: 806000 | update: 806 | d_loss: 0.71 | pi_loss: -0.20 | d_exp:  0.2321 | d_nov:  0.6916 | d_rand:  0.6999
INFO:src.model.gail:iter: 807000 | update: 807 | d_loss: 0.64 | pi_loss: -0.01 | d_exp:  0.2321 | d_nov:  0.7209 | d_rand:  0.6999
INFO:src.model.gail:iter: 808000 | update: 808 | d_loss: 0.60 | pi_loss:  0.03 | d_exp:  0.2321 | d_nov:  0.7695 | d_rand:  0.6999
INFO:src.model.gail:iter: 809000 | update: 809 | d_loss: 0.74 | pi_loss:  0.18 | d_exp:  0.2322 | d_nov:  0.6658 | d_rand:  0.6999
INFO:src.model.gail:iter: 810000 | update: 810 | d_loss: 0.78 | pi_loss:  0.30 | d_exp:  0.2323 | d_nov:  0.6324 | d_rand:  0.6999
INFO:src.model.gail:iter: 811000 | update: 811 | d_loss: 0.65 | pi_loss:  0.20 | d_exp:  0.2323 | d_nov:  0.7124 | d_rand:  0.7000
INFO:src.model.gail:iter: 812000 | update: 812 | d_loss: 0.58 | pi_loss: -0.03 | d_exp:  0.2323 | d_nov:  0.7849 | d_rand:  0.7000
INFO:src.model.gail:iter: 813000 | update: 813 | d_loss: 0.73 | pi_loss: -0.08 | d_exp:  0.2324 | d_nov:  0.6906 | d_rand:  0.7000
INFO:src.model.gail:iter: 814000 | update: 814 | d_loss: 0.63 | pi_loss:  0.00 | d_exp:  0.2324 | d_nov:  0.7350 | d_rand:  0.7000
INFO:src.model.gail:iter: 815000 | update: 815 | d_loss: 0.70 | pi_loss:  0.33 | d_exp:  0.2325 | d_nov:  0.6901 | d_rand:  0.7000
INFO:src.model.gail:iter: 816000 | update: 816 | d_loss: 0.79 | pi_loss:  0.18 | d_exp:  0.2325 | d_nov:  0.6331 | d_rand:  0.7000
INFO:src.model.gail:iter: 817000 | update: 817 | d_loss: 0.78 | pi_loss:  0.14 | d_exp:  0.2326 | d_nov:  0.6329 | d_rand:  0.7000
INFO:src.model.gail:iter: 818000 | update: 818 | d_loss: 0.68 | pi_loss: -0.15 | d_exp:  0.2326 | d_nov:  0.7008 | d_rand:  0.7001
INFO:src.model.gail:iter: 819000 | update: 819 | d_loss: 0.61 | pi_loss: -0.19 | d_exp:  0.2327 | d_nov:  0.7553 | d_rand:  0.7001
INFO:src.model.gail:iter: 820000 | update: 820 | d_loss: 0.73 | pi_loss: -0.17 | d_exp:  0.2327 | d_nov:  0.6632 | d_rand:  0.7001
INFO:src.model.gail:iter: 821000 | update: 821 | d_loss: 0.61 | pi_loss: -0.11 | d_exp:  0.2328 | d_nov:  0.7443 | d_rand:  0.7001
INFO:src.model.gail:iter: 822000 | update: 822 | d_loss: 0.56 | pi_loss: -0.11 | d_exp:  0.2328 | d_nov:  0.8032 | d_rand:  0.7001
INFO:src.model.gail:iter: 823000 | update: 823 | d_loss: 0.64 | pi_loss:  0.07 | d_exp:  0.2329 | d_nov:  0.7265 | d_rand:  0.7001
INFO:src.model.gail:iter: 824000 | update: 824 | d_loss: 0.54 | pi_loss: -0.07 | d_exp:  0.2329 | d_nov:  0.7995 | d_rand:  0.7001
INFO:src.model.gail:iter: 825000 | update: 825 | d_loss: 0.69 | pi_loss:  0.02 | d_exp:  0.2329 | d_nov:  0.6864 | d_rand:  0.7001
INFO:src.model.gail:iter: 826000 | update: 826 | d_loss: 0.84 | pi_loss:  0.02 | d_exp:  0.2329 | d_nov:  0.6124 | d_rand:  0.7002
INFO:src.model.gail:iter: 827000 | update: 827 | d_loss: 0.72 | pi_loss:  0.05 | d_exp:  0.2330 | d_nov:  0.6876 | d_rand:  0.7002
INFO:src.model.gail:iter: 828000 | update: 828 | d_loss: 0.65 | pi_loss: -0.07 | d_exp:  0.2330 | d_nov:  0.7178 | d_rand:  0.7002
INFO:src.model.gail:iter: 829000 | update: 829 | d_loss: 0.71 | pi_loss:  0.44 | d_exp:  0.2331 | d_nov:  0.6851 | d_rand:  0.7002
INFO:src.model.gail:iter: 830000 | update: 830 | d_loss: 0.64 | pi_loss:  0.21 | d_exp:  0.2331 | d_nov:  0.7284 | d_rand:  0.7002
INFO:src.model.gail:iter: 831000 | update: 831 | d_loss: 0.56 | pi_loss:  0.02 | d_exp:  0.2331 | d_nov:  0.7738 | d_rand:  0.7002
INFO:src.model.gail:iter: 832000 | update: 832 | d_loss: 0.68 | pi_loss:  0.22 | d_exp:  0.2332 | d_nov:  0.7107 | d_rand:  0.7002
INFO:src.model.gail:iter: 833000 | update: 833 | d_loss: 0.58 | pi_loss:  0.38 | d_exp:  0.2332 | d_nov:  0.7741 | d_rand:  0.7002
INFO:src.model.gail:iter: 834000 | update: 834 | d_loss: 0.61 | pi_loss: -0.02 | d_exp:  0.2332 | d_nov:  0.7453 | d_rand:  0.7002
INFO:src.model.gail:iter: 835000 | update: 835 | d_loss: 0.68 | pi_loss:  0.03 | d_exp:  0.2332 | d_nov:  0.7115 | d_rand:  0.7002
INFO:src.model.gail:iter: 836000 | update: 836 | d_loss: 0.79 | pi_loss: -0.03 | d_exp:  0.2333 | d_nov:  0.6274 | d_rand:  0.7002
INFO:src.model.gail:iter: 837000 | update: 837 | d_loss: 0.62 | pi_loss: -0.05 | d_exp:  0.2333 | d_nov:  0.7529 | d_rand:  0.7003
INFO:src.model.gail:iter: 838000 | update: 838 | d_loss: 0.66 | pi_loss: -0.00 | d_exp:  0.2333 | d_nov:  0.7197 | d_rand:  0.7003
INFO:src.model.gail:iter: 839000 | update: 839 | d_loss: 0.55 | pi_loss:  0.15 | d_exp:  0.2334 | d_nov:  0.7943 | d_rand:  0.7003
INFO:src.model.gail:iter: 840000 | update: 840 | d_loss: 0.64 | pi_loss: -0.13 | d_exp:  0.2334 | d_nov:  0.7263 | d_rand:  0.7003
INFO:src.model.gail:iter: 841000 | update: 841 | d_loss: 0.60 | pi_loss: -0.11 | d_exp:  0.2334 | d_nov:  0.7669 | d_rand:  0.7003
INFO:src.model.gail:iter: 842000 | update: 842 | d_loss: 0.58 | pi_loss: -0.06 | d_exp:  0.2334 | d_nov:  0.7709 | d_rand:  0.7003
INFO:src.model.gail:iter: 843000 | update: 843 | d_loss: 0.61 | pi_loss: -0.07 | d_exp:  0.2334 | d_nov:  0.7557 | d_rand:  0.7003
INFO:src.model.gail:iter: 844000 | update: 844 | d_loss: 0.66 | pi_loss:  0.15 | d_exp:  0.2334 | d_nov:  0.7137 | d_rand:  0.7003
INFO:src.model.gail:iter: 845000 | update: 845 | d_loss: 0.64 | pi_loss:  0.04 | d_exp:  0.2334 | d_nov:  0.7251 | d_rand:  0.7003
INFO:src.model.gail:iter: 846000 | update: 846 | d_loss: 0.63 | pi_loss: -0.06 | d_exp:  0.2335 | d_nov:  0.7464 | d_rand:  0.7003
INFO:src.model.gail:iter: 847000 | update: 847 | d_loss: 0.66 | pi_loss: -0.02 | d_exp:  0.2335 | d_nov:  0.7108 | d_rand:  0.7003
INFO:src.model.gail:iter: 848000 | update: 848 | d_loss: 0.69 | pi_loss:  0.28 | d_exp:  0.2335 | d_nov:  0.7036 | d_rand:  0.7003
INFO:src.model.gail:iter: 849000 | update: 849 | d_loss: 0.64 | pi_loss:  0.24 | d_exp:  0.2335 | d_nov:  0.7182 | d_rand:  0.7003
INFO:src.model.gail:iter: 850000 | update: 850 | d_loss: 0.69 | pi_loss:  0.12 | d_exp:  0.2335 | d_nov:  0.7111 | d_rand:  0.7003
INFO:src.model.gail:iter: 851000 | update: 851 | d_loss: 0.55 | pi_loss: -0.14 | d_exp:  0.2336 | d_nov:  0.7837 | d_rand:  0.7003
INFO:src.model.gail:iter: 852000 | update: 852 | d_loss: 0.68 | pi_loss: -0.08 | d_exp:  0.2336 | d_nov:  0.7052 | d_rand:  0.7003
INFO:src.model.gail:iter: 853000 | update: 853 | d_loss: 0.65 | pi_loss:  0.26 | d_exp:  0.2336 | d_nov:  0.7202 | d_rand:  0.7003
INFO:src.model.gail:iter: 854000 | update: 854 | d_loss: 0.70 | pi_loss:  0.01 | d_exp:  0.2336 | d_nov:  0.6913 | d_rand:  0.7003
INFO:src.model.gail:iter: 855000 | update: 855 | d_loss: 0.59 | pi_loss: -0.22 | d_exp:  0.2336 | d_nov:  0.7648 | d_rand:  0.7003
INFO:src.model.gail:iter: 856000 | update: 856 | d_loss: 0.72 | pi_loss:  0.07 | d_exp:  0.2337 | d_nov:  0.6833 | d_rand:  0.7003
INFO:src.model.gail:iter: 857000 | update: 857 | d_loss: 0.65 | pi_loss:  0.14 | d_exp:  0.2337 | d_nov:  0.7101 | d_rand:  0.7004
INFO:src.model.gail:iter: 858000 | update: 858 | d_loss: 0.74 | pi_loss:  0.07 | d_exp:  0.2337 | d_nov:  0.6794 | d_rand:  0.7004
INFO:src.model.gail:iter: 859000 | update: 859 | d_loss: 0.76 | pi_loss:  0.49 | d_exp:  0.2337 | d_nov:  0.6508 | d_rand:  0.7004
INFO:src.model.gail:iter: 860000 | update: 860 | d_loss: 0.62 | pi_loss:  0.01 | d_exp:  0.2338 | d_nov:  0.7445 | d_rand:  0.7004
INFO:src.model.gail:iter: 861000 | update: 861 | d_loss: 0.66 | pi_loss: -0.01 | d_exp:  0.2338 | d_nov:  0.7169 | d_rand:  0.7004
INFO:src.model.gail:iter: 862000 | update: 862 | d_loss: 0.63 | pi_loss: -0.16 | d_exp:  0.2338 | d_nov:  0.7426 | d_rand:  0.7004
INFO:src.model.gail:iter: 863000 | update: 863 | d_loss: 0.62 | pi_loss:  0.07 | d_exp:  0.2338 | d_nov:  0.7420 | d_rand:  0.7004
INFO:src.model.gail:iter: 864000 | update: 864 | d_loss: 0.59 | pi_loss:  0.33 | d_exp:  0.2338 | d_nov:  0.7758 | d_rand:  0.7004
INFO:src.model.gail:iter: 865000 | update: 865 | d_loss: 0.68 | pi_loss:  0.01 | d_exp:  0.2339 | d_nov:  0.6964 | d_rand:  0.7004
INFO:src.model.gail:iter: 866000 | update: 866 | d_loss: 0.78 | pi_loss:  0.12 | d_exp:  0.2339 | d_nov:  0.6288 | d_rand:  0.7004
INFO:src.model.gail:iter: 867000 | update: 867 | d_loss: 0.65 | pi_loss:  0.01 | d_exp:  0.2339 | d_nov:  0.7114 | d_rand:  0.7004
INFO:src.model.gail:iter: 868000 | update: 868 | d_loss: 0.56 | pi_loss:  0.04 | d_exp:  0.2339 | d_nov:  0.7858 | d_rand:  0.7004
INFO:src.model.gail:iter: 869000 | update: 869 | d_loss: 0.56 | pi_loss:  0.06 | d_exp:  0.2339 | d_nov:  0.7804 | d_rand:  0.7004
INFO:src.model.gail:iter: 870000 | update: 870 | d_loss: 0.53 | pi_loss: -0.23 | d_exp:  0.2340 | d_nov:  0.8154 | d_rand:  0.7004
INFO:src.model.gail:iter: 871000 | update: 871 | d_loss: 0.59 | pi_loss:  0.14 | d_exp:  0.2340 | d_nov:  0.7563 | d_rand:  0.7004
INFO:src.model.gail:iter: 872000 | update: 872 | d_loss: 0.61 | pi_loss: -0.10 | d_exp:  0.2340 | d_nov:  0.7480 | d_rand:  0.7004
INFO:src.model.gail:iter: 873000 | update: 873 | d_loss: 0.53 | pi_loss: -0.16 | d_exp:  0.2340 | d_nov:  0.8296 | d_rand:  0.7004
INFO:src.model.gail:iter: 874000 | update: 874 | d_loss: 0.61 | pi_loss:  0.27 | d_exp:  0.2340 | d_nov:  0.7397 | d_rand:  0.7004
INFO:src.model.gail:iter: 875000 | update: 875 | d_loss: 0.80 | pi_loss: -0.01 | d_exp:  0.2340 | d_nov:  0.6287 | d_rand:  0.7004
INFO:src.model.gail:iter: 876000 | update: 876 | d_loss: 0.64 | pi_loss:  0.09 | d_exp:  0.2340 | d_nov:  0.7197 | d_rand:  0.7004
INFO:src.model.gail:iter: 877000 | update: 877 | d_loss: 0.57 | pi_loss:  0.10 | d_exp:  0.2340 | d_nov:  0.7698 | d_rand:  0.7004
INFO:src.model.gail:iter: 878000 | update: 878 | d_loss: 0.66 | pi_loss: -0.03 | d_exp:  0.2340 | d_nov:  0.7223 | d_rand:  0.7004
INFO:src.model.gail:iter: 879000 | update: 879 | d_loss: 0.75 | pi_loss:  0.31 | d_exp:  0.2340 | d_nov:  0.6652 | d_rand:  0.7004
INFO:src.model.gail:iter: 880000 | update: 880 | d_loss: 0.69 | pi_loss:  0.08 | d_exp:  0.2340 | d_nov:  0.6868 | d_rand:  0.7004
INFO:src.model.gail:iter: 881000 | update: 881 | d_loss: 0.65 | pi_loss: -0.13 | d_exp:  0.2341 | d_nov:  0.7423 | d_rand:  0.7004
INFO:src.model.gail:iter: 882000 | update: 882 | d_loss: 0.68 | pi_loss:  0.09 | d_exp:  0.2341 | d_nov:  0.7017 | d_rand:  0.7005
INFO:src.model.gail:iter: 883000 | update: 883 | d_loss: 0.67 | pi_loss:  0.17 | d_exp:  0.2341 | d_nov:  0.7067 | d_rand:  0.7005
INFO:src.model.gail:iter: 884000 | update: 884 | d_loss: 0.70 | pi_loss:  0.03 | d_exp:  0.2341 | d_nov:  0.6915 | d_rand:  0.7005
INFO:src.model.gail:iter: 885000 | update: 885 | d_loss: 0.70 | pi_loss: -0.05 | d_exp:  0.2341 | d_nov:  0.6856 | d_rand:  0.7005
INFO:src.model.gail:iter: 886000 | update: 886 | d_loss: 0.67 | pi_loss:  0.15 | d_exp:  0.2342 | d_nov:  0.7104 | d_rand:  0.7005
INFO:src.model.gail:iter: 887000 | update: 887 | d_loss: 0.73 | pi_loss: -0.08 | d_exp:  0.2342 | d_nov:  0.6781 | d_rand:  0.7005
INFO:src.model.gail:iter: 888000 | update: 888 | d_loss: 0.70 | pi_loss:  0.14 | d_exp:  0.2342 | d_nov:  0.6872 | d_rand:  0.7005
INFO:src.model.gail:iter: 889000 | update: 889 | d_loss: 0.70 | pi_loss: -0.02 | d_exp:  0.2342 | d_nov:  0.6888 | d_rand:  0.7005
INFO:src.model.gail:iter: 890000 | update: 890 | d_loss: 0.85 | pi_loss: -0.15 | d_exp:  0.2343 | d_nov:  0.6390 | d_rand:  0.7005
INFO:src.model.gail:iter: 891000 | update: 891 | d_loss: 0.63 | pi_loss:  0.07 | d_exp:  0.2343 | d_nov:  0.7375 | d_rand:  0.7005
INFO:src.model.gail:iter: 892000 | update: 892 | d_loss: 0.64 | pi_loss:  0.18 | d_exp:  0.2343 | d_nov:  0.7309 | d_rand:  0.7005
INFO:src.model.gail:iter: 893000 | update: 893 | d_loss: 0.62 | pi_loss:  0.13 | d_exp:  0.2343 | d_nov:  0.7374 | d_rand:  0.7005
INFO:src.model.gail:iter: 894000 | update: 894 | d_loss: 0.69 | pi_loss:  0.22 | d_exp:  0.2343 | d_nov:  0.7000 | d_rand:  0.7005
INFO:src.model.gail:iter: 895000 | update: 895 | d_loss: 0.64 | pi_loss:  0.04 | d_exp:  0.2344 | d_nov:  0.7256 | d_rand:  0.7005
INFO:src.model.gail:iter: 896000 | update: 896 | d_loss: 0.50 | pi_loss: -0.13 | d_exp:  0.2344 | d_nov:  0.8446 | d_rand:  0.7005
INFO:src.model.gail:iter: 897000 | update: 897 | d_loss: 0.62 | pi_loss: -0.10 | d_exp:  0.2344 | d_nov:  0.7661 | d_rand:  0.7005
INFO:src.model.gail:iter: 898000 | update: 898 | d_loss: 0.57 | pi_loss:  0.19 | d_exp:  0.2344 | d_nov:  0.7795 | d_rand:  0.7005
INFO:src.model.gail:iter: 899000 | update: 899 | d_loss: 0.69 | pi_loss:  0.07 | d_exp:  0.2344 | d_nov:  0.7304 | d_rand:  0.7005
INFO:src.model.gail:iter: 900000 | update: 900 | d_loss: 0.94 | pi_loss:  0.45 | d_exp:  0.2344 | d_nov:  0.5437 | d_rand:  0.7005
INFO:src.model.gail:iter: 901000 | update: 901 | d_loss: 0.62 | pi_loss:  0.01 | d_exp:  0.2344 | d_nov:  0.7379 | d_rand:  0.7006
INFO:src.model.gail:iter: 902000 | update: 902 | d_loss: 0.47 | pi_loss: -0.19 | d_exp:  0.2344 | d_nov:  0.8558 | d_rand:  0.7006
INFO:src.model.gail:iter: 903000 | update: 903 | d_loss: 0.56 | pi_loss: -0.17 | d_exp:  0.2344 | d_nov:  0.7912 | d_rand:  0.7006
INFO:src.model.gail:iter: 904000 | update: 904 | d_loss: 0.66 | pi_loss: -0.03 | d_exp:  0.2344 | d_nov:  0.7103 | d_rand:  0.7006
INFO:src.model.gail:iter: 905000 | update: 905 | d_loss: 0.84 | pi_loss:  0.22 | d_exp:  0.2345 | d_nov:  0.5913 | d_rand:  0.7006
INFO:src.model.gail:iter: 906000 | update: 906 | d_loss: 0.84 | pi_loss:  0.14 | d_exp:  0.2345 | d_nov:  0.6038 | d_rand:  0.7006
INFO:src.model.gail:iter: 907000 | update: 907 | d_loss: 0.76 | pi_loss: -0.05 | d_exp:  0.2345 | d_nov:  0.6489 | d_rand:  0.7006
INFO:src.model.gail:iter: 908000 | update: 908 | d_loss: 0.58 | pi_loss:  0.13 | d_exp:  0.2345 | d_nov:  0.7730 | d_rand:  0.7006
INFO:src.model.gail:iter: 909000 | update: 909 | d_loss: 0.53 | pi_loss: -0.08 | d_exp:  0.2345 | d_nov:  0.8168 | d_rand:  0.7006
INFO:src.model.gail:iter: 910000 | update: 910 | d_loss: 0.52 | pi_loss: -0.13 | d_exp:  0.2345 | d_nov:  0.8146 | d_rand:  0.7006
INFO:src.model.gail:iter: 911000 | update: 911 | d_loss: 0.55 | pi_loss: -0.12 | d_exp:  0.2346 | d_nov:  0.7901 | d_rand:  0.7006
INFO:src.model.gail:iter: 912000 | update: 912 | d_loss: 0.75 | pi_loss:  0.07 | d_exp:  0.2346 | d_nov:  0.6450 | d_rand:  0.7006
INFO:src.model.gail:iter: 913000 | update: 913 | d_loss: 0.55 | pi_loss: -0.05 | d_exp:  0.2346 | d_nov:  0.7944 | d_rand:  0.7006
INFO:src.model.gail:iter: 914000 | update: 914 | d_loss: 0.61 | pi_loss: -0.18 | d_exp:  0.2346 | d_nov:  0.7623 | d_rand:  0.7006
INFO:src.model.gail:iter: 915000 | update: 915 | d_loss: 0.62 | pi_loss: -0.04 | d_exp:  0.2346 | d_nov:  0.7419 | d_rand:  0.7006
INFO:src.model.gail:iter: 916000 | update: 916 | d_loss: 0.61 | pi_loss:  0.22 | d_exp:  0.2346 | d_nov:  0.7424 | d_rand:  0.7006
INFO:src.model.gail:iter: 917000 | update: 917 | d_loss: 0.59 | pi_loss: -0.20 | d_exp:  0.2346 | d_nov:  0.7601 | d_rand:  0.7006
INFO:src.model.gail:iter: 918000 | update: 918 | d_loss: 0.55 | pi_loss: -0.31 | d_exp:  0.2346 | d_nov:  0.7942 | d_rand:  0.7006
INFO:src.model.gail:iter: 919000 | update: 919 | d_loss: 0.60 | pi_loss: -0.12 | d_exp:  0.2346 | d_nov:  0.7483 | d_rand:  0.7006
INFO:src.model.gail:iter: 920000 | update: 920 | d_loss: 0.74 | pi_loss:  0.05 | d_exp:  0.2346 | d_nov:  0.6610 | d_rand:  0.7006
INFO:src.model.gail:iter: 921000 | update: 921 | d_loss: 0.65 | pi_loss:  0.12 | d_exp:  0.2346 | d_nov:  0.7166 | d_rand:  0.7006
INFO:src.model.gail:iter: 922000 | update: 922 | d_loss: 0.67 | pi_loss:  0.10 | d_exp:  0.2346 | d_nov:  0.7045 | d_rand:  0.7006
INFO:src.model.gail:iter: 923000 | update: 923 | d_loss: 0.71 | pi_loss:  0.44 | d_exp:  0.2346 | d_nov:  0.6836 | d_rand:  0.7006
INFO:src.model.gail:iter: 924000 | update: 924 | d_loss: 0.45 | pi_loss: -0.16 | d_exp:  0.2346 | d_nov:  0.8679 | d_rand:  0.7006
INFO:src.model.gail:iter: 925000 | update: 925 | d_loss: 0.51 | pi_loss: -0.25 | d_exp:  0.2346 | d_nov:  0.8215 | d_rand:  0.7006
INFO:src.model.gail:iter: 926000 | update: 926 | d_loss: 0.57 | pi_loss: -0.07 | d_exp:  0.2346 | d_nov:  0.7796 | d_rand:  0.7006
INFO:src.model.gail:iter: 927000 | update: 927 | d_loss: 0.68 | pi_loss: -0.11 | d_exp:  0.2346 | d_nov:  0.7000 | d_rand:  0.7006
INFO:src.model.gail:iter: 928000 | update: 928 | d_loss: 0.71 | pi_loss:  0.05 | d_exp:  0.2346 | d_nov:  0.6903 | d_rand:  0.7006
INFO:src.model.gail:iter: 929000 | update: 929 | d_loss: 0.82 | pi_loss: -0.02 | d_exp:  0.2346 | d_nov:  0.6165 | d_rand:  0.7006
INFO:src.model.gail:iter: 930000 | update: 930 | d_loss: 0.78 | pi_loss:  0.44 | d_exp:  0.2346 | d_nov:  0.6283 | d_rand:  0.7006
INFO:src.model.gail:iter: 931000 | update: 931 | d_loss: 0.59 | pi_loss: -0.05 | d_exp:  0.2347 | d_nov:  0.7767 | d_rand:  0.7006
INFO:src.model.gail:iter: 932000 | update: 932 | d_loss: 0.64 | pi_loss:  0.04 | d_exp:  0.2347 | d_nov:  0.7212 | d_rand:  0.7006
INFO:src.model.gail:iter: 933000 | update: 933 | d_loss: 0.67 | pi_loss: -0.16 | d_exp:  0.2347 | d_nov:  0.7138 | d_rand:  0.7006
INFO:src.model.gail:iter: 934000 | update: 934 | d_loss: 0.59 | pi_loss:  0.27 | d_exp:  0.2347 | d_nov:  0.7691 | d_rand:  0.7006
INFO:src.model.gail:iter: 935000 | update: 935 | d_loss: 0.63 | pi_loss: -0.02 | d_exp:  0.2347 | d_nov:  0.7323 | d_rand:  0.7006
INFO:src.model.gail:iter: 936000 | update: 936 | d_loss: 0.60 | pi_loss:  0.22 | d_exp:  0.2347 | d_nov:  0.7634 | d_rand:  0.7006
INFO:src.model.gail:iter: 937000 | update: 937 | d_loss: 0.65 | pi_loss:  0.05 | d_exp:  0.2347 | d_nov:  0.7175 | d_rand:  0.7006
INFO:src.model.gail:iter: 938000 | update: 938 | d_loss: 0.62 | pi_loss:  0.15 | d_exp:  0.2347 | d_nov:  0.7448 | d_rand:  0.7006
INFO:src.model.gail:iter: 939000 | update: 939 | d_loss: 0.58 | pi_loss:  0.01 | d_exp:  0.2347 | d_nov:  0.7753 | d_rand:  0.7006
INFO:src.model.gail:iter: 940000 | update: 940 | d_loss: 0.61 | pi_loss:  0.01 | d_exp:  0.2347 | d_nov:  0.7593 | d_rand:  0.7006
INFO:src.model.gail:iter: 941000 | update: 941 | d_loss: 0.62 | pi_loss:  0.08 | d_exp:  0.2347 | d_nov:  0.7350 | d_rand:  0.7006
INFO:src.model.gail:iter: 942000 | update: 942 | d_loss: 0.48 | pi_loss: -0.12 | d_exp:  0.2347 | d_nov:  0.8420 | d_rand:  0.7006
INFO:src.model.gail:iter: 943000 | update: 943 | d_loss: 0.65 | pi_loss:  0.02 | d_exp:  0.2347 | d_nov:  0.7154 | d_rand:  0.7006
INFO:src.model.gail:iter: 944000 | update: 944 | d_loss: 0.64 | pi_loss:  0.15 | d_exp:  0.2347 | d_nov:  0.7402 | d_rand:  0.7006
INFO:src.model.gail:iter: 945000 | update: 945 | d_loss: 0.54 | pi_loss: -0.10 | d_exp:  0.2347 | d_nov:  0.7979 | d_rand:  0.7006
INFO:src.model.gail:iter: 946000 | update: 946 | d_loss: 0.73 | pi_loss:  0.26 | d_exp:  0.2347 | d_nov:  0.6651 | d_rand:  0.7006
INFO:src.model.gail:iter: 947000 | update: 947 | d_loss: 0.75 | pi_loss: -0.00 | d_exp:  0.2347 | d_nov:  0.6532 | d_rand:  0.7006
INFO:src.model.gail:iter: 948000 | update: 948 | d_loss: 0.69 | pi_loss:  0.00 | d_exp:  0.2347 | d_nov:  0.6987 | d_rand:  0.7006
INFO:src.model.gail:iter: 949000 | update: 949 | d_loss: 0.64 | pi_loss: -0.06 | d_exp:  0.2348 | d_nov:  0.7274 | d_rand:  0.7006
INFO:src.model.gail:iter: 950000 | update: 950 | d_loss: 0.71 | pi_loss: -0.04 | d_exp:  0.2348 | d_nov:  0.6709 | d_rand:  0.7006
INFO:src.model.gail:iter: 951000 | update: 951 | d_loss: 0.56 | pi_loss:  0.06 | d_exp:  0.2348 | d_nov:  0.7826 | d_rand:  0.7006
INFO:src.model.gail:iter: 952000 | update: 952 | d_loss: 0.64 | pi_loss:  0.36 | d_exp:  0.2348 | d_nov:  0.7351 | d_rand:  0.7006
INFO:src.model.gail:iter: 953000 | update: 953 | d_loss: 0.61 | pi_loss:  0.08 | d_exp:  0.2348 | d_nov:  0.7577 | d_rand:  0.7006
INFO:src.model.gail:iter: 954000 | update: 954 | d_loss: 0.71 | pi_loss:  0.18 | d_exp:  0.2348 | d_nov:  0.6868 | d_rand:  0.7006
INFO:src.model.gail:iter: 955000 | update: 955 | d_loss: 0.73 | pi_loss: -0.15 | d_exp:  0.2348 | d_nov:  0.6654 | d_rand:  0.7006
INFO:src.model.gail:iter: 956000 | update: 956 | d_loss: 0.54 | pi_loss: -0.19 | d_exp:  0.2348 | d_nov:  0.8015 | d_rand:  0.7006
INFO:src.model.gail:iter: 957000 | update: 957 | d_loss: 0.65 | pi_loss:  0.32 | d_exp:  0.2348 | d_nov:  0.7195 | d_rand:  0.7006
INFO:src.model.gail:iter: 958000 | update: 958 | d_loss: 0.57 | pi_loss:  0.09 | d_exp:  0.2348 | d_nov:  0.7865 | d_rand:  0.7006
INFO:src.model.gail:iter: 959000 | update: 959 | d_loss: 0.81 | pi_loss:  0.29 | d_exp:  0.2348 | d_nov:  0.6255 | d_rand:  0.7006
INFO:src.model.gail:iter: 960000 | update: 960 | d_loss: 0.71 | pi_loss:  0.16 | d_exp:  0.2348 | d_nov:  0.6906 | d_rand:  0.7006
INFO:src.model.gail:iter: 961000 | update: 961 | d_loss: 0.70 | pi_loss:  0.06 | d_exp:  0.2348 | d_nov:  0.7164 | d_rand:  0.7006
INFO:src.model.gail:iter: 962000 | update: 962 | d_loss: 0.74 | pi_loss:  0.02 | d_exp:  0.2349 | d_nov:  0.6579 | d_rand:  0.7006
INFO:src.model.gail:iter: 963000 | update: 963 | d_loss: 0.49 | pi_loss: -0.09 | d_exp:  0.2349 | d_nov:  0.8416 | d_rand:  0.7007
INFO:src.model.gail:iter: 964000 | update: 964 | d_loss: 0.53 | pi_loss: -0.01 | d_exp:  0.2349 | d_nov:  0.8028 | d_rand:  0.7007
INFO:src.model.gail:iter: 965000 | update: 965 | d_loss: 0.60 | pi_loss: -0.22 | d_exp:  0.2349 | d_nov:  0.7702 | d_rand:  0.7007
INFO:src.model.gail:iter: 966000 | update: 966 | d_loss: 0.69 | pi_loss: -0.07 | d_exp:  0.2349 | d_nov:  0.6933 | d_rand:  0.7007
INFO:src.model.gail:iter: 967000 | update: 967 | d_loss: 0.87 | pi_loss: -0.16 | d_exp:  0.2349 | d_nov:  0.5951 | d_rand:  0.7007
INFO:src.model.gail:iter: 968000 | update: 968 | d_loss: 0.77 | pi_loss: -0.12 | d_exp:  0.2349 | d_nov:  0.6518 | d_rand:  0.7007
INFO:src.model.gail:iter: 969000 | update: 969 | d_loss: 0.67 | pi_loss: -0.10 | d_exp:  0.2349 | d_nov:  0.7075 | d_rand:  0.7007
INFO:src.model.gail:iter: 970000 | update: 970 | d_loss: 0.76 | pi_loss:  0.09 | d_exp:  0.2349 | d_nov:  0.6385 | d_rand:  0.7007
INFO:src.model.gail:iter: 971000 | update: 971 | d_loss: 0.77 | pi_loss:  0.08 | d_exp:  0.2349 | d_nov:  0.6338 | d_rand:  0.7007
INFO:src.model.gail:iter: 972000 | update: 972 | d_loss: 0.71 | pi_loss: -0.04 | d_exp:  0.2349 | d_nov:  0.6827 | d_rand:  0.7007
INFO:src.model.gail:iter: 973000 | update: 973 | d_loss: 0.62 | pi_loss: -0.14 | d_exp:  0.2350 | d_nov:  0.7442 | d_rand:  0.7007
INFO:src.model.gail:iter: 974000 | update: 974 | d_loss: 0.68 | pi_loss:  0.38 | d_exp:  0.2350 | d_nov:  0.7049 | d_rand:  0.7007
INFO:src.model.gail:iter: 975000 | update: 975 | d_loss: 0.65 | pi_loss: -0.10 | d_exp:  0.2350 | d_nov:  0.7601 | d_rand:  0.7007
INFO:src.model.gail:iter: 976000 | update: 976 | d_loss: 0.78 | pi_loss: -0.06 | d_exp:  0.2350 | d_nov:  0.6383 | d_rand:  0.7007
INFO:src.model.gail:iter: 977000 | update: 977 | d_loss: 0.74 | pi_loss:  0.41 | d_exp:  0.2350 | d_nov:  0.6732 | d_rand:  0.7007
INFO:src.model.gail:iter: 978000 | update: 978 | d_loss: 0.68 | pi_loss:  0.19 | d_exp:  0.2350 | d_nov:  0.7004 | d_rand:  0.7007
INFO:src.model.gail:iter: 979000 | update: 979 | d_loss: 0.62 | pi_loss: -0.09 | d_exp:  0.2350 | d_nov:  0.7460 | d_rand:  0.7007
INFO:src.model.gail:iter: 980000 | update: 980 | d_loss: 0.68 | pi_loss:  0.36 | d_exp:  0.2350 | d_nov:  0.7063 | d_rand:  0.7007
INFO:src.model.gail:iter: 981000 | update: 981 | d_loss: 0.78 | pi_loss:  0.40 | d_exp:  0.2350 | d_nov:  0.6694 | d_rand:  0.7007
INFO:src.model.gail:iter: 982000 | update: 982 | d_loss: 0.56 | pi_loss:  0.09 | d_exp:  0.2350 | d_nov:  0.7795 | d_rand:  0.7007
INFO:src.model.gail:iter: 983000 | update: 983 | d_loss: 0.67 | pi_loss: -0.23 | d_exp:  0.2351 | d_nov:  0.7097 | d_rand:  0.7007
INFO:src.model.gail:iter: 984000 | update: 984 | d_loss: 0.68 | pi_loss:  0.12 | d_exp:  0.2351 | d_nov:  0.7024 | d_rand:  0.7007
INFO:src.model.gail:iter: 985000 | update: 985 | d_loss: 0.66 | pi_loss:  0.16 | d_exp:  0.2351 | d_nov:  0.7097 | d_rand:  0.7007
INFO:src.model.gail:iter: 986000 | update: 986 | d_loss: 0.70 | pi_loss:  0.09 | d_exp:  0.2351 | d_nov:  0.6981 | d_rand:  0.7007
INFO:src.model.gail:iter: 987000 | update: 987 | d_loss: 0.63 | pi_loss:  0.04 | d_exp:  0.2351 | d_nov:  0.7327 | d_rand:  0.7007
INFO:src.model.gail:iter: 988000 | update: 988 | d_loss: 0.60 | pi_loss: -0.01 | d_exp:  0.2351 | d_nov:  0.7486 | d_rand:  0.7007
INFO:src.model.gail:iter: 989000 | update: 989 | d_loss: 0.61 | pi_loss: -0.15 | d_exp:  0.2351 | d_nov:  0.7467 | d_rand:  0.7007
INFO:src.model.gail:iter: 990000 | update: 990 | d_loss: 0.76 | pi_loss: -0.14 | d_exp:  0.2351 | d_nov:  0.6649 | d_rand:  0.7007
INFO:src.model.gail:iter: 991000 | update: 991 | d_loss: 0.77 | pi_loss: -0.19 | d_exp:  0.2351 | d_nov:  0.6335 | d_rand:  0.7007
INFO:src.model.gail:iter: 992000 | update: 992 | d_loss: 0.65 | pi_loss: -0.06 | d_exp:  0.2351 | d_nov:  0.7244 | d_rand:  0.7007
INFO:src.model.gail:iter: 993000 | update: 993 | d_loss: 0.68 | pi_loss: -0.01 | d_exp:  0.2351 | d_nov:  0.7382 | d_rand:  0.7007
INFO:src.model.gail:iter: 994000 | update: 994 | d_loss: 0.72 | pi_loss:  0.15 | d_exp:  0.2351 | d_nov:  0.6800 | d_rand:  0.7007
INFO:src.model.gail:iter: 995000 | update: 995 | d_loss: 0.69 | pi_loss:  0.02 | d_exp:  0.2351 | d_nov:  0.6905 | d_rand:  0.7007
INFO:src.model.gail:iter: 996000 | update: 996 | d_loss: 0.63 | pi_loss: -0.01 | d_exp:  0.2352 | d_nov:  0.7327 | d_rand:  0.7007
INFO:src.model.gail:iter: 997000 | update: 997 | d_loss: 0.56 | pi_loss:  0.08 | d_exp:  0.2352 | d_nov:  0.7860 | d_rand:  0.7007
INFO:src.model.gail:iter: 998000 | update: 998 | d_loss: 0.68 | pi_loss: -0.01 | d_exp:  0.2352 | d_nov:  0.6971 | d_rand:  0.7007
INFO:src.model.gail:iter: 999000 | update: 999 | d_loss: 0.65 | pi_loss:  0.07 | d_exp:  0.2352 | d_nov:  0.7236 | d_rand:  0.7007
INFO:src.model.gail:iter: 1000000 | update: 1000 | d_loss: 0.62 | pi_loss: -0.18 | d_exp:  0.2352 | d_nov:  0.7393 | d_rand:  0.7007
INFO:src.model.gail:--finished training. saving checkpoint--
INFO:src.model.gail:creating simulated data
INFO:__main__:-- training narrative planner with sim data --
INFO:src.model.bcq:-- no behavior cloning with run_name seed_1 --
INFO:src.model.bcq:-- training bcq behavior cloning --
INFO:src.model.bcq:training behavior cloning | step 1000 or 100000 | loss  0.0613
INFO:src.model.bcq:training behavior cloning | step 2000 or 100000 | loss  0.0595
INFO:src.model.bcq:training behavior cloning | step 3000 or 100000 | loss  0.0599
INFO:src.model.bcq:training behavior cloning | step 4000 or 100000 | loss  0.0571
INFO:src.model.bcq:training behavior cloning | step 5000 or 100000 | loss  0.0582
INFO:src.model.bcq:training behavior cloning | step 6000 or 100000 | loss  0.0583
INFO:src.model.bcq:training behavior cloning | step 7000 or 100000 | loss  0.0561
INFO:src.model.bcq:training behavior cloning | step 8000 or 100000 | loss  0.0518
INFO:src.model.bcq:training behavior cloning | step 9000 or 100000 | loss  0.0518
INFO:src.model.bcq:training behavior cloning | step 10000 or 100000 | loss  0.0478
INFO:src.model.bcq:training behavior cloning | step 11000 or 100000 | loss  0.0474
INFO:src.model.bcq:training behavior cloning | step 12000 or 100000 | loss  0.0440
INFO:src.model.bcq:training behavior cloning | step 13000 or 100000 | loss  0.0462
INFO:src.model.bcq:training behavior cloning | step 14000 or 100000 | loss  0.0417
INFO:src.model.bcq:training behavior cloning | step 15000 or 100000 | loss  0.0373
INFO:src.model.bcq:training behavior cloning | step 16000 or 100000 | loss  0.0385
INFO:src.model.bcq:training behavior cloning | step 17000 or 100000 | loss  0.0356
INFO:src.model.bcq:training behavior cloning | step 18000 or 100000 | loss  0.0360
INFO:src.model.bcq:training behavior cloning | step 19000 or 100000 | loss  0.0314
INFO:src.model.bcq:training behavior cloning | step 20000 or 100000 | loss  0.0334
INFO:src.model.bcq:training behavior cloning | step 21000 or 100000 | loss  0.0341
INFO:src.model.bcq:training behavior cloning | step 22000 or 100000 | loss  0.0348
INFO:src.model.bcq:training behavior cloning | step 23000 or 100000 | loss  0.0316
INFO:src.model.bcq:training behavior cloning | step 24000 or 100000 | loss  0.0339
INFO:src.model.bcq:training behavior cloning | step 25000 or 100000 | loss  0.0322
INFO:src.model.bcq:training behavior cloning | step 26000 or 100000 | loss  0.0324
INFO:src.model.bcq:training behavior cloning | step 27000 or 100000 | loss  0.0295
INFO:src.model.bcq:training behavior cloning | step 28000 or 100000 | loss  0.0272
INFO:src.model.bcq:training behavior cloning | step 29000 or 100000 | loss  0.0299
INFO:src.model.bcq:training behavior cloning | step 30000 or 100000 | loss  0.0301
INFO:src.model.bcq:training behavior cloning | step 31000 or 100000 | loss  0.0310
INFO:src.model.bcq:training behavior cloning | step 32000 or 100000 | loss  0.0310
INFO:src.model.bcq:training behavior cloning | step 33000 or 100000 | loss  0.0297
INFO:src.model.bcq:training behavior cloning | step 34000 or 100000 | loss  0.0267
INFO:src.model.bcq:training behavior cloning | step 35000 or 100000 | loss  0.0271
INFO:src.model.bcq:training behavior cloning | step 36000 or 100000 | loss  0.0286
INFO:src.model.bcq:training behavior cloning | step 37000 or 100000 | loss  0.0285
INFO:src.model.bcq:training behavior cloning | step 38000 or 100000 | loss  0.0305
INFO:src.model.bcq:training behavior cloning | step 39000 or 100000 | loss  0.0279
INFO:src.model.bcq:training behavior cloning | step 40000 or 100000 | loss  0.0282
INFO:src.model.bcq:training behavior cloning | step 41000 or 100000 | loss  0.0273
INFO:src.model.bcq:training behavior cloning | step 42000 or 100000 | loss  0.0299
INFO:src.model.bcq:training behavior cloning | step 43000 or 100000 | loss  0.0269
INFO:src.model.bcq:training behavior cloning | step 44000 or 100000 | loss  0.0280
INFO:src.model.bcq:training behavior cloning | step 45000 or 100000 | loss  0.0289
INFO:src.model.bcq:training behavior cloning | step 46000 or 100000 | loss  0.0301
INFO:src.model.bcq:training behavior cloning | step 47000 or 100000 | loss  0.0282
INFO:src.model.bcq:training behavior cloning | step 48000 or 100000 | loss  0.0269
INFO:src.model.bcq:training behavior cloning | step 49000 or 100000 | loss  0.0283
INFO:src.model.bcq:training behavior cloning | step 50000 or 100000 | loss  0.0289
INFO:src.model.bcq:training behavior cloning | step 51000 or 100000 | loss  0.0262
INFO:src.model.bcq:training behavior cloning | step 52000 or 100000 | loss  0.0263
INFO:src.model.bcq:training behavior cloning | step 53000 or 100000 | loss  0.0276
INFO:src.model.bcq:training behavior cloning | step 54000 or 100000 | loss  0.0302
INFO:src.model.bcq:training behavior cloning | step 55000 or 100000 | loss  0.0271
INFO:src.model.bcq:training behavior cloning | step 56000 or 100000 | loss  0.0256
INFO:src.model.bcq:training behavior cloning | step 57000 or 100000 | loss  0.0286
INFO:src.model.bcq:training behavior cloning | step 58000 or 100000 | loss  0.0269
INFO:src.model.bcq:training behavior cloning | step 59000 or 100000 | loss  0.0270
INFO:src.model.bcq:training behavior cloning | step 60000 or 100000 | loss  0.0271
INFO:src.model.bcq:training behavior cloning | step 61000 or 100000 | loss  0.0263
INFO:src.model.bcq:training behavior cloning | step 62000 or 100000 | loss  0.0255
INFO:src.model.bcq:training behavior cloning | step 63000 or 100000 | loss  0.0252
INFO:src.model.bcq:training behavior cloning | step 64000 or 100000 | loss  0.0277
INFO:src.model.bcq:training behavior cloning | step 65000 or 100000 | loss  0.0249
INFO:src.model.bcq:training behavior cloning | step 66000 or 100000 | loss  0.0276
INFO:src.model.bcq:training behavior cloning | step 67000 or 100000 | loss  0.0291
INFO:src.model.bcq:training behavior cloning | step 68000 or 100000 | loss  0.0268
INFO:src.model.bcq:training behavior cloning | step 69000 or 100000 | loss  0.0273
INFO:src.model.bcq:training behavior cloning | step 70000 or 100000 | loss  0.0266
INFO:src.model.bcq:training behavior cloning | step 71000 or 100000 | loss  0.0242
INFO:src.model.bcq:training behavior cloning | step 72000 or 100000 | loss  0.0317
INFO:src.model.bcq:training behavior cloning | step 73000 or 100000 | loss  0.0258
INFO:src.model.bcq:training behavior cloning | step 74000 or 100000 | loss  0.0275
INFO:src.model.bcq:training behavior cloning | step 75000 or 100000 | loss  0.0273
INFO:src.model.bcq:training behavior cloning | step 76000 or 100000 | loss  0.0274
INFO:src.model.bcq:training behavior cloning | step 77000 or 100000 | loss  0.0263
INFO:src.model.bcq:training behavior cloning | step 78000 or 100000 | loss  0.0296
INFO:src.model.bcq:training behavior cloning | step 79000 or 100000 | loss  0.0245
INFO:src.model.bcq:training behavior cloning | step 80000 or 100000 | loss  0.0239
INFO:src.model.bcq:training behavior cloning | step 81000 or 100000 | loss  0.0280
INFO:src.model.bcq:training behavior cloning | step 82000 or 100000 | loss  0.0338
INFO:src.model.bcq:training behavior cloning | step 83000 or 100000 | loss  0.0277
INFO:src.model.bcq:training behavior cloning | step 84000 or 100000 | loss  0.0269
INFO:src.model.bcq:training behavior cloning | step 85000 or 100000 | loss  0.0274
INFO:src.model.bcq:training behavior cloning | step 86000 or 100000 | loss  0.0279
INFO:src.model.bcq:training behavior cloning | step 87000 or 100000 | loss  0.0274
INFO:src.model.bcq:training behavior cloning | step 88000 or 100000 | loss  0.0291
INFO:src.model.bcq:training behavior cloning | step 89000 or 100000 | loss  0.0266
INFO:src.model.bcq:training behavior cloning | step 90000 or 100000 | loss  0.0292
INFO:src.model.bcq:training behavior cloning | step 91000 or 100000 | loss  0.0276
INFO:src.model.bcq:training behavior cloning | step 92000 or 100000 | loss  0.0264
INFO:src.model.bcq:training behavior cloning | step 93000 or 100000 | loss  0.0284
INFO:src.model.bcq:training behavior cloning | step 94000 or 100000 | loss  0.0270
INFO:src.model.bcq:training behavior cloning | step 95000 or 100000 | loss  0.0254
INFO:src.model.bcq:training behavior cloning | step 96000 or 100000 | loss  0.0258
INFO:src.model.bcq:training behavior cloning | step 97000 or 100000 | loss  0.0246
INFO:src.model.bcq:training behavior cloning | step 98000 or 100000 | loss  0.0241
INFO:src.model.bcq:training behavior cloning | step 99000 or 100000 | loss  0.0279
INFO:src.model.bcq:training behavior cloning | step 100000 or 100000 | loss  0.0263
INFO:src.model.bcq:--finished training behavior cloning--
INFO:src.model.bcq:-- saved behavior cloning with run_name seed_1 --
INFO:src.model.bcq:-- no bcq with run_name seed_1 --
INFO:src.model.bcq:-- training bcq --
INFO:src.model.bcq:epoch: 1000/100000 | Q loss:  294.5188 | ecr:  1.1967 | is:  18.7092 | wis:  18.2759 | dr:  139.8637 | dm:  100.8889
INFO:src.model.bcq:epoch: 2000/100000 | Q loss:  117.5737 | ecr:  4.5270 | is: -6.5368 | wis: -103.9413 | dr:  135.4273 | dm:  107.2129
INFO:src.model.bcq:epoch: 3000/100000 | Q loss:  81.8520 | ecr:  11.5321 | is: -789.4693 | wis: -38.6624 | dr:  123.8785 | dm:  108.0963
INFO:src.model.bcq:epoch: 4000/100000 | Q loss:  83.5792 | ecr:  22.7896 | is: -539.2269 | wis: -6.5793 | dr:  127.9183 | dm:  106.6760
INFO:src.model.bcq:epoch: 5000/100000 | Q loss:  42.2794 | ecr:  37.2887 | is:  1154.1875 | wis:  85.7408 | dr:  7912.6589 | dm:  121.4727
INFO:src.model.bcq:epoch: 6000/100000 | Q loss:  26.9408 | ecr:  52.0710 | is:  1701.8503 | wis:  79.4404 | dr:  22557.2120 | dm:  122.6476
INFO:src.model.bcq:epoch: 7000/100000 | Q loss:  18.7719 | ecr:  65.9558 | is:  881.2829 | wis:  2.5920 | dr:  8171.0639 | dm:  122.6476
INFO:src.model.bcq:epoch: 8000/100000 | Q loss:  33.3459 | ecr:  76.0827 | is:  56.2549 | wis:  11.5404 | dr:  325.6883 | dm:  122.6476
INFO:src.model.bcq:epoch: 9000/100000 | Q loss:  6.5505 | ecr:  82.5308 | is:  32.8888 | wis:  132.4629 | dr:  148.8242 | dm:  121.9854
INFO:src.model.bcq:epoch: 10000/100000 | Q loss:  2.9277 | ecr:  87.2046 | is:  5.5620 | wis:  161.1614 | dr:  134.7328 | dm:  122.5836
INFO:src.model.bcq:epoch: 11000/100000 | Q loss:  2.2757 | ecr:  89.5325 | is:  12.0311 | wis:  210.3843 | dr:  135.6422 | dm:  122.7293
INFO:src.model.bcq:epoch: 12000/100000 | Q loss:  1.7504 | ecr:  90.7395 | is:  6.9203 | wis:  144.7105 | dr:  136.1173 | dm:  122.5209
INFO:src.model.bcq:epoch: 13000/100000 | Q loss:  2.6976 | ecr:  90.7343 | is:  6.3178 | wis:  178.2750 | dr:  136.7472 | dm:  122.8537
INFO:src.model.bcq:epoch: 14000/100000 | Q loss:  23.8480 | ecr:  90.1637 | is:  0.0651 | wis:  111.3500 | dr:  136.9506 | dm:  121.4154
INFO:src.model.bcq:epoch: 15000/100000 | Q loss:  3.0237 | ecr:  89.7994 | is: -3.1062 | wis:  247.2179 | dr:  137.0464 | dm:  118.4528
INFO:src.model.bcq:epoch: 16000/100000 | Q loss:  2.5638 | ecr:  89.4329 | is:  8.2983 | wis:  202.8645 | dr:  138.3806 | dm:  123.1839
INFO:src.model.bcq:epoch: 17000/100000 | Q loss:  2.3311 | ecr:  89.0899 | is: -281.2903 | wis:  48.3948 | dr:  135.3375 | dm:  120.1933
INFO:src.model.bcq:epoch: 18000/100000 | Q loss:  4.9423 | ecr:  89.1858 | is: -0.3945 | wis:  73.8964 | dr:  137.1265 | dm:  123.2123
INFO:src.model.bcq:epoch: 19000/100000 | Q loss:  5.2708 | ecr:  88.8787 | is: -16.9568 | wis:  104.3386 | dr:  143.1292 | dm:  117.9249
INFO:src.model.bcq:epoch: 20000/100000 | Q loss:  14.5213 | ecr:  90.0193 | is:  60.1088 | wis:  139.9639 | dr:  388.7867 | dm:  119.1681
INFO:src.model.bcq:epoch: 21000/100000 | Q loss:  8.0567 | ecr:  90.6965 | is:  61.8460 | wis:  29.5003 | dr:  162.2840 | dm:  121.5614
INFO:src.model.bcq:epoch: 22000/100000 | Q loss:  9.0228 | ecr:  90.9144 | is:  130.7544 | wis:  86.0403 | dr:  163.1004 | dm:  114.3819
INFO:src.model.bcq:epoch: 23000/100000 | Q loss:  3.6664 | ecr:  90.5537 | is:  133.3838 | wis:  0.2794 | dr:  170.2020 | dm:  115.7788
INFO:src.model.bcq:epoch: 24000/100000 | Q loss:  6.7866 | ecr:  90.3452 | is:  82.5717 | wis:  130.2767 | dr:  142.3279 | dm:  116.8985
INFO:src.model.bcq:epoch: 25000/100000 | Q loss:  2.5508 | ecr:  90.1654 | is:  1010.3680 | wis: -0.8487 | dr:  261.2401 | dm:  118.0224
INFO:src.model.bcq:epoch: 26000/100000 | Q loss:  5.0434 | ecr:  89.8606 | is:  1390.6921 | wis: -1.5921 | dr:  515.4992 | dm:  118.3984
INFO:src.model.bcq:epoch: 27000/100000 | Q loss:  11.6330 | ecr:  89.7247 | is:  752.2143 | wis: -3.5630 | dr:  377.8652 | dm:  115.0461
INFO:src.model.bcq:epoch: 28000/100000 | Q loss:  3.5134 | ecr:  89.8189 | is: -86.4835 | wis: -75.7078 | dr:  155.6575 | dm:  118.9985
INFO:src.model.bcq:epoch: 29000/100000 | Q loss:  2.9868 | ecr:  89.7915 | is: -106.5075 | wis: -132.3567 | dr:  139.3251 | dm:  115.6779
INFO:src.model.bcq:epoch: 30000/100000 | Q loss:  3.3359 | ecr:  89.8087 | is: -47.8711 | wis: -95.3685 | dr:  138.5154 | dm:  112.3217
INFO:src.model.bcq:epoch: 31000/100000 | Q loss:  3.0893 | ecr:  89.6264 | is: -363.7149 | wis: -102.5270 | dr:  152.6974 | dm:  115.2737
INFO:src.model.bcq:epoch: 32000/100000 | Q loss:  6.7671 | ecr:  89.6716 | is: -22.4902 | wis: -68.7015 | dr:  137.1309 | dm:  112.4016
INFO:src.model.bcq:epoch: 33000/100000 | Q loss:  6.1774 | ecr:  89.9251 | is: -1.3848 | wis:  33.2129 | dr:  137.9021 | dm:  110.1847
INFO:src.model.bcq:epoch: 34000/100000 | Q loss:  46.7842 | ecr:  89.5920 | is:  30.6511 | wis:  85.1068 | dr:  135.2790 | dm:  107.7250
INFO:src.model.bcq:epoch: 35000/100000 | Q loss:  33.6900 | ecr:  88.8647 | is: -18.2915 | wis: -3.3831 | dr:  138.3214 | dm:  116.1687
INFO:src.model.bcq:epoch: 36000/100000 | Q loss:  94.7266 | ecr:  88.7856 | is: -8.0785 | wis: -95.7630 | dr:  140.4933 | dm:  116.3896
INFO:src.model.bcq:epoch: 37000/100000 | Q loss:  4.7614 | ecr:  88.8833 | is:  29.2106 | wis: -22.1225 | dr:  138.3598 | dm:  118.0601
INFO:src.model.bcq:epoch: 38000/100000 | Q loss:  3.7051 | ecr:  89.5662 | is:  63.2026 | wis: -36.2062 | dr:  140.4136 | dm:  116.5436
INFO:src.model.bcq:epoch: 39000/100000 | Q loss:  2.9556 | ecr:  90.4483 | is:  9.4574 | wis: -69.4196 | dr:  142.2942 | dm:  116.6724
INFO:src.model.bcq:epoch: 40000/100000 | Q loss:  4.1878 | ecr:  91.0296 | is: -159.9283 | wis: -95.5775 | dr:  201.6216 | dm:  117.8742
INFO:src.model.bcq:epoch: 41000/100000 | Q loss:  9.8512 | ecr:  92.2472 | is: -28.6884 | wis: -87.5210 | dr:  296.6112 | dm:  117.5062
INFO:src.model.bcq:epoch: 42000/100000 | Q loss:  22.3072 | ecr:  91.8221 | is: -44.1541 | wis: -87.1024 | dr:  373.5880 | dm:  116.6165
INFO:src.model.bcq:epoch: 43000/100000 | Q loss:  3.8012 | ecr:  90.8647 | is: -9.7216 | wis: -87.0052 | dr:  322.2129 | dm:  116.6067
INFO:src.model.bcq:epoch: 44000/100000 | Q loss:  11.9370 | ecr:  90.6304 | is: -6.9495 | wis: -8.2613 | dr:  174.0913 | dm:  114.9893
INFO:src.model.bcq:epoch: 45000/100000 | Q loss:  5.1783 | ecr:  90.9379 | is: -6.6009 | wis:  5.4126 | dr:  174.5419 | dm:  116.0166
INFO:src.model.bcq:epoch: 46000/100000 | Q loss:  16.9661 | ecr:  90.7280 | is: -16.8839 | wis:  12.0773 | dr:  188.7271 | dm:  117.9868
INFO:src.model.bcq:epoch: 47000/100000 | Q loss:  14.0421 | ecr:  90.2471 | is: -41.2273 | wis:  97.8336 | dr:  138.7337 | dm:  116.9013
INFO:src.model.bcq:epoch: 48000/100000 | Q loss:  3.6652 | ecr:  90.1820 | is: -3.1110 | wis:  103.2842 | dr:  137.1950 | dm:  117.3678
INFO:src.model.bcq:epoch: 49000/100000 | Q loss:  11.0201 | ecr:  90.4550 | is: -2.1102 | wis:  141.8667 | dr:  140.1020 | dm:  118.8068
INFO:src.model.bcq:epoch: 50000/100000 | Q loss:  3.1317 | ecr:  90.0555 | is: -0.0702 | wis:  138.4766 | dr:  138.5706 | dm:  116.2190
INFO:src.model.bcq:epoch: 51000/100000 | Q loss:  2.9112 | ecr:  89.3265 | is: -0.8106 | wis:  105.1484 | dr:  142.1680 | dm:  116.0891
INFO:src.model.bcq:epoch: 52000/100000 | Q loss:  2.5623 | ecr:  89.2073 | is:  0.1936 | wis:  85.8967 | dr:  166.4885 | dm:  112.5708
INFO:src.model.bcq:epoch: 53000/100000 | Q loss:  3.4612 | ecr:  89.7605 | is: -2.1010 | wis:  81.4391 | dr:  143.8925 | dm:  112.5447
INFO:src.model.bcq:epoch: 54000/100000 | Q loss:  6.9418 | ecr:  90.5848 | is: -20.3560 | wis:  9.7525 | dr:  135.9477 | dm:  112.8820
INFO:src.model.bcq:epoch: 55000/100000 | Q loss:  6.1914 | ecr:  90.6045 | is: -41.8095 | wis: -66.4982 | dr:  138.3171 | dm:  111.3938
INFO:src.model.bcq:epoch: 56000/100000 | Q loss:  12.8366 | ecr:  91.0811 | is: -14.5590 | wis:  40.7855 | dr:  135.7145 | dm:  112.4703
INFO:src.model.bcq:epoch: 57000/100000 | Q loss:  2.1778 | ecr:  90.8434 | is: -28.1472 | wis: -17.3752 | dr:  139.9581 | dm:  111.6230
INFO:src.model.bcq:epoch: 58000/100000 | Q loss:  2.2897 | ecr:  90.7952 | is: -14.8943 | wis: -20.3768 | dr:  136.4884 | dm:  114.4876
INFO:src.model.bcq:epoch: 59000/100000 | Q loss:  14.2022 | ecr:  91.2121 | is: -4.1503 | wis:  145.9720 | dr:  134.5920 | dm:  114.6772
INFO:src.model.bcq:epoch: 60000/100000 | Q loss:  3.2327 | ecr:  91.2030 | is: -6.4092 | wis:  157.4950 | dr:  134.8868 | dm:  112.8660
INFO:src.model.bcq:epoch: 61000/100000 | Q loss:  15.8079 | ecr:  91.2251 | is:  7.7355 | wis:  165.0836 | dr:  135.2623 | dm:  109.3509
INFO:src.model.bcq:epoch: 62000/100000 | Q loss:  10.5173 | ecr:  91.1382 | is:  11.1409 | wis:  266.0314 | dr:  134.6986 | dm:  110.1901
INFO:src.model.bcq:epoch: 63000/100000 | Q loss:  3.2011 | ecr:  91.1284 | is:  3.6619 | wis:  191.8602 | dr:  134.4054 | dm:  111.1421
INFO:src.model.bcq:epoch: 64000/100000 | Q loss:  2.7606 | ecr:  90.9197 | is: -20.4666 | wis:  0.6593 | dr:  121.8991 | dm:  109.8876
INFO:src.model.bcq:epoch: 65000/100000 | Q loss:  11.2846 | ecr:  90.9906 | is:  0.5188 | wis:  30.3059 | dr:  121.7572 | dm:  112.2911
INFO:src.model.bcq:epoch: 66000/100000 | Q loss:  2.4613 | ecr:  91.0964 | is: -2.2578 | wis:  83.7674 | dr:  128.5559 | dm:  110.3317
INFO:src.model.bcq:epoch: 67000/100000 | Q loss:  16.4284 | ecr:  91.0757 | is: -1.2316 | wis:  76.7523 | dr:  126.9846 | dm:  109.2079
INFO:src.model.bcq:epoch: 68000/100000 | Q loss:  43.7207 | ecr:  91.3461 | is:  7.4106 | wis:  71.5582 | dr:  135.0355 | dm:  103.3778
INFO:src.model.bcq:epoch: 69000/100000 | Q loss:  2.4289 | ecr:  91.6817 | is:  3.5005 | wis:  55.5925 | dr:  136.1069 | dm:  105.1323
INFO:src.model.bcq:epoch: 70000/100000 | Q loss:  2.6607 | ecr:  91.9228 | is: -1.5665 | wis:  32.6636 | dr:  138.7545 | dm:  105.5106
INFO:src.model.bcq:epoch: 71000/100000 | Q loss:  12.0497 | ecr:  91.9805 | is: -2.8252 | wis:  62.3987 | dr:  140.3847 | dm:  105.7053
INFO:src.model.bcq:epoch: 72000/100000 | Q loss:  13.4390 | ecr:  91.7860 | is: -2.9829 | wis:  13.4240 | dr:  138.8128 | dm:  106.2758
INFO:src.model.bcq:epoch: 73000/100000 | Q loss:  3.2057 | ecr:  92.1260 | is: -8.3364 | wis: -45.4675 | dr:  137.1532 | dm:  106.6904
INFO:src.model.bcq:epoch: 74000/100000 | Q loss:  2.9736 | ecr:  91.9008 | is:  3.7896 | wis:  91.5822 | dr:  136.4677 | dm:  107.3900
INFO:src.model.bcq:epoch: 75000/100000 | Q loss:  8.7256 | ecr:  92.1682 | is: -2.3052 | wis: -41.1786 | dr:  137.1259 | dm:  106.5973
INFO:src.model.bcq:epoch: 76000/100000 | Q loss:  9.6326 | ecr:  92.0389 | is: -0.2451 | wis:  15.4665 | dr:  137.0150 | dm:  105.7635
INFO:src.model.bcq:epoch: 77000/100000 | Q loss:  2.1705 | ecr:  92.1038 | is:  1.4240 | wis:  2.9191 | dr:  135.4350 | dm:  105.1033
INFO:src.model.bcq:epoch: 78000/100000 | Q loss:  2.0669 | ecr:  92.0127 | is:  5.3323 | wis: -24.4691 | dr:  135.8777 | dm:  104.0770
INFO:src.model.bcq:epoch: 79000/100000 | Q loss:  3.9614 | ecr:  92.0648 | is: -1.7028 | wis:  46.2000 | dr:  133.9682 | dm:  106.9260
INFO:src.model.bcq:epoch: 80000/100000 | Q loss:  2.3724 | ecr:  91.7571 | is: -0.8602 | wis:  42.4453 | dr:  134.1975 | dm:  106.5628
INFO:src.model.bcq:epoch: 81000/100000 | Q loss:  13.1102 | ecr:  91.6696 | is:  0.2357 | wis:  100.0624 | dr:  134.0805 | dm:  106.5628
INFO:src.model.bcq:epoch: 82000/100000 | Q loss:  5.2683 | ecr:  91.3280 | is: -1.1628 | wis:  0.0795 | dr:  132.3649 | dm:  107.3203
INFO:src.model.bcq:epoch: 83000/100000 | Q loss:  2.3428 | ecr:  91.4247 | is: -3.0552 | wis:  0.2345 | dr:  130.6788 | dm:  108.5121
INFO:src.model.bcq:epoch: 84000/100000 | Q loss:  54.2446 | ecr:  91.7943 | is: -1.1408 | wis:  11.2609 | dr:  131.7871 | dm:  108.7203
INFO:src.model.bcq:epoch: 85000/100000 | Q loss:  2.4168 | ecr:  91.9797 | is:  23.3938 | wis:  156.1541 | dr:  132.8430 | dm:  107.4735
INFO:src.model.bcq:epoch: 86000/100000 | Q loss:  8.3262 | ecr:  92.0371 | is:  69.9106 | wis:  120.3304 | dr:  127.8245 | dm:  107.2113
INFO:src.model.bcq:epoch: 87000/100000 | Q loss:  4.3571 | ecr:  92.0278 | is: -2.0274 | wis: -8.5722 | dr:  133.4701 | dm:  107.6208
INFO:src.model.bcq:epoch: 88000/100000 | Q loss:  2.6881 | ecr:  92.3720 | is: -12.9961 | wis: -23.7819 | dr:  119.5420 | dm:  107.2387
INFO:src.model.bcq:epoch: 89000/100000 | Q loss:  10.6099 | ecr:  92.0403 | is: -0.5697 | wis:  67.9574 | dr:  129.4624 | dm:  108.8221
INFO:src.model.bcq:epoch: 90000/100000 | Q loss:  2.9205 | ecr:  92.1400 | is: -0.8203 | wis: -12.7284 | dr:  120.3925 | dm:  108.6634
INFO:src.model.bcq:epoch: 91000/100000 | Q loss:  3.1487 | ecr:  92.3324 | is: -0.5079 | wis:  29.6728 | dr:  105.4057 | dm:  109.0252
INFO:src.model.bcq:epoch: 92000/100000 | Q loss:  2.2394 | ecr:  92.5163 | is: -0.4150 | wis:  37.7006 | dr:  110.9971 | dm:  110.0993
INFO:src.model.bcq:epoch: 93000/100000 | Q loss:  3.2414 | ecr:  91.8826 | is:  6.1916 | wis:  37.5225 | dr:  135.8496 | dm:  108.4002
INFO:src.model.bcq:epoch: 94000/100000 | Q loss:  10.7150 | ecr:  91.8040 | is:  37.3781 | wis:  203.5551 | dr:  133.4872 | dm:  109.4678
INFO:src.model.bcq:epoch: 95000/100000 | Q loss:  8.4555 | ecr:  91.6941 | is:  20.2625 | wis:  202.6107 | dr:  139.2420 | dm:  109.6963
INFO:src.model.bcq:epoch: 96000/100000 | Q loss:  12.9450 | ecr:  91.5656 | is:  24.7688 | wis:  240.3158 | dr:  139.2995 | dm:  108.6955
INFO:src.model.bcq:epoch: 97000/100000 | Q loss:  2.3779 | ecr:  91.7383 | is:  0.0464 | wis:  130.1811 | dr:  138.9070 | dm:  108.7916
INFO:src.model.bcq:epoch: 98000/100000 | Q loss:  4.7306 | ecr:  91.3673 | is: -35.5754 | wis:  50.4343 | dr:  138.1056 | dm:  109.5092
INFO:src.model.bcq:epoch: 99000/100000 | Q loss:  3.1800 | ecr:  91.5829 | is: -50.0851 | wis:  55.2290 | dr:  137.9343 | dm:  107.7853
INFO:src.model.bcq:epoch: 100000/100000 | Q loss:  4.3380 | ecr:  91.2985 | is:  5.9258 | wis:  83.6868 | dr:  143.1241 | dm:  107.7262
INFO:src.model.bcq:-- saved bcq with run_name seed_1 --
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  91.2985 | is:  5.9258 | wis:  83.6868 | dr:  143.1241 | dm:  107.7262
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  91.2985 | is:  5.9258 | wis:  83.6868 | dr:  143.1241 | dm:  107.7262
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  91.2985 | is:  5.9258 | wis:  83.6868 | dr:  143.1241 | dm:  107.7262
INFO:__main__:-- training narrative planner with combined data --
INFO:src.model.bcq:-- no behavior cloning with run_name seed_1 --
INFO:src.model.bcq:-- training bcq behavior cloning --
INFO:src.model.bcq:training behavior cloning | step 1000 or 100000 | loss  0.0898
INFO:src.model.bcq:training behavior cloning | step 2000 or 100000 | loss  0.0897
INFO:src.model.bcq:training behavior cloning | step 3000 or 100000 | loss  0.0776
INFO:src.model.bcq:training behavior cloning | step 4000 or 100000 | loss  0.0648
INFO:src.model.bcq:training behavior cloning | step 5000 or 100000 | loss  0.0621
INFO:src.model.bcq:training behavior cloning | step 6000 or 100000 | loss  0.0620
INFO:src.model.bcq:training behavior cloning | step 7000 or 100000 | loss  0.0610
INFO:src.model.bcq:training behavior cloning | step 8000 or 100000 | loss  0.0565
INFO:src.model.bcq:training behavior cloning | step 9000 or 100000 | loss  0.0574
INFO:src.model.bcq:training behavior cloning | step 10000 or 100000 | loss  0.0559
INFO:src.model.bcq:training behavior cloning | step 11000 or 100000 | loss  0.0565
INFO:src.model.bcq:training behavior cloning | step 12000 or 100000 | loss  0.0541
INFO:src.model.bcq:training behavior cloning | step 13000 or 100000 | loss  0.0535
INFO:src.model.bcq:training behavior cloning | step 14000 or 100000 | loss  0.0523
INFO:src.model.bcq:training behavior cloning | step 15000 or 100000 | loss  0.0522
INFO:src.model.bcq:training behavior cloning | step 16000 or 100000 | loss  0.0472
INFO:src.model.bcq:training behavior cloning | step 17000 or 100000 | loss  0.0517
INFO:src.model.bcq:training behavior cloning | step 18000 or 100000 | loss  0.0505
INFO:src.model.bcq:training behavior cloning | step 19000 or 100000 | loss  0.0449
INFO:src.model.bcq:training behavior cloning | step 20000 or 100000 | loss  0.0471
INFO:src.model.bcq:training behavior cloning | step 21000 or 100000 | loss  0.0461
INFO:src.model.bcq:training behavior cloning | step 22000 or 100000 | loss  0.0460
INFO:src.model.bcq:training behavior cloning | step 23000 or 100000 | loss  0.0434
INFO:src.model.bcq:training behavior cloning | step 24000 or 100000 | loss  0.0399
INFO:src.model.bcq:training behavior cloning | step 25000 or 100000 | loss  0.0389
INFO:src.model.bcq:training behavior cloning | step 26000 or 100000 | loss  0.0434
INFO:src.model.bcq:training behavior cloning | step 27000 or 100000 | loss  0.0422
INFO:src.model.bcq:training behavior cloning | step 28000 or 100000 | loss  0.0417
INFO:src.model.bcq:training behavior cloning | step 29000 or 100000 | loss  0.0410
INFO:src.model.bcq:training behavior cloning | step 30000 or 100000 | loss  0.0399
INFO:src.model.bcq:training behavior cloning | step 31000 or 100000 | loss  0.0379
INFO:src.model.bcq:training behavior cloning | step 32000 or 100000 | loss  0.0374
INFO:src.model.bcq:training behavior cloning | step 33000 or 100000 | loss  0.0376
INFO:src.model.bcq:training behavior cloning | step 34000 or 100000 | loss  0.0371
INFO:src.model.bcq:training behavior cloning | step 35000 or 100000 | loss  0.0380
INFO:src.model.bcq:training behavior cloning | step 36000 or 100000 | loss  0.0372
INFO:src.model.bcq:training behavior cloning | step 37000 or 100000 | loss  0.0361
INFO:src.model.bcq:training behavior cloning | step 38000 or 100000 | loss  0.0355
INFO:src.model.bcq:training behavior cloning | step 39000 or 100000 | loss  0.0369
INFO:src.model.bcq:training behavior cloning | step 40000 or 100000 | loss  0.0390
INFO:src.model.bcq:training behavior cloning | step 41000 or 100000 | loss  0.0378
INFO:src.model.bcq:training behavior cloning | step 42000 or 100000 | loss  0.0387
INFO:src.model.bcq:training behavior cloning | step 43000 or 100000 | loss  0.0361
INFO:src.model.bcq:training behavior cloning | step 44000 or 100000 | loss  0.0391
INFO:src.model.bcq:training behavior cloning | step 45000 or 100000 | loss  0.0379
INFO:src.model.bcq:training behavior cloning | step 46000 or 100000 | loss  0.0373
INFO:src.model.bcq:training behavior cloning | step 47000 or 100000 | loss  0.0366
INFO:src.model.bcq:training behavior cloning | step 48000 or 100000 | loss  0.0353
INFO:src.model.bcq:training behavior cloning | step 49000 or 100000 | loss  0.0353
INFO:src.model.bcq:training behavior cloning | step 50000 or 100000 | loss  0.0342
INFO:src.model.bcq:training behavior cloning | step 51000 or 100000 | loss  0.0360
INFO:src.model.bcq:training behavior cloning | step 52000 or 100000 | loss  0.0359
INFO:src.model.bcq:training behavior cloning | step 53000 or 100000 | loss  0.0349
INFO:src.model.bcq:training behavior cloning | step 54000 or 100000 | loss  0.0360
INFO:src.model.bcq:training behavior cloning | step 55000 or 100000 | loss  0.0348
INFO:src.model.bcq:training behavior cloning | step 56000 or 100000 | loss  0.0340
INFO:src.model.bcq:training behavior cloning | step 57000 or 100000 | loss  0.0368
INFO:src.model.bcq:training behavior cloning | step 58000 or 100000 | loss  0.0353
INFO:src.model.bcq:training behavior cloning | step 59000 or 100000 | loss  0.0331
INFO:src.model.bcq:training behavior cloning | step 60000 or 100000 | loss  0.0336
INFO:src.model.bcq:training behavior cloning | step 61000 or 100000 | loss  0.0328
INFO:src.model.bcq:training behavior cloning | step 62000 or 100000 | loss  0.0351
INFO:src.model.bcq:training behavior cloning | step 63000 or 100000 | loss  0.0339
INFO:src.model.bcq:training behavior cloning | step 64000 or 100000 | loss  0.0358
INFO:src.model.bcq:training behavior cloning | step 65000 or 100000 | loss  0.0347
INFO:src.model.bcq:training behavior cloning | step 66000 or 100000 | loss  0.0326
INFO:src.model.bcq:training behavior cloning | step 67000 or 100000 | loss  0.0359
INFO:src.model.bcq:training behavior cloning | step 68000 or 100000 | loss  0.0312
INFO:src.model.bcq:training behavior cloning | step 69000 or 100000 | loss  0.0345
INFO:src.model.bcq:training behavior cloning | step 70000 or 100000 | loss  0.0326
INFO:src.model.bcq:training behavior cloning | step 71000 or 100000 | loss  0.0337
INFO:src.model.bcq:training behavior cloning | step 72000 or 100000 | loss  0.0358
INFO:src.model.bcq:training behavior cloning | step 73000 or 100000 | loss  0.0351
INFO:src.model.bcq:training behavior cloning | step 74000 or 100000 | loss  0.0402
INFO:src.model.bcq:training behavior cloning | step 75000 or 100000 | loss  0.0377
INFO:src.model.bcq:training behavior cloning | step 76000 or 100000 | loss  0.0360
INFO:src.model.bcq:training behavior cloning | step 77000 or 100000 | loss  0.0321
INFO:src.model.bcq:training behavior cloning | step 78000 or 100000 | loss  0.0311
INFO:src.model.bcq:training behavior cloning | step 79000 or 100000 | loss  0.0347
INFO:src.model.bcq:training behavior cloning | step 80000 or 100000 | loss  0.0332
INFO:src.model.bcq:training behavior cloning | step 81000 or 100000 | loss  0.0321
INFO:src.model.bcq:training behavior cloning | step 82000 or 100000 | loss  0.0315
INFO:src.model.bcq:training behavior cloning | step 83000 or 100000 | loss  0.0352
INFO:src.model.bcq:training behavior cloning | step 84000 or 100000 | loss  0.0328
INFO:src.model.bcq:training behavior cloning | step 85000 or 100000 | loss  0.0347
INFO:src.model.bcq:training behavior cloning | step 86000 or 100000 | loss  0.0356
INFO:src.model.bcq:training behavior cloning | step 87000 or 100000 | loss  0.0320
INFO:src.model.bcq:training behavior cloning | step 88000 or 100000 | loss  0.0343
INFO:src.model.bcq:training behavior cloning | step 89000 or 100000 | loss  0.0340
INFO:src.model.bcq:training behavior cloning | step 90000 or 100000 | loss  0.0342
INFO:src.model.bcq:training behavior cloning | step 91000 or 100000 | loss  0.0320
INFO:src.model.bcq:training behavior cloning | step 92000 or 100000 | loss  0.0334
INFO:src.model.bcq:training behavior cloning | step 93000 or 100000 | loss  0.0410
INFO:src.model.bcq:training behavior cloning | step 94000 or 100000 | loss  0.0342
INFO:src.model.bcq:training behavior cloning | step 95000 or 100000 | loss  0.0326
INFO:src.model.bcq:training behavior cloning | step 96000 or 100000 | loss  0.0343
INFO:src.model.bcq:training behavior cloning | step 97000 or 100000 | loss  0.0321
INFO:src.model.bcq:training behavior cloning | step 98000 or 100000 | loss  0.0318
INFO:src.model.bcq:training behavior cloning | step 99000 or 100000 | loss  0.0357
INFO:src.model.bcq:training behavior cloning | step 100000 or 100000 | loss  0.0361
INFO:src.model.bcq:--finished training behavior cloning--
INFO:src.model.bcq:-- saved behavior cloning with run_name seed_1 --
INFO:src.model.bcq:-- no bcq with run_name seed_1 --
INFO:src.model.bcq:-- training bcq --
INFO:src.model.bcq:epoch: 1000/100000 | Q loss:  432.7890 | ecr:  1.6072 | is:  1732.0467 | wis:  0.8399 | dr:  2161.6170 | dm:  122.6653
INFO:src.model.bcq:epoch: 2000/100000 | Q loss:  225.5126 | ecr:  4.4555 | is:  162.2136 | wis:  156.1677 | dr:  135.2546 | dm:  122.5151
INFO:src.model.bcq:epoch: 3000/100000 | Q loss:  148.7573 | ecr:  11.9155 | is:  3.3501 | wis: -10.4812 | dr:  133.3741 | dm:  98.2380
INFO:src.model.bcq:epoch: 4000/100000 | Q loss:  185.2307 | ecr:  22.8076 | is:  0.0760 | wis:  138.4838 | dr:  134.0305 | dm:  99.3422
INFO:src.model.bcq:epoch: 5000/100000 | Q loss:  124.1483 | ecr:  33.6050 | is: -7.2321 | wis:  188.3270 | dr:  130.4310 | dm:  98.2742
INFO:src.model.bcq:epoch: 6000/100000 | Q loss:  94.4384 | ecr:  47.2178 | is: -1.3392 | wis: -103.3328 | dr:  122.8933 | dm:  94.2911
INFO:src.model.bcq:epoch: 7000/100000 | Q loss:  81.9882 | ecr:  57.9547 | is: -1.5877 | wis: -192.0690 | dr:  132.9793 | dm:  100.9303
INFO:src.model.bcq:epoch: 8000/100000 | Q loss:  60.6682 | ecr:  66.4613 | is: -132.0340 | wis: -67.0989 | dr:  133.1772 | dm:  103.4610
INFO:src.model.bcq:epoch: 9000/100000 | Q loss:  77.3080 | ecr:  73.8371 | is: -0.1901 | wis:  12.4820 | dr:  134.5245 | dm:  102.6079
INFO:src.model.bcq:epoch: 10000/100000 | Q loss:  52.7331 | ecr:  78.1385 | is: -0.0000 | wis:  47.1997 | dr:  134.5855 | dm:  100.0392
INFO:src.model.bcq:epoch: 11000/100000 | Q loss:  36.5357 | ecr:  81.7136 | is:  0.0000 | wis:  5.8517 | dr:  134.3345 | dm:  101.8906
INFO:src.model.bcq:epoch: 12000/100000 | Q loss:  47.4215 | ecr:  84.0769 | is:  0.0000 | wis:  26.8890 | dr:  136.0198 | dm:  104.6095
INFO:src.model.bcq:epoch: 13000/100000 | Q loss:  35.8650 | ecr:  84.8480 | is:  0.0000 | wis:  5.2684 | dr:  147.8446 | dm:  110.3286
INFO:src.model.bcq:epoch: 14000/100000 | Q loss:  31.7650 | ecr:  86.3182 | is:  0.0000 | wis:  16.9148 | dr:  133.1504 | dm:  114.9204
INFO:src.model.bcq:epoch: 15000/100000 | Q loss:  22.3569 | ecr:  87.6489 | is:  0.0000 | wis:  0.8013 | dr:  133.5484 | dm:  116.6336
INFO:src.model.bcq:epoch: 16000/100000 | Q loss:  36.4963 | ecr:  88.6475 | is: -0.0000 | wis:  0.8123 | dr:  133.4590 | dm:  117.4161
INFO:src.model.bcq:epoch: 17000/100000 | Q loss:  23.9062 | ecr:  88.6489 | is: -0.0000 | wis:  0.8125 | dr:  137.8933 | dm:  116.6787
INFO:src.model.bcq:epoch: 18000/100000 | Q loss:  20.2281 | ecr:  89.0580 | is: -0.0000 | wis:  0.8121 | dr:  139.5588 | dm:  116.2961
INFO:src.model.bcq:epoch: 19000/100000 | Q loss:  24.4750 | ecr:  88.7601 | is: -0.0000 | wis:  0.8006 | dr:  139.3154 | dm:  117.1230
INFO:src.model.bcq:epoch: 20000/100000 | Q loss:  45.6335 | ecr:  88.7097 | is: -0.0009 | wis:  0.7915 | dr:  136.4167 | dm:  116.6633
INFO:src.model.bcq:epoch: 21000/100000 | Q loss:  10.2023 | ecr:  89.9022 | is: -0.0000 | wis:  0.7934 | dr:  138.4535 | dm:  116.2083
INFO:src.model.bcq:epoch: 22000/100000 | Q loss:  11.2390 | ecr:  90.4458 | is: -0.0005 | wis: -0.1752 | dr:  138.0917 | dm:  116.2023
INFO:src.model.bcq:epoch: 23000/100000 | Q loss:  16.0316 | ecr:  91.2429 | is: -0.0005 | wis:  0.6746 | dr:  139.2039 | dm:  117.1943
INFO:src.model.bcq:epoch: 24000/100000 | Q loss:  6.8984 | ecr:  91.1907 | is: -0.0000 | wis:  0.8065 | dr:  142.0279 | dm:  115.9437
INFO:src.model.bcq:epoch: 25000/100000 | Q loss:  11.0354 | ecr:  91.2972 | is: -0.0000 | wis:  0.8114 | dr:  141.4163 | dm:  116.0731
INFO:src.model.bcq:epoch: 26000/100000 | Q loss:  9.2640 | ecr:  91.3474 | is: -0.0001 | wis:  0.5805 | dr:  211.9020 | dm:  116.3667
INFO:src.model.bcq:epoch: 27000/100000 | Q loss:  8.3233 | ecr:  91.7295 | is: -0.0000 | wis:  0.8324 | dr:  183.5113 | dm:  115.5501
INFO:src.model.bcq:epoch: 28000/100000 | Q loss:  5.9137 | ecr:  91.3725 | is: -0.0000 | wis:  0.8046 | dr:  219.2792 | dm:  115.7620
INFO:src.model.bcq:epoch: 29000/100000 | Q loss:  16.0165 | ecr:  91.5501 | is: -0.0000 | wis:  0.5989 | dr:  150.8166 | dm:  115.0649
INFO:src.model.bcq:epoch: 30000/100000 | Q loss:  16.1418 | ecr:  91.9768 | is: -0.0000 | wis:  0.8339 | dr:  149.5348 | dm:  115.2286
INFO:src.model.bcq:epoch: 31000/100000 | Q loss:  7.3534 | ecr:  91.9815 | is: -0.0000 | wis:  0.7417 | dr:  147.2366 | dm:  115.8714
INFO:src.model.bcq:epoch: 32000/100000 | Q loss:  7.7247 | ecr:  92.2437 | is: -0.0000 | wis:  0.6752 | dr:  143.8282 | dm:  115.5281
INFO:src.model.bcq:epoch: 33000/100000 | Q loss:  4.8870 | ecr:  92.4635 | is: -0.0000 | wis:  0.6755 | dr:  150.3249 | dm:  113.8265
INFO:src.model.bcq:epoch: 34000/100000 | Q loss:  4.0025 | ecr:  93.0481 | is: -0.0000 | wis:  0.6700 | dr:  142.3957 | dm:  113.6653
INFO:src.model.bcq:epoch: 35000/100000 | Q loss:  7.1350 | ecr:  93.0115 | is: -0.0000 | wis:  0.7603 | dr:  142.8219 | dm:  111.3785
INFO:src.model.bcq:epoch: 36000/100000 | Q loss:  5.3570 | ecr:  92.7270 | is: -0.0000 | wis:  0.4438 | dr:  143.2608 | dm:  112.5517
INFO:src.model.bcq:epoch: 37000/100000 | Q loss:  3.0585 | ecr:  92.7391 | is: -0.0000 | wis: -1.4314 | dr:  141.5184 | dm:  111.2275
INFO:src.model.bcq:epoch: 38000/100000 | Q loss:  51.8580 | ecr:  91.9197 | is: -0.0000 | wis:  0.9125 | dr:  136.0680 | dm:  112.1251
INFO:src.model.bcq:epoch: 39000/100000 | Q loss:  6.0639 | ecr:  90.3945 | is:  0.0000 | wis:  0.8306 | dr:  135.4433 | dm:  110.6865
INFO:src.model.bcq:epoch: 40000/100000 | Q loss:  10.0319 | ecr:  90.4937 | is: -0.0000 | wis:  0.8693 | dr:  129.6389 | dm:  112.4985
INFO:src.model.bcq:epoch: 41000/100000 | Q loss:  6.6499 | ecr:  90.7524 | is: -0.0000 | wis:  0.6915 | dr:  130.6051 | dm:  112.2435
INFO:src.model.bcq:epoch: 42000/100000 | Q loss:  2.7437 | ecr:  91.1101 | is: -0.0000 | wis:  0.9050 | dr:  133.6158 | dm:  111.4876
INFO:src.model.bcq:epoch: 43000/100000 | Q loss:  2.5154 | ecr:  91.6242 | is: -0.0000 | wis:  0.8696 | dr:  129.5520 | dm:  112.9166
INFO:src.model.bcq:epoch: 44000/100000 | Q loss:  2.3747 | ecr:  91.9123 | is: -0.0000 | wis:  0.7991 | dr:  130.1670 | dm:  112.3362
INFO:src.model.bcq:epoch: 45000/100000 | Q loss:  2.3753 | ecr:  92.2045 | is: -0.0000 | wis: -3.2818 | dr:  129.2702 | dm:  112.2916
INFO:src.model.bcq:epoch: 46000/100000 | Q loss:  3.3394 | ecr:  92.5543 | is: -0.0000 | wis: -5.8774 | dr:  135.5902 | dm:  111.7448
INFO:src.model.bcq:epoch: 47000/100000 | Q loss:  2.0489 | ecr:  92.6313 | is: -0.0000 | wis: -24.5982 | dr:  137.3909 | dm:  112.1090
INFO:src.model.bcq:epoch: 48000/100000 | Q loss:  6.4967 | ecr:  93.0527 | is: -0.0001 | wis: -57.2186 | dr:  124.5481 | dm:  112.0726
INFO:src.model.bcq:epoch: 49000/100000 | Q loss:  2.6266 | ecr:  92.4022 | is: -0.0000 | wis:  0.9253 | dr:  125.3437 | dm:  112.4944
INFO:src.model.bcq:epoch: 50000/100000 | Q loss:  3.7795 | ecr:  92.1285 | is: -0.0000 | wis:  0.8298 | dr:  125.3173 | dm:  112.5125
INFO:src.model.bcq:epoch: 51000/100000 | Q loss:  3.4510 | ecr:  91.9514 | is:  0.0000 | wis:  0.9126 | dr:  134.4556 | dm:  111.9013
INFO:src.model.bcq:epoch: 52000/100000 | Q loss:  3.5708 | ecr:  92.4798 | is:  0.0000 | wis:  0.8207 | dr:  126.9498 | dm:  112.7888
INFO:src.model.bcq:epoch: 53000/100000 | Q loss:  2.9441 | ecr:  92.9120 | is:  0.0000 | wis:  0.8366 | dr:  135.9059 | dm:  112.8383
INFO:src.model.bcq:epoch: 54000/100000 | Q loss:  2.0552 | ecr:  93.1326 | is:  0.0000 | wis:  0.8808 | dr:  136.5268 | dm:  110.7363
INFO:src.model.bcq:epoch: 55000/100000 | Q loss:  2.0292 | ecr:  92.7412 | is:  0.0000 | wis:  0.8277 | dr:  132.7849 | dm:  112.2714
INFO:src.model.bcq:epoch: 56000/100000 | Q loss:  2.6393 | ecr:  92.6184 | is:  0.0000 | wis:  0.8139 | dr:  126.3716 | dm:  111.3453
INFO:src.model.bcq:epoch: 57000/100000 | Q loss:  5.0649 | ecr:  92.8967 | is: -0.0000 | wis:  0.8135 | dr:  127.5962 | dm:  110.8377
INFO:src.model.bcq:epoch: 58000/100000 | Q loss:  6.8769 | ecr:  92.9379 | is:  0.0000 | wis:  0.9409 | dr:  135.8012 | dm:  110.6495
INFO:src.model.bcq:epoch: 59000/100000 | Q loss:  1.9918 | ecr:  93.3071 | is: -0.0000 | wis:  0.8409 | dr:  131.4757 | dm:  111.8797
INFO:src.model.bcq:epoch: 60000/100000 | Q loss:  1.8898 | ecr:  93.4113 | is: -0.0000 | wis:  0.8128 | dr:  134.4122 | dm:  111.8280
INFO:src.model.bcq:epoch: 61000/100000 | Q loss:  2.2396 | ecr:  93.3538 | is:  0.0000 | wis:  0.8126 | dr:  126.2923 | dm:  113.2575
INFO:src.model.bcq:epoch: 62000/100000 | Q loss:  2.4789 | ecr:  93.1451 | is:  0.0000 | wis:  0.8126 | dr:  125.4146 | dm:  111.3695
INFO:src.model.bcq:epoch: 63000/100000 | Q loss:  7.6078 | ecr:  92.8859 | is: -0.0000 | wis: -0.2754 | dr:  135.1565 | dm:  109.2684
INFO:src.model.bcq:epoch: 64000/100000 | Q loss:  2.0901 | ecr:  92.8083 | is:  0.0000 | wis:  0.9334 | dr:  135.6840 | dm:  110.9028
INFO:src.model.bcq:epoch: 65000/100000 | Q loss:  2.3526 | ecr:  92.7098 | is:  0.0000 | wis:  0.8399 | dr:  136.3075 | dm:  112.2059
INFO:src.model.bcq:epoch: 66000/100000 | Q loss:  1.6120 | ecr:  92.6933 | is:  0.0000 | wis:  0.8253 | dr:  136.8448 | dm:  111.4404
INFO:src.model.bcq:epoch: 67000/100000 | Q loss:  4.4089 | ecr:  93.4902 | is: -0.0000 | wis:  0.8145 | dr:  135.8378 | dm:  111.7360
INFO:src.model.bcq:epoch: 68000/100000 | Q loss:  1.6709 | ecr:  93.1454 | is:  0.0000 | wis:  0.9179 | dr:  136.9575 | dm:  110.1111
INFO:src.model.bcq:epoch: 69000/100000 | Q loss:  2.0843 | ecr:  93.2383 | is:  0.0000 | wis:  0.8467 | dr:  136.2565 | dm:  108.6803
INFO:src.model.bcq:epoch: 70000/100000 | Q loss:  3.6325 | ecr:  93.5832 | is:  0.0000 | wis:  0.8136 | dr:  134.6591 | dm:  110.6731
INFO:src.model.bcq:epoch: 71000/100000 | Q loss:  13.0787 | ecr:  93.2800 | is:  0.0000 | wis:  0.9370 | dr:  137.0273 | dm:  110.7690
INFO:src.model.bcq:epoch: 72000/100000 | Q loss:  2.5598 | ecr:  92.5376 | is:  0.0000 | wis:  0.8139 | dr:  137.0726 | dm:  110.1355
INFO:src.model.bcq:epoch: 73000/100000 | Q loss:  2.6887 | ecr:  92.5472 | is: -0.0000 | wis:  0.8177 | dr:  137.1192 | dm:  110.9387
INFO:src.model.bcq:epoch: 74000/100000 | Q loss:  9.6184 | ecr:  92.3682 | is:  0.0000 | wis:  0.9126 | dr:  136.6722 | dm:  113.4770
INFO:src.model.bcq:epoch: 75000/100000 | Q loss:  8.1818 | ecr:  92.3139 | is:  0.0000 | wis:  0.9809 | dr:  137.5558 | dm:  110.9231
INFO:src.model.bcq:epoch: 76000/100000 | Q loss:  2.3422 | ecr:  92.5305 | is:  0.0000 | wis:  0.9590 | dr:  136.3052 | dm:  111.7300
INFO:src.model.bcq:epoch: 77000/100000 | Q loss:  8.8408 | ecr:  92.7918 | is:  0.0000 | wis:  0.8883 | dr:  137.3214 | dm:  109.9383
INFO:src.model.bcq:epoch: 78000/100000 | Q loss:  3.1714 | ecr:  93.1089 | is:  0.0000 | wis:  0.9069 | dr:  137.1384 | dm:  111.9612
INFO:src.model.bcq:epoch: 79000/100000 | Q loss:  2.2389 | ecr:  93.1491 | is: -0.0000 | wis:  1.0025 | dr:  136.0867 | dm:  112.4735
INFO:src.model.bcq:epoch: 80000/100000 | Q loss:  1.7517 | ecr:  93.2496 | is: -0.0000 | wis:  0.9094 | dr:  135.8947 | dm:  111.4939
INFO:src.model.bcq:epoch: 81000/100000 | Q loss:  1.7636 | ecr:  93.2207 | is: -0.0000 | wis:  0.9118 | dr:  126.6291 | dm:  110.8742
INFO:src.model.bcq:epoch: 82000/100000 | Q loss:  1.8985 | ecr:  93.2215 | is: -0.0000 | wis:  0.9121 | dr:  125.2871 | dm:  110.7851
INFO:src.model.bcq:epoch: 83000/100000 | Q loss:  2.8944 | ecr:  93.5508 | is: -0.0000 | wis:  0.9130 | dr:  125.0747 | dm:  110.8330
INFO:src.model.bcq:epoch: 84000/100000 | Q loss:  1.8159 | ecr:  93.6160 | is: -0.0000 | wis:  0.9121 | dr:  135.4305 | dm:  108.8108
INFO:src.model.bcq:epoch: 85000/100000 | Q loss:  1.9206 | ecr:  93.5804 | is: -0.0000 | wis:  0.9127 | dr:  136.9752 | dm:  110.9218
INFO:src.model.bcq:epoch: 86000/100000 | Q loss:  1.8865 | ecr:  93.7617 | is: -0.0000 | wis:  0.9442 | dr:  137.5713 | dm:  110.6566
INFO:src.model.bcq:epoch: 87000/100000 | Q loss:  2.0670 | ecr:  94.0581 | is: -0.0000 | wis:  0.9547 | dr:  137.1002 | dm:  111.4338
INFO:src.model.bcq:epoch: 88000/100000 | Q loss:  1.8865 | ecr:  93.8530 | is: -0.0000 | wis:  0.9440 | dr:  137.0713 | dm:  110.8876
INFO:src.model.bcq:epoch: 89000/100000 | Q loss:  2.1874 | ecr:  94.0121 | is: -0.0000 | wis:  0.9143 | dr:  136.6223 | dm:  110.2220
INFO:src.model.bcq:epoch: 90000/100000 | Q loss:  3.1235 | ecr:  94.2187 | is: -0.0000 | wis:  0.9139 | dr:  136.9142 | dm:  110.3733
INFO:src.model.bcq:epoch: 91000/100000 | Q loss:  2.1596 | ecr:  94.3663 | is: -0.0000 | wis:  0.9129 | dr:  137.3264 | dm:  109.0648
INFO:src.model.bcq:epoch: 92000/100000 | Q loss:  1.9428 | ecr:  94.5424 | is: -0.0000 | wis:  0.9124 | dr:  137.8191 | dm:  110.3808
INFO:src.model.bcq:epoch: 93000/100000 | Q loss:  4.0238 | ecr:  94.5322 | is: -0.0000 | wis:  0.9121 | dr:  137.5345 | dm:  110.2102
INFO:src.model.bcq:epoch: 94000/100000 | Q loss:  2.3901 | ecr:  94.5309 | is: -0.0000 | wis:  0.9280 | dr:  137.2919 | dm:  111.0984
INFO:src.model.bcq:epoch: 95000/100000 | Q loss:  2.1830 | ecr:  94.6065 | is:  0.0000 | wis:  0.9130 | dr:  136.9650 | dm:  110.2926
INFO:src.model.bcq:epoch: 96000/100000 | Q loss:  1.7971 | ecr:  94.5243 | is: -0.0000 | wis:  0.9144 | dr:  136.7006 | dm:  110.5141
INFO:src.model.bcq:epoch: 97000/100000 | Q loss:  4.7470 | ecr:  94.1528 | is:  0.0000 | wis:  0.9134 | dr:  136.0230 | dm:  110.6681
INFO:src.model.bcq:epoch: 98000/100000 | Q loss:  28.5177 | ecr:  94.1006 | is:  0.0001 | wis:  30.8623 | dr:  135.8708 | dm:  109.3218
INFO:src.model.bcq:epoch: 99000/100000 | Q loss:  3.9064 | ecr:  94.4221 | is:  0.0000 | wis:  0.9245 | dr:  138.1731 | dm:  109.0070
INFO:src.model.bcq:epoch: 100000/100000 | Q loss:  2.4307 | ecr:  94.7883 | is:  0.0000 | wis:  0.9624 | dr:  136.6739 | dm:  108.9498
INFO:src.model.bcq:-- saved bcq with run_name seed_1 --
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  94.7883 | is:  0.0000 | wis:  0.9624 | dr:  136.6739 | dm:  108.9498
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  94.7883 | is:  0.0000 | wis:  0.9624 | dr:  136.6739 | dm:  108.9498
INFO:src.model.bcq:epoch: 2/100000 | Q loss:  0.0000 | ecr:  94.7883 | is:  0.0000 | wis:  0.9624 | dr:  136.6739 | dm:  108.9498
INFO:__main__:++++++++++++++++++++++++++++++
INFO:__main__:===== starting run with seed 2 =====
INFO:src.utils:-- loading data from ../processed_data/narrative_trajectories.pkl with test size 0.5 --
INFO:__main__:-- training fqe for evaluation --
INFO:src.model.fqe:-- no fqe with run_name seed_2 --
INFO:src.model.fqe:-- training fqe --
INFO:src.model.fqe:epoch: 1000/100000 | Q loss:  30.2136 | ecr:  22.8771
INFO:src.model.fqe:epoch: 2000/100000 | Q loss:  25.6974 | ecr:  36.3232
INFO:src.model.fqe:epoch: 3000/100000 | Q loss:  14.7644 | ecr:  54.5268
INFO:src.model.fqe:epoch: 4000/100000 | Q loss:  22.8098 | ecr:  67.3737
INFO:src.model.fqe:epoch: 5000/100000 | Q loss:  4.0985 | ecr:  82.3798
INFO:src.model.fqe:epoch: 6000/100000 | Q loss:  1.3043 | ecr:  92.2004
INFO:src.model.fqe:epoch: 7000/100000 | Q loss:  0.4659 | ecr:  101.2152
INFO:src.model.fqe:epoch: 8000/100000 | Q loss:  4.8212 | ecr:  104.2460
INFO:src.model.fqe:epoch: 9000/100000 | Q loss:  1.7244 | ecr:  109.0886
INFO:src.model.fqe:epoch: 10000/100000 | Q loss:  0.0580 | ecr:  113.0321
INFO:src.model.fqe:epoch: 11000/100000 | Q loss:  0.0433 | ecr:  117.4447
INFO:src.model.fqe:epoch: 12000/100000 | Q loss:  0.7544 | ecr:  119.8845
INFO:src.model.fqe:epoch: 13000/100000 | Q loss:  0.1124 | ecr:  121.3178
INFO:src.model.fqe:epoch: 14000/100000 | Q loss:  0.0918 | ecr:  122.1426
INFO:src.model.fqe:epoch: 15000/100000 | Q loss:  0.0521 | ecr:  123.1264
INFO:src.model.fqe:epoch: 16000/100000 | Q loss:  0.0075 | ecr:  123.1678
INFO:src.model.fqe:epoch: 17000/100000 | Q loss:  1.5721 | ecr:  123.7045
INFO:src.model.fqe:epoch: 18000/100000 | Q loss:  46.4489 | ecr:  123.5877
INFO:src.model.fqe:epoch: 19000/100000 | Q loss:  0.6101 | ecr:  124.2039
INFO:src.model.fqe:epoch: 20000/100000 | Q loss:  0.4184 | ecr:  124.2013
INFO:src.model.fqe:epoch: 21000/100000 | Q loss:  0.2267 | ecr:  124.4291
INFO:src.model.fqe:epoch: 22000/100000 | Q loss:  0.0086 | ecr:  125.2499
INFO:src.model.fqe:epoch: 23000/100000 | Q loss:  0.0275 | ecr:  126.2189
INFO:src.model.fqe:epoch: 24000/100000 | Q loss:  0.0026 | ecr:  125.9384
INFO:src.model.fqe:epoch: 25000/100000 | Q loss:  0.0046 | ecr:  126.3672
INFO:src.model.fqe:epoch: 26000/100000 | Q loss:  0.0096 | ecr:  127.0298
INFO:src.model.fqe:epoch: 27000/100000 | Q loss:  0.0033 | ecr:  127.3568
INFO:src.model.fqe:epoch: 28000/100000 | Q loss:  0.0150 | ecr:  128.2426
INFO:src.model.fqe:epoch: 29000/100000 | Q loss:  0.0023 | ecr:  127.9306
INFO:src.model.fqe:epoch: 30000/100000 | Q loss:  0.0003 | ecr:  127.9506
INFO:src.model.fqe:epoch: 31000/100000 | Q loss:  0.0418 | ecr:  127.6570
INFO:src.model.fqe:epoch: 32000/100000 | Q loss:  0.0410 | ecr:  127.6617
INFO:src.model.fqe:epoch: 33000/100000 | Q loss:  0.0012 | ecr:  127.4942
INFO:src.model.fqe:epoch: 34000/100000 | Q loss:  0.0008 | ecr:  127.4012
INFO:src.model.fqe:epoch: 35000/100000 | Q loss:  0.0008 | ecr:  127.8341
INFO:src.model.fqe:epoch: 36000/100000 | Q loss:  0.0044 | ecr:  128.2273
INFO:src.model.fqe:epoch: 37000/100000 | Q loss:  0.0095 | ecr:  128.0559
INFO:src.model.fqe:epoch: 38000/100000 | Q loss:  0.0017 | ecr:  128.1467
INFO:src.model.fqe:epoch: 39000/100000 | Q loss:  0.0085 | ecr:  127.9268
INFO:src.model.fqe:epoch: 40000/100000 | Q loss:  0.0000 | ecr:  128.2820
INFO:src.model.fqe:epoch: 41000/100000 | Q loss:  0.0134 | ecr:  127.1679
INFO:src.model.fqe:epoch: 42000/100000 | Q loss:  0.0051 | ecr:  127.5255
INFO:src.model.fqe:epoch: 43000/100000 | Q loss:  0.0153 | ecr:  127.8120
INFO:src.model.fqe:epoch: 44000/100000 | Q loss:  0.0021 | ecr:  128.7652
INFO:src.model.fqe:epoch: 45000/100000 | Q loss:  0.1988 | ecr:  128.3155
INFO:src.model.fqe:epoch: 46000/100000 | Q loss:  0.0308 | ecr:  128.4200
INFO:src.model.fqe:epoch: 47000/100000 | Q loss:  0.0324 | ecr:  128.4221
INFO:src.model.fqe:epoch: 48000/100000 | Q loss:  0.0012 | ecr:  128.2136
INFO:src.model.fqe:epoch: 49000/100000 | Q loss:  0.0011 | ecr:  128.2700
INFO:src.model.fqe:epoch: 50000/100000 | Q loss:  0.3028 | ecr:  127.9707
INFO:src.model.fqe:epoch: 51000/100000 | Q loss:  0.0147 | ecr:  127.5257
INFO:src.model.fqe:epoch: 52000/100000 | Q loss:  0.0177 | ecr:  126.9402
INFO:src.model.fqe:epoch: 53000/100000 | Q loss:  0.0142 | ecr:  127.0039
INFO:src.model.fqe:epoch: 54000/100000 | Q loss:  0.0029 | ecr:  126.7882
INFO:src.model.fqe:epoch: 55000/100000 | Q loss:  0.0002 | ecr:  126.6409
INFO:src.model.fqe:epoch: 56000/100000 | Q loss:  0.0032 | ecr:  126.0629
INFO:src.model.fqe:epoch: 57000/100000 | Q loss:  0.0087 | ecr:  127.1411
INFO:src.model.fqe:epoch: 58000/100000 | Q loss:  0.0248 | ecr:  126.7944
INFO:src.model.fqe:epoch: 59000/100000 | Q loss:  0.0015 | ecr:  127.3739
INFO:src.model.fqe:epoch: 60000/100000 | Q loss:  0.0530 | ecr:  127.1463
INFO:src.model.fqe:epoch: 61000/100000 | Q loss:  0.0002 | ecr:  127.1335
INFO:src.model.fqe:epoch: 62000/100000 | Q loss:  0.0086 | ecr:  126.5872
INFO:src.model.fqe:epoch: 63000/100000 | Q loss:  0.0000 | ecr:  126.7989
INFO:src.model.fqe:epoch: 64000/100000 | Q loss:  0.0007 | ecr:  126.8463
INFO:src.model.fqe:epoch: 65000/100000 | Q loss:  0.0055 | ecr:  126.8832
INFO:src.model.fqe:epoch: 66000/100000 | Q loss:  0.0006 | ecr:  126.2291
INFO:src.model.fqe:epoch: 67000/100000 | Q loss:  0.0001 | ecr:  126.1881
INFO:src.model.fqe:epoch: 68000/100000 | Q loss:  0.0620 | ecr:  126.4423
INFO:src.model.fqe:epoch: 69000/100000 | Q loss:  0.0000 | ecr:  126.7887
INFO:src.model.fqe:epoch: 70000/100000 | Q loss:  0.0001 | ecr:  127.0309
INFO:src.model.fqe:epoch: 71000/100000 | Q loss:  0.0000 | ecr:  126.7852
INFO:src.model.fqe:epoch: 72000/100000 | Q loss:  0.0000 | ecr:  127.2556
INFO:src.model.fqe:epoch: 73000/100000 | Q loss:  0.0001 | ecr:  127.0421
INFO:src.model.fqe:epoch: 74000/100000 | Q loss:  0.0144 | ecr:  127.5636
INFO:src.model.fqe:epoch: 75000/100000 | Q loss:  0.0010 | ecr:  127.5501
INFO:src.model.fqe:epoch: 76000/100000 | Q loss:  0.0012 | ecr:  127.0972
INFO:src.model.fqe:epoch: 77000/100000 | Q loss:  0.0004 | ecr:  126.7987
INFO:src.model.fqe:epoch: 78000/100000 | Q loss:  0.0013 | ecr:  126.7149
INFO:src.model.fqe:epoch: 79000/100000 | Q loss:  0.0337 | ecr:  126.4641
INFO:src.model.fqe:epoch: 80000/100000 | Q loss:  0.0080 | ecr:  126.3385
INFO:src.model.fqe:epoch: 81000/100000 | Q loss:  0.0005 | ecr:  126.6989
INFO:src.model.fqe:epoch: 82000/100000 | Q loss:  0.0035 | ecr:  127.1263
INFO:src.model.fqe:epoch: 83000/100000 | Q loss:  0.0011 | ecr:  127.1627
INFO:src.model.fqe:epoch: 84000/100000 | Q loss:  0.0020 | ecr:  126.9179
INFO:src.model.fqe:epoch: 85000/100000 | Q loss:  0.0028 | ecr:  126.9649
INFO:src.model.fqe:epoch: 86000/100000 | Q loss:  0.0015 | ecr:  127.6592
INFO:src.model.fqe:epoch: 87000/100000 | Q loss:  0.0000 | ecr:  128.0795
INFO:src.model.fqe:epoch: 88000/100000 | Q loss:  0.0000 | ecr:  128.5723
INFO:src.model.fqe:epoch: 89000/100000 | Q loss:  0.0001 | ecr:  128.9757
INFO:src.model.fqe:epoch: 90000/100000 | Q loss:  0.0000 | ecr:  128.0846
INFO:src.model.fqe:epoch: 91000/100000 | Q loss:  0.0000 | ecr:  127.0531
INFO:src.model.fqe:epoch: 92000/100000 | Q loss:  0.0006 | ecr:  127.9790
INFO:src.model.fqe:epoch: 93000/100000 | Q loss:  0.0005 | ecr:  128.9255
INFO:src.model.fqe:epoch: 94000/100000 | Q loss:  0.0234 | ecr:  129.1724
INFO:src.model.fqe:epoch: 95000/100000 | Q loss:  0.0032 | ecr:  129.7866
INFO:src.model.fqe:epoch: 96000/100000 | Q loss:  0.0014 | ecr:  129.4412
INFO:src.model.fqe:epoch: 97000/100000 | Q loss:  0.0070 | ecr:  128.8072
INFO:src.model.fqe:epoch: 98000/100000 | Q loss:  0.0032 | ecr:  128.4787
INFO:src.model.fqe:epoch: 99000/100000 | Q loss:  0.0002 | ecr:  128.6734
INFO:src.model.fqe:epoch: 100000/100000 | Q loss:  0.0004 | ecr:  128.5890
INFO:src.model.fqe:-- saved fqe with run_name seed_2 --
INFO:__main__:-- training narrative planner with original data --
INFO:src.model.bcq:-- no behavior cloning with run_name seed_2 --
INFO:src.model.bcq:-- training bcq behavior cloning --
INFO:src.model.bcq:training behavior cloning | step 1000 or 100000 | loss  0.0718
INFO:src.model.bcq:training behavior cloning | step 2000 or 100000 | loss  0.0549
INFO:src.model.bcq:training behavior cloning | step 3000 or 100000 | loss  0.0481
INFO:src.model.bcq:training behavior cloning | step 4000 or 100000 | loss  0.0430
INFO:src.model.bcq:training behavior cloning | step 5000 or 100000 | loss  0.0433
INFO:src.model.bcq:training behavior cloning | step 6000 or 100000 | loss  0.0395
INFO:src.model.bcq:training behavior cloning | step 7000 or 100000 | loss  0.0407
INFO:src.model.bcq:training behavior cloning | step 8000 or 100000 | loss  0.0417
INFO:src.model.bcq:training behavior cloning | step 9000 or 100000 | loss  0.0387
INFO:src.model.bcq:training behavior cloning | step 10000 or 100000 | loss  0.0389
INFO:src.model.bcq:training behavior cloning | step 11000 or 100000 | loss  0.0382
INFO:src.model.bcq:training behavior cloning | step 12000 or 100000 | loss  0.0420
INFO:src.model.bcq:training behavior cloning | step 13000 or 100000 | loss  0.0409
INFO:src.model.bcq:training behavior cloning | step 14000 or 100000 | loss  0.0389
INFO:src.model.bcq:training behavior cloning | step 15000 or 100000 | loss  0.0394
INFO:src.model.bcq:training behavior cloning | step 16000 or 100000 | loss  0.0379
INFO:src.model.bcq:training behavior cloning | step 17000 or 100000 | loss  0.0363
INFO:src.model.bcq:training behavior cloning | step 18000 or 100000 | loss  0.0332
INFO:src.model.bcq:training behavior cloning | step 19000 or 100000 | loss  0.0382
INFO:src.model.bcq:training behavior cloning | step 20000 or 100000 | loss  0.0379
INFO:src.model.bcq:training behavior cloning | step 21000 or 100000 | loss  0.0387
INFO:src.model.bcq:training behavior cloning | step 22000 or 100000 | loss  0.0394
INFO:src.model.bcq:training behavior cloning | step 23000 or 100000 | loss  0.0371
INFO:src.model.bcq:training behavior cloning | step 24000 or 100000 | loss  0.0392
INFO:src.model.bcq:training behavior cloning | step 25000 or 100000 | loss  0.0376
INFO:src.model.bcq:training behavior cloning | step 26000 or 100000 | loss  0.0387
INFO:src.model.bcq:training behavior cloning | step 27000 or 100000 | loss  0.0379
INFO:src.model.bcq:training behavior cloning | step 28000 or 100000 | loss  0.0410
INFO:src.model.bcq:training behavior cloning | step 29000 or 100000 | loss  0.0352
INFO:src.model.bcq:training behavior cloning | step 30000 or 100000 | loss  0.0393
INFO:src.model.bcq:training behavior cloning | step 31000 or 100000 | loss  0.0405
INFO:src.model.bcq:training behavior cloning | step 32000 or 100000 | loss  0.0390
INFO:src.model.bcq:training behavior cloning | step 33000 or 100000 | loss  0.0385
INFO:src.model.bcq:training behavior cloning | step 34000 or 100000 | loss  0.0409
INFO:src.model.bcq:training behavior cloning | step 35000 or 100000 | loss  0.0393
INFO:src.model.bcq:training behavior cloning | step 36000 or 100000 | loss  0.0374
INFO:src.model.bcq:training behavior cloning | step 37000 or 100000 | loss  0.0372
INFO:src.model.bcq:training behavior cloning | step 38000 or 100000 | loss  0.0411
INFO:src.model.bcq:training behavior cloning | step 39000 or 100000 | loss  0.0387
INFO:src.model.bcq:training behavior cloning | step 40000 or 100000 | loss  0.0402
INFO:src.model.bcq:training behavior cloning | step 41000 or 100000 | loss  0.0374
INFO:src.model.bcq:training behavior cloning | step 42000 or 100000 | loss  0.0386
INFO:src.model.bcq:training behavior cloning | step 43000 or 100000 | loss  0.0393
INFO:src.model.bcq:training behavior cloning | step 44000 or 100000 | loss  0.0374
INFO:src.model.bcq:training behavior cloning | step 45000 or 100000 | loss  0.0377
INFO:src.model.bcq:training behavior cloning | step 46000 or 100000 | loss  0.0401
INFO:src.model.bcq:training behavior cloning | step 47000 or 100000 | loss  0.0393
INFO:src.model.bcq:training behavior cloning | step 48000 or 100000 | loss  0.0395
INFO:src.model.bcq:training behavior cloning | step 49000 or 100000 | loss  0.0377
INFO:src.model.bcq:training behavior cloning | step 50000 or 100000 | loss  0.0393
INFO:src.model.bcq:training behavior cloning | step 51000 or 100000 | loss  0.0410
INFO:src.model.bcq:training behavior cloning | step 52000 or 100000 | loss  0.0416
INFO:src.model.bcq:training behavior cloning | step 53000 or 100000 | loss  0.0407
INFO:src.model.bcq:training behavior cloning | step 54000 or 100000 | loss  0.0380
INFO:src.model.bcq:training behavior cloning | step 55000 or 100000 | loss  0.0389
INFO:src.model.bcq:training behavior cloning | step 56000 or 100000 | loss  0.0393
INFO:src.model.bcq:training behavior cloning | step 57000 or 100000 | loss  0.0376
INFO:src.model.bcq:training behavior cloning | step 58000 or 100000 | loss  0.0400
INFO:src.model.bcq:training behavior cloning | step 59000 or 100000 | loss  0.0374
INFO:src.model.bcq:training behavior cloning | step 60000 or 100000 | loss  0.0385
INFO:src.model.bcq:training behavior cloning | step 61000 or 100000 | loss  0.0398
INFO:src.model.bcq:training behavior cloning | step 62000 or 100000 | loss  0.0410
INFO:src.model.bcq:training behavior cloning | step 63000 or 100000 | loss  0.0392
INFO:src.model.bcq:training behavior cloning | step 64000 or 100000 | loss  0.0395
INFO:src.model.bcq:training behavior cloning | step 65000 or 100000 | loss  0.0425
INFO:src.model.bcq:training behavior cloning | step 66000 or 100000 | loss  0.0381
INFO:src.model.bcq:training behavior cloning | step 67000 or 100000 | loss  0.0385
INFO:src.model.bcq:training behavior cloning | step 68000 or 100000 | loss  0.0407
INFO:src.model.bcq:training behavior cloning | step 69000 or 100000 | loss  0.0361
INFO:src.model.bcq:training behavior cloning | step 70000 or 100000 | loss  0.0394
INFO:src.model.bcq:training behavior cloning | step 71000 or 100000 | loss  0.0393
INFO:src.model.bcq:training behavior cloning | step 72000 or 100000 | loss  0.0380
INFO:src.model.bcq:training behavior cloning | step 73000 or 100000 | loss  0.0392
INFO:src.model.bcq:training behavior cloning | step 74000 or 100000 | loss  0.0381
INFO:src.model.bcq:training behavior cloning | step 75000 or 100000 | loss  0.0409
INFO:src.model.bcq:training behavior cloning | step 76000 or 100000 | loss  0.0403
INFO:src.model.bcq:training behavior cloning | step 77000 or 100000 | loss  0.0416
INFO:src.model.bcq:training behavior cloning | step 78000 or 100000 | loss  0.0364
INFO:src.model.bcq:training behavior cloning | step 79000 or 100000 | loss  0.0393
INFO:src.model.bcq:training behavior cloning | step 80000 or 100000 | loss  0.0386
INFO:src.model.bcq:training behavior cloning | step 81000 or 100000 | loss  0.0372
INFO:src.model.bcq:training behavior cloning | step 82000 or 100000 | loss  0.0356
INFO:src.model.bcq:training behavior cloning | step 83000 or 100000 | loss  0.0389
INFO:src.model.bcq:training behavior cloning | step 84000 or 100000 | loss  0.0382
INFO:src.model.bcq:training behavior cloning | step 85000 or 100000 | loss  0.0393
INFO:src.model.bcq:training behavior cloning | step 86000 or 100000 | loss  0.0396
INFO:src.model.bcq:training behavior cloning | step 87000 or 100000 | loss  0.0383
INFO:src.model.bcq:training behavior cloning | step 88000 or 100000 | loss  0.0406
INFO:src.model.bcq:training behavior cloning | step 89000 or 100000 | loss  0.0395
INFO:src.model.bcq:training behavior cloning | step 90000 or 100000 | loss  0.0380
INFO:src.model.bcq:training behavior cloning | step 91000 or 100000 | loss  0.0394
INFO:src.model.bcq:training behavior cloning | step 92000 or 100000 | loss  0.0398
INFO:src.model.bcq:training behavior cloning | step 93000 or 100000 | loss  0.0387
INFO:src.model.bcq:training behavior cloning | step 94000 or 100000 | loss  0.0390
INFO:src.model.bcq:training behavior cloning | step 95000 or 100000 | loss  0.0376
INFO:src.model.bcq:training behavior cloning | step 96000 or 100000 | loss  0.0380
INFO:src.model.bcq:training behavior cloning | step 97000 or 100000 | loss  0.0379
INFO:src.model.bcq:training behavior cloning | step 98000 or 100000 | loss  0.0371
INFO:src.model.bcq:training behavior cloning | step 99000 or 100000 | loss  0.0384
INFO:src.model.bcq:training behavior cloning | step 100000 or 100000 | loss  0.0373
INFO:src.model.bcq:--finished training behavior cloning--
INFO:src.model.bcq:-- saved behavior cloning with run_name seed_2 --
INFO:src.model.bcq:-- no bcq with run_name seed_2 --
INFO:src.model.bcq:-- training bcq --
INFO:src.model.bcq:epoch: 1000/100000 | Q loss:  385.3727 | ecr:  7.3103 | is:  0.0000 | wis:  0.4772 | dr:  130.5718 | dm:  111.6212
INFO:src.model.bcq:epoch: 2000/100000 | Q loss:  145.3094 | ecr:  19.5810 | is:  0.0000 | wis:  0.8277 | dr:  129.1292 | dm:  110.7511
INFO:src.model.bcq:epoch: 3000/100000 | Q loss:  135.3124 | ecr:  35.3263 | is:  0.0000 | wis:  1.0021 | dr:  129.9389 | dm:  113.8250
INFO:src.model.bcq:epoch: 4000/100000 | Q loss:  121.0313 | ecr:  54.1969 | is:  0.0000 | wis:  1.0000 | dr:  130.2870 | dm:  114.5407
INFO:src.model.bcq:epoch: 5000/100000 | Q loss:  80.7273 | ecr:  71.8425 | is:  0.0000 | wis:  1.0000 | dr:  129.9837 | dm:  115.4846
INFO:src.model.bcq:epoch: 6000/100000 | Q loss:  47.8823 | ecr:  87.8500 | is:  0.0000 | wis:  0.9341 | dr:  130.1092 | dm:  115.3600
INFO:src.model.bcq:epoch: 7000/100000 | Q loss:  37.4587 | ecr:  98.4930 | is:  0.0000 | wis:  0.9500 | dr:  131.9356 | dm:  116.2257
INFO:src.model.bcq:epoch: 8000/100000 | Q loss:  16.8267 | ecr:  106.1516 | is:  0.0000 | wis:  0.9504 | dr:  132.2786 | dm:  113.9004
INFO:src.model.bcq:epoch: 9000/100000 | Q loss:  11.2551 | ecr:  110.9845 | is:  0.0000 | wis:  0.9784 | dr:  137.7776 | dm:  112.9758
INFO:src.model.bcq:epoch: 10000/100000 | Q loss:  8.3084 | ecr:  115.0708 | is:  0.0000 | wis:  0.9999 | dr:  132.0891 | dm:  113.8779
INFO:src.model.bcq:epoch: 11000/100000 | Q loss:  5.9277 | ecr:  118.9379 | is:  0.0000 | wis:  0.9945 | dr:  131.4237 | dm:  113.5566
INFO:src.model.bcq:epoch: 12000/100000 | Q loss:  7.8483 | ecr:  122.0625 | is:  0.0000 | wis:  0.9988 | dr:  132.9840 | dm:  114.0044
INFO:src.model.bcq:epoch: 13000/100000 | Q loss:  1.6085 | ecr:  124.2460 | is:  0.0000 | wis:  0.9975 | dr:  132.0128 | dm:  113.5911
INFO:src.model.bcq:epoch: 14000/100000 | Q loss:  0.9646 | ecr:  126.2643 | is:  0.0000 | wis:  0.9985 | dr:  132.0113 | dm:  113.5911
INFO:src.model.bcq:epoch: 15000/100000 | Q loss:  13.7367 | ecr:  128.7125 | is:  0.0000 | wis:  1.0000 | dr:  132.4274 | dm:  113.5793
INFO:src.model.bcq:epoch: 16000/100000 | Q loss:  1.3729 | ecr:  129.8544 | is:  0.0000 | wis:  1.0000 | dr:  132.7933 | dm:  114.2199
INFO:src.model.bcq:epoch: 17000/100000 | Q loss:  2.9189 | ecr:  130.4587 | is:  0.0000 | wis:  1.0000 | dr:  133.4967 | dm:  114.6717
INFO:src.model.bcq:epoch: 18000/100000 | Q loss:  0.1769 | ecr:  130.2075 | is:  0.0000 | wis:  1.0000 | dr:  133.0969 | dm:  114.6717
INFO:src.model.bcq:epoch: 19000/100000 | Q loss:  4.9774 | ecr:  131.2474 | is:  0.0000 | wis:  1.0000 | dr:  133.0789 | dm:  114.6717
INFO:src.model.bcq:epoch: 20000/100000 | Q loss:  0.8208 | ecr:  130.3301 | is:  0.0000 | wis:  1.0000 | dr:  132.9358 | dm:  114.6717
INFO:src.model.bcq:epoch: 21000/100000 | Q loss:  2.0271 | ecr:  129.2092 | is:  0.0000 | wis:  1.0000 | dr:  133.0028 | dm:  114.6717
INFO:src.model.bcq:epoch: 22000/100000 | Q loss:  0.1828 | ecr:  129.0134 | is: -0.0000 | wis:  1.0000 | dr:  133.0667 | dm:  114.6935
INFO:src.model.bcq:epoch: 23000/100000 | Q loss:  0.6161 | ecr:  128.7450 | is: -0.0000 | wis:  1.0000 | dr:  133.0713 | dm:  114.1783
INFO:src.model.bcq:epoch: 24000/100000 | Q loss:  4.9084 | ecr:  127.4350 | is: -0.0000 | wis:  1.0000 | dr:  133.0707 | dm:  114.1783
INFO:src.model.bcq:epoch: 25000/100000 | Q loss:  3.2041 | ecr:  126.4625 | is:  0.0000 | wis:  1.0000 | dr:  133.0754 | dm:  114.7380
INFO:src.model.bcq:epoch: 26000/100000 | Q loss:  3.7894 | ecr:  124.9812 | is:  0.0000 | wis:  1.0000 | dr:  133.0376 | dm:  114.7380
INFO:src.model.bcq:epoch: 27000/100000 | Q loss:  4.8635 | ecr:  124.1420 | is: -0.0000 | wis:  1.0000 | dr:  133.0576 | dm:  114.7380
INFO:src.model.bcq:epoch: 28000/100000 | Q loss:  4.5444 | ecr:  124.0824 | is:  0.0000 | wis:  1.0000 | dr:  133.0283 | dm:  114.7380
INFO:src.model.bcq:epoch: 29000/100000 | Q loss:  15.1688 | ecr:  121.5421 | is:  0.0000 | wis:  1.0000 | dr:  132.9086 | dm:  114.5890
INFO:src.model.bcq:epoch: 30000/100000 | Q loss:  1.3855 | ecr:  121.5568 | is:  0.0000 | wis:  1.0000 | dr:  132.9709 | dm:  114.9892
INFO:src.model.bcq:epoch: 31000/100000 | Q loss:  2.7163 | ecr:  123.0815 | is:  0.0000 | wis:  1.0000 | dr:  132.9596 | dm:  114.7380
INFO:src.model.bcq:epoch: 32000/100000 | Q loss:  1.7506 | ecr:  124.5086 | is: -0.0000 | wis:  1.0000 | dr:  132.9656 | dm:  114.5890
INFO:src.model.bcq:epoch: 33000/100000 | Q loss:  2.3842 | ecr:  125.1044 | is:  0.0000 | wis:  1.0000 | dr:  133.0214 | dm:  114.6935
INFO:src.model.bcq:epoch: 34000/100000 | Q loss:  4.6750 | ecr:  126.5785 | is:  0.0000 | wis:  1.0000 | dr:  133.0990 | dm:  114.6935
INFO:src.model.bcq:epoch: 35000/100000 | Q loss:  4.3607 | ecr:  125.4432 | is:  0.0000 | wis:  1.0000 | dr:  133.0089 | dm:  114.6935
INFO:src.model.bcq:epoch: 36000/100000 | Q loss:  1.5515 | ecr:  124.3631 | is:  0.0000 | wis:  1.0000 | dr:  133.0102 | dm:  114.6935
INFO:src.model.bcq:epoch: 37000/100000 | Q loss:  3.5526 | ecr:  124.2951 | is:  0.0000 | wis:  1.0000 | dr:  132.7734 | dm:  114.9447
INFO:src.model.bcq:epoch: 38000/100000 | Q loss:  4.0865 | ecr:  124.8131 | is: -0.0000 | wis:  1.0000 | dr:  133.1009 | dm:  114.6935
INFO:src.model.bcq:epoch: 39000/100000 | Q loss:  1.5766 | ecr:  124.4351 | is: -0.0000 | wis:  1.0000 | dr:  133.0234 | dm:  114.8789
INFO:src.model.bcq:epoch: 40000/100000 | Q loss:  0.9253 | ecr:  124.7417 | is:  0.0000 | wis:  1.0000 | dr:  133.0546 | dm:  114.8789
INFO:src.model.bcq:epoch: 41000/100000 | Q loss:  6.2078 | ecr:  123.4786 | is:  0.0000 | wis:  1.0000 | dr:  133.1065 | dm:  114.8789
INFO:src.model.bcq:epoch: 42000/100000 | Q loss:  6.4197 | ecr:  122.4757 | is:  0.0000 | wis:  1.0000 | dr:  133.2604 | dm:  114.8789
INFO:src.model.bcq:epoch: 43000/100000 | Q loss:  3.3539 | ecr:  122.2575 | is: -0.0000 | wis:  1.0000 | dr:  132.8845 | dm:  115.0668
INFO:src.model.bcq:epoch: 44000/100000 | Q loss:  2.5406 | ecr:  121.9373 | is: -0.0000 | wis:  1.0000 | dr:  133.1724 | dm:  114.8789
INFO:src.model.bcq:epoch: 45000/100000 | Q loss:  2.3405 | ecr:  121.9737 | is: -0.0000 | wis:  1.0000 | dr:  132.9637 | dm:  115.0668
INFO:src.model.bcq:epoch: 46000/100000 | Q loss:  2.3954 | ecr:  122.2040 | is: -0.0000 | wis:  1.0000 | dr:  133.3027 | dm:  115.0668
INFO:src.model.bcq:epoch: 47000/100000 | Q loss:  1.4230 | ecr:  122.4451 | is: -0.0000 | wis:  1.0000 | dr:  133.1187 | dm:  115.0668
INFO:src.model.bcq:epoch: 48000/100000 | Q loss:  25.3174 | ecr:  122.1770 | is:  0.0000 | wis:  1.0000 | dr:  132.9687 | dm:  115.0668
INFO:src.model.bcq:epoch: 49000/100000 | Q loss:  0.8282 | ecr:  121.4579 | is: -0.0000 | wis:  1.0000 | dr:  132.9574 | dm:  115.0668
INFO:src.model.bcq:epoch: 50000/100000 | Q loss:  2.8792 | ecr:  120.5542 | is: -0.0000 | wis:  1.0000 | dr:  132.9799 | dm:  114.8789
INFO:src.model.bcq:epoch: 51000/100000 | Q loss:  2.9071 | ecr:  121.1799 | is: -0.0000 | wis:  1.0000 | dr:  133.4546 | dm:  115.0450
INFO:src.model.bcq:epoch: 52000/100000 | Q loss:  2.1247 | ecr:  121.6531 | is: -0.0000 | wis:  1.0000 | dr:  133.6795 | dm:  115.2969
INFO:src.model.bcq:epoch: 53000/100000 | Q loss:  2.8916 | ecr:  120.3512 | is: -0.0000 | wis:  1.0000 | dr:  133.4656 | dm:  115.0668
INFO:src.model.bcq:epoch: 54000/100000 | Q loss:  1.0480 | ecr:  119.7446 | is: -0.0000 | wis:  1.0000 | dr:  133.7637 | dm:  114.8789
INFO:src.model.bcq:epoch: 55000/100000 | Q loss:  2.1135 | ecr:  120.2589 | is: -0.0000 | wis:  1.0000 | dr:  133.3382 | dm:  114.3218
INFO:src.model.bcq:epoch: 56000/100000 | Q loss:  0.9747 | ecr:  119.4780 | is: -0.0000 | wis:  1.0000 | dr:  133.0982 | dm:  114.1339
INFO:src.model.bcq:epoch: 57000/100000 | Q loss:  0.5470 | ecr:  119.1610 | is: -0.0000 | wis:  1.0000 | dr:  132.9779 | dm:  114.1339
INFO:src.model.bcq:epoch: 58000/100000 | Q loss:  1.3897 | ecr:  118.9343 | is: -0.0000 | wis:  1.0000 | dr:  133.0121 | dm:  114.1339
INFO:src.model.bcq:epoch: 59000/100000 | Q loss:  0.4853 | ecr:  118.4722 | is: -0.0000 | wis:  1.0000 | dr:  132.9172 | dm:  114.1339
INFO:src.model.bcq:epoch: 60000/100000 | Q loss:  2.4271 | ecr:  117.0177 | is: -0.0000 | wis:  1.0000 | dr:  132.8035 | dm:  114.1339
INFO:src.model.bcq:epoch: 61000/100000 | Q loss:  0.5584 | ecr:  117.0503 | is: -0.0000 | wis:  1.0000 | dr:  132.5873 | dm:  114.1339
INFO:src.model.bcq:epoch: 62000/100000 | Q loss:  0.5895 | ecr:  117.2217 | is: -0.0000 | wis:  1.0000 | dr:  130.3258 | dm:  114.9843
INFO:src.model.bcq:epoch: 63000/100000 | Q loss:  0.1837 | ecr:  117.1672 | is: -0.0000 | wis:  1.0000 | dr:  131.6228 | dm:  114.9843
INFO:src.model.bcq:epoch: 64000/100000 | Q loss:  0.9831 | ecr:  116.5951 | is: -0.0000 | wis:  1.0000 | dr:  131.8858 | dm:  114.1339
INFO:src.model.bcq:epoch: 65000/100000 | Q loss:  4.6500 | ecr:  116.5901 | is: -0.0000 | wis:  1.0000 | dr:  130.7092 | dm:  113.7494
INFO:src.model.bcq:epoch: 66000/100000 | Q loss:  6.3286 | ecr:  117.6404 | is: -0.0000 | wis:  1.0000 | dr:  132.2634 | dm:  114.1121
INFO:src.model.bcq:epoch: 67000/100000 | Q loss:  3.1308 | ecr:  117.2862 | is: -0.0000 | wis:  1.0000 | dr:  132.5440 | dm:  113.7963
INFO:src.model.bcq:epoch: 68000/100000 | Q loss:  5.9291 | ecr:  117.7164 | is: -0.0000 | wis:  1.0000 | dr:  132.7293 | dm:  113.8303
INFO:src.model.bcq:epoch: 69000/100000 | Q loss:  2.6590 | ecr:  116.7300 | is: -0.0000 | wis:  1.0000 | dr:  132.7521 | dm:  114.2174
INFO:src.model.bcq:epoch: 70000/100000 | Q loss:  0.8699 | ecr:  116.0100 | is: -0.0000 | wis:  1.0000 | dr:  132.8280 | dm:  114.2174
INFO:src.model.bcq:epoch: 71000/100000 | Q loss:  1.4686 | ecr:  115.3968 | is: -0.0000 | wis:  1.0000 | dr:  132.7324 | dm:  114.2174
INFO:src.model.bcq:epoch: 72000/100000 | Q loss:  0.9098 | ecr:  115.4782 | is: -0.0000 | wis:  1.0000 | dr:  133.0128 | dm:  114.2174
INFO:src.model.bcq:epoch: 73000/100000 | Q loss:  1.4374 | ecr:  115.3793 | is: -0.0000 | wis:  1.0000 | dr:  133.0017 | dm:  114.2174
INFO:src.model.bcq:epoch: 74000/100000 | Q loss:  2.0322 | ecr:  116.0944 | is: -0.0000 | wis:  1.0000 | dr:  132.7467 | dm:  115.0479
INFO:src.model.bcq:epoch: 75000/100000 | Q loss:  2.8823 | ecr:  115.7884 | is: -0.0000 | wis:  1.0000 | dr:  132.7595 | dm:  115.0234
