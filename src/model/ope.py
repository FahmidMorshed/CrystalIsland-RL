import dataclasses
from copy import deepcopy
from typing import List, Dict

import numpy as np
import pandas as pd


def doubly_robust_estimate(df_test, policy, np_discount) -> (List[Dict], float, float):
    """The Doubly Robust estimator.
        Let s_t, a_t, and r_t be the state, action, and reward at timestep t.
        This method takes a traiend Q-model for the evaluation policy \pi_e on behavior
        data generated by \pi_b.
        For behavior policy \pi_b and evaluation policy \pi_e, define the
        cumulative importance ratio at timestep t as:
        p_t = \sum_{t'=0}^t (\pi_e(a_{t'} | s_{t'}) / \pi_b(a_{t'} | s_{t'})).
        Consider an episode with length T. Let V_T = 0.
        For all t in {0, T - 1}, use the following recursive update:
        V_t^DR = (\sum_{a \in A} \pi_e(a | s_t) Q(s_t, a))
            + p_t * (r_t + \gamma * V_{t+1}^DR - Q(s_t, a_t))
        This estimator computes the expected return for \pi_e for an episode as:
        V^{\pi_e}(s_0) = V_0^DR
        and returns the mean and standard deviation over episodes.
        For more information refer to https://arxiv.org/pdf/1911.06854.pdf"""
    all_estimates = []

    for student_id, df in df_test.groupby('student_id'):
        estimates_per_episode = {}
        rewards, old_prob = np.array(df["reward"]), np.array(df["action_prob"])
        ep_length = len(df)

        states = np.stack(df['state'])
        actions = np.array(df['action'])
        new_prob = policy.action_probs(states, actions)
        new_prob = new_prob.squeeze().detach().cpu().numpy()

        v_behavior = 0.0
        v_target = 0.0
        q_values = policy.estimate_q(states, actions)
        q_values = q_values.detach().cpu().numpy()
        v_values = policy.estimate_v(states)
        v_values = v_values.detach().cpu().numpy()

        assert q_values.shape == v_values.shape == (ep_length,)

        for t in reversed(range(ep_length)):
            v_behavior = rewards[t] + np_discount * v_behavior
            v_target = v_values[t] + (new_prob[t] / old_prob[t]) * (
                    rewards[t] + np_discount * v_target - q_values[t]
            )
        v_target = v_target.item()

        estimates_per_episode["student_id"] = student_id
        estimates_per_episode["v_behavior"] = v_behavior
        estimates_per_episode["v_target"] = v_target

        all_estimates.append(estimates_per_episode)

    mean_behavior = sum(est['v_behavior'] for est in all_estimates) / len(all_estimates)
    mean_target = sum(est['v_target'] for est in all_estimates) / len(all_estimates)

    return all_estimates, mean_behavior, mean_target

# TODO Doesnt work
def importance_sampling_estimate(df_test, policy, np_discount) -> (List[Dict], float, float):
    """The step-wise IS estimator.
    Let s_t, a_t, and r_t be the state, action, and reward at timestep t.
    For behavior policy \pi_b and evaluation policy \pi_e, define the
    cumulative importance ratio at timestep t as:
    p_t = \sum_{t'=0}^t (\pi_e(a_{t'} | s_{t'}) / \pi_b(a_{t'} | s_{t'})).
    This estimator computes the expected return for \pi_e for an episode as:
    V^{\pi_e}(s_0) = \sum_t \gamma ^ {t} * p_t * r_t
    and returns the mean and standard deviation over episodes.
    For more information refer to https://arxiv.org/pdf/1911.06854.pdf"""

    all_estimates = []

    for student_id, df in df_test.groupby('student_id'):
        estimates_per_episode = {}
        rewards, old_prob = np.array(df["reward"]), np.array(df["action_prob"])
        ep_length = len(df)

        states = np.stack(df['state'])
        actions = np.array(df['action'])
        new_prob = policy.action_probs(states, actions)
        new_prob = new_prob.squeeze().detach().cpu().numpy()

        # calculate importance ratios
        p = []
        for t in range(ep_length):
            if t == 0:
                pt_prev = 1.0
            else:
                pt_prev = p[t - 1]
            pt = pt_prev * new_prob[t] / old_prob[t]
            p.append(pt)

        v_behavior = 0.0
        v_target = 0.0

        for t in range(ep_length):
            v_behavior += rewards[t] * np_discount ** t
            v_target += p[t] * rewards[t] * np_discount ** t

        v_target = v_target.item()

        estimates_per_episode["student_id"] = student_id
        estimates_per_episode["v_behavior"] = v_behavior
        estimates_per_episode["v_target"] = v_target

        all_estimates.append(estimates_per_episode)

    mean_behavior = sum(est['v_behavior'] for est in all_estimates) / len(all_estimates)
    mean_target = sum(est['v_target'] for est in all_estimates) / len(all_estimates)

    return all_estimates, mean_behavior, mean_target


# TODO Doesn't work
def weighted_importance_sampling_estimate(df_test, policy, np_discount) -> (List[Dict], float, float):
    """The step-wise WIS estimator.
    Let s_t, a_t, and r_t be the state, action, and reward at timestep t.
    For behavior policy \pi_b and evaluation policy \pi_e, define the
    cumulative importance ratio at timestep t as:
    p_t = \sum_{t'=0}^t (\pi_e(a_{t'} | s_{t'}) / \pi_b(a_{t'} | s_{t'})).
    Define the average importance ratio over episodes i in the dataset D as:
    w_t = \sum_{i \in D} p^(i)_t / |D|
    This estimator computes the expected return for \pi_e for an episode as:
    V^{\pi_e}(s_0) = \E[\sum_t \gamma ^ {t} * (p_t / w_t) * r_t]
    and returns the mean and standard deviation over episodes.
    For more information refer to https://arxiv.org/pdf/1911.06854.pdf"""

    all_estimates = []

    for student_id, df in df_test.groupby('student_id'):
        estimates_per_episode = {}
        rewards, old_prob = np.array(df["reward"]), np.array(df["action_prob"])
        ep_length = len(df)

        states = np.stack(df['state'])
        actions = np.array(df['action'])
        new_prob = policy.action_probs(states, actions)
        new_prob = new_prob.squeeze().detach().cpu().numpy()

        # calculate importance ratios
        p = []
        for t in range(ep_length):
            if t == 0:
                pt_prev = 1.0
            else:
                pt_prev = p[t - 1]
            pt = pt_prev * new_prob[t] / old_prob[t]
            p.append(pt)

        cummulative_ips_values = []
        episode_timestep_count = []
        for t, p_t in enumerate(p):
            if t >= len(cummulative_ips_values):
                cummulative_ips_values.append(p_t)
                episode_timestep_count.append(1.0)
            else:
                cummulative_ips_values[t] += p_t
                episode_timestep_count[t] += 1.0

        v_behavior = 0.0
        v_target = 0.0

        for t in range(ep_length):
            v_behavior += rewards[t] * np_discount ** t
            w_t = cummulative_ips_values[t] / episode_timestep_count[t]
            v_target += p[t] / w_t * rewards[t] * np_discount ** t

        v_target = v_target.item()

        estimates_per_episode["student_id"] = student_id
        estimates_per_episode["v_behavior"] = v_behavior
        estimates_per_episode["v_target"] = v_target

        all_estimates.append(estimates_per_episode)

    mean_behavior = sum(est['v_behavior'] for est in all_estimates) / len(all_estimates)
    mean_target = sum(est['v_target'] for est in all_estimates) / len(all_estimates)

    return all_estimates, mean_behavior, mean_target